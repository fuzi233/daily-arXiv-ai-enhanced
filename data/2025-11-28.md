<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Auxiliary Metrics Help Decoding Skill Neurons in the Wild](https://arxiv.org/abs/2511.21610)
*Yixiu Zhao,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 论文提出高效分析大语言模型“技能神经元”的方法，扩展到多技能任务，实验验证能发现驱动任务和隐藏捷径的神经元，提升模型解释性。


<details>
  <summary>Details</summary>
Motivation: 目前虽大语言模型在多项任务表现出色，但其内部机制仍不透明。已有研究通过软提示初步识别部分“技能神经元”，该论文旨在扩展至更复杂多技能场景，并提高神经元分析的解释性和实用性。

Method: 基于软提示训练和神经元激活分析，结合外部标签和模型自信分数等辅助指标，无需人工聚合token即可发现可解释的、任务相关的神经元行为。

Result: 在开放式文本生成与自然语言推理等任务中验证了方法的有效性，能检测驱动已知技能的神经元，并揭示算术推理中模型未被识别的捷径。

Conclusion: 该论文提出了一种简单轻量且广泛适用的方法，能够隔离和分析大语言模型中编码特定技能的神经元，并在多个技能场景下取得了有效结果。

Abstract: Large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, yet their internal mechanisms remain largely opaque. In this paper, we introduce a simple, lightweight, and broadly applicable method with a focus on isolating neurons that encode specific skills. Building upon prior work that identified "skill neurons" via soft prompt training on classification tasks, our approach extends the analysis to complex scenarios involving multiple skills. We correlate neuron activations with auxiliary metrics -- such as external labels and the model's own confidence score -- thereby uncovering interpretable and task-specific behaviors without the need for manual token aggregation. We empirically validate our method on tasks spanning open-ended text generation and natural language inference, demonstrating its ability to detect neurons that not only drive known skills but also reveal previously unidentified shortcuts in arithmetic reasoning on BigBench.

</details>


### [2] [Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining](https://arxiv.org/abs/2511.21613)
*Dongyang Fan,Diba Hashemi,Sai Praneeth Karimireddy,Martin Jaggi*

Main category: cs.CL

TL;DR: 本文发现整合细粒度元数据可显著提升大型语言模型的预训练效率与表现，并提出了相关实用方法与建议。


<details>
  <summary>Details</summary>
Motivation: 此前工作仅关注于URL等单一元数据信号，尚未深入研究其它类型元数据对加速预训练的潜力，激发了全面探索不同元数据类型提升预训练效率的动机。

Method: 本文通过探究不同类型元数据（包括文档质量等指标）在大型语言模型预训练中的作用，提出元数据附加（appending）和可学习元标记（meta-tokens）的方法，并采用探针分析法理解元数据对模型学习的影响。

Result: 发现细粒度元数据能显著加速预训练，通过辅助预测元数据以及引入可学习元标记，亦能部分恢复加速效果，并提供了实际应用建议。

Conclusion: 将细粒度元数据整合到大型语言模型的预训练中，可以有效提升训练效率和模型效果。

Abstract: Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.

</details>


### [3] [The author is dead, but what if they never lived? A reception experiment on Czech AI- and human-authored poetry](https://arxiv.org/abs/2511.21629)
*Anna Marklová,Ondřej Vinš,Martina Vokáčová,Jiří Milička*

Main category: cs.CL

TL;DR: AI可以写出与人类诗人难以区分的捷克语诗歌，读者对作者身份有刻板印象，审美与作者判断密切相关。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能创造性地产生文本，但关于AI生成诗歌的研究通常集中于英语。论文旨在探究AI在捷克语诗歌创作中的表现，以及捷克读者对AI和人类诗歌作品的识别与审美偏好。

Method: 通过让捷克语母语者对AI生成和人类创作的诗歌进行鉴别和审美评价，并用回归模型分析其判断和偏好。

Result: 被试仅能以机会水准判断作者身份（准确率45.8%），表明AI诗歌与人类诗歌在捷克语中难以区分。当被认为是AI创作时，诗歌评价更低，即使AI诗歌实际评价并不差。对诗歌越喜欢，反而越难正确判断其作者。文学背景对识别无影响。

Conclusion: AI可以在捷克语等形态复杂、训练数据较少的语言中创作出与人类诗歌难以区分的作品。读者对诗歌的审美评价和他们对作者身份的判断具有强关联。

Abstract: Large language models are increasingly capable of producing creative texts, yet most studies on AI-generated poetry focus on English -- a language that dominates training data. In this paper, we examine the perception of AI- and human-written Czech poetry. We ask if Czech native speakers are able to identify it and how they aesthetically judge it. Participants performed at chance level when guessing authorship (45.8\% correct on average), indicating that Czech AI-generated poems were largely indistinguishable from human-written ones. Aesthetic evaluations revealed a strong authorship bias: when participants believed a poem was AI-generated, they rated it as less favorably, even though AI poems were in fact rated equally or more favorably than human ones on average. The logistic regression model uncovered that the more the people liked a poem, the less probable was that they accurately assign the authorship. Familiarity with poetry or literary background had no effect on recognition accuracy. Our findings show that AI can convincingly produce poetry even in a morphologically complex, low-resource (with respect of the training data of AI models) Slavic language such as Czech. The results suggest that readers' beliefs about authorship and the aesthetic evaluation of the poem are interconnected.

</details>


### [4] [ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration](https://arxiv.org/abs/2511.21689)
*Hongjin Su,Shizhe Diao,Ximing Lu,Mingjie Liu,Jiacheng Xu,Xin Dong,Yonggan Fu,Peter Belcak,Hanrong Ye,Hongxu Yin,Yi Dong,Evelina Bakhturina,Tao Yu,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: 提出ToolOrchestra框架，通过训练小型编排器智能管理多工具，极大提高大型任务的效率与效果，Orchestrator模型成本更低、表现优于GPT-5，能广泛泛化并满足用户偏好，适用于实用型多工具推理场景。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型虽具备强大通用性，但解决类似于“人类最终考试”这类复杂任务时，仍面临认知和算力上的挑战，亟需提升智能上线与计算效率。

Method: 提出ToolOrchestra方法，通过强化学习训练小型编排器，协调使用各类智能工具，并在奖励函数中考虑结果、效率和用户偏好。

Result: ToolOrchestra生成的Orchestrator（8B参数）在HLE任务上准确率为37.1%，优于GPT-5（35.1%），效率提升2.5倍，同时在tau2-Bench与FRAMES任务上以约30%的成本大幅超越GPT-5，且对未见过的工具有很好的泛化能力。

Conclusion: 使用轻量级编排模型组合多样化工具，比现有方法在效率和效果上表现更优，支持规模化实用的工具增强推理系统。

Abstract: Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.

</details>


### [5] [Revisiting Generalization Across Difficulty Levels: It's Not So Easy](https://arxiv.org/abs/2511.21692)
*Yeganeh Kordi,Nihal V. Nayak,Max Zuo,Ilana Nguyen,Stephen H. Bach*

Main category: cs.CL

TL;DR: LLM很难通过只在简单或困难样本上训练而泛化到所有难度任务，有效训练和评测必须覆盖不同任务难度。


<details>
  <summary>Details</summary>
Motivation: 当前针对大型语言模型（LLM）“任务难度泛化能力”的研究结论不一，尤其是训练集的难易程度与测试效果之间的关联。因此，作者希望通过系统评测，明确LLM在不同难度任务上的泛化能力，并为数据选取和评测方法提供指导。

Method: 作者使用了六个数据集，结合上千种LLM模型的输出和教育测评领域常用的项目反应理论（IRT）对题目难度进行排名，从而排除人为主观评价，更为客观、细致地分析LLM在不同难度任务上的表现。

Result: 通过更大规模和细粒度的分析，发现LLM在难易不同的任务间“跨难度泛化能力”有限。仅用简单或困难数据训练，无法保证模型对所有难度任务有提升——必须覆盖多种难度的数据才能取得较好表现。

Conclusion: 数据难度分布对LLM训练与评估结果影响重大。只关注单一难度水平存在风险，训练和测试均需涵盖多样化难度的数据，才能提升泛化能力和性能。

Abstract: We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [On the Limits of Innate Planning in Large Language Models](https://arxiv.org/abs/2511.21591)
*Charles Schepanowski,Charles Ling*

Main category: cs.AI

TL;DR: 该文发现主流LLMs在无外部工具辅助下，在需要状态保持和规划能力的任务（如8-puzzle）表现有限，主要因内部状态易错和启发式能力弱，提示未来需要增强模型的结构化推理和状态跟踪能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在众多基准测试中表现优异，但其是否具备有效的规划能力和有状态推理能力仍不明确。作者旨在直接检验这些能力，不依赖代码执行和其他工具。

Method: 采用8-puzzle经典任务，对LLMs进行考察。这一任务要求状态跟踪和目标导向规划，并可精确逐步评估。通过Zero-Shot、Chain-of-Thought、Algorithm-of-Thought等常见提示方式，以及分级纠错反馈，对四种模型进行测试，部分实验采用外部动作校验器，限制只能作出有效移动。

Result: 反馈机制在某些模型-提示组合中能提升成功率，但多数成功的实验过程较长、计算量大且间接。引入仅允许有效行为的外部校验器后，所有模型均无法解题。定性分析发现两大缺陷：（1）内部状态表征脆弱，常作出无效动作；（2）启发式规划能力弱，经常进入循环或作出不向目标收敛的选择。

Conclusion: 当前的LLMs在不借助如代码解释器等外部工具时，规划和有状态推理能力存在明显局限。需进一步研究以实现对显式状态维护与结构化搜索的能力。

Abstract: Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.

</details>


### [7] [Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling](https://arxiv.org/abs/2511.21636)
*Peter S. Hovmand,Kari O'Donnell,Callie Ogland-Hand,Brian Biroscak,Douglas D. Gunzler*

Main category: cs.AI

TL;DR: 本文提出了将系统动力学和结构方程建模结合的新数学框架，旨在为AI/ML负责任开发提供因果分析工具，使数据科学家能够更有效地分析和比较模型结果。


<details>
  <summary>Details</summary>
Motivation: 由于AI/ML模型在解决问题方面日益重要，同时可能加剧人类偏见，亟需借助系统动力学因果模型来促进AI/ML的责任开发；然而，不同方法之间的基本假设差异使得结合变得困难。

Method: 将系统动力学与结构方程建模的方法进行整合，从分布中生成系统，开发分析方法并比较结果。

Result: 提出了一个可以整合系统动力学与结构方程建模的数学框架，应用于数据科学和AI/ML，以更好地分析和理解系统的因果结构。

Conclusion: 本文建立了系统动力学与结构方程建模之间的统一数学框架，为AI/ML的责任开发提供了理论基础。

Abstract: AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's "the unavoidable a priori"). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.

</details>


### [8] [Agentic Learner with Grow-and-Refine Multimodal Semantic Memory](https://arxiv.org/abs/2511.21678)
*Weihao Bo,Shan Zhang,Yanpeng Sun,Jingjing Wu,Qunyi Xie,Xiao Tan,Kunbin Chen,Wei He,Xiaofan Li,Na Zhao,Jingdong Wang,Zechao Li*

Main category: cs.AI

TL;DR: 本文提出ViLoMem双流多模态记忆框架，为MLLMs显著减少重复错误、提升准确率，并具备强泛化与终身学习能力，在多项基准上均表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在问题独立推理中常重复错误，且记忆增强方法多只基于轨迹存储，缺乏对多模态信息与人类认知中语义记忆的集成，影响知识积累和通用学习能力。

Method: 提出ViLoMem双流记忆框架，将视觉分散和逻辑推理错误分别编码，采用生长与精炼原则，持续更新和积累多模态语义知识，并在六项多模态基准测试上做实验和消融分析。

Result: ViLoMem在六个基准测试上持续提升pass@1准确率，显著减少视觉与逻辑重复错误，消融实验验证了分离式双流记忆的必要性和效用。

Conclusion: ViLoMem通过双流记忆框架显著提升MLLMs在多模态任务中的表现，减少重复性错误，强化语义和策略保留。

Abstract: MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.

</details>
