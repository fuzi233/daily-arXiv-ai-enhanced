<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 36]
- [cs.AI](#cs.AI) [Total: 23]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models](https://arxiv.org/abs/2511.13722)
*William Guo,Adaku Uchendu,Ana Smith*

Main category: cs.CL

TL;DR: 现有文本水印方法在保持语义正确性方面表现不错，但写作风格难以保真且容易被简单的对抗性攻击（如回译）绕过，这影响了其实际应用的可行性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成文本可能带来风险，急需可靠的水印方法检测AI生成内容。当前水印方法影响文本质量，并易被攻击去除水印信号，行业存在采用水印技术的阻力。

Method: 评估多种文本水印技术，比较了不同对抗性攻击方法（如释义和回译攻击）对水印检测能力的影响，同时使用语言学指标评估文本质量和写作风格的保真度。

Result: 水印技术能较好地保留文本语义，但写作风格与原文差距较大，且对抗性攻击（尤其是回译攻击）会显著削弱检测能力。

Conclusion: 水印技术能够保留语义，但在保护原文写作风格方面存在偏差，并且容易受到对抗性攻击（尤其是回译攻击）的影响。

Abstract: To mitigate the potential harms of Large Language Models (LLMs)generated text, researchers have proposed watermarking, a process of embedding detectable signals within text. With watermarking, we can always accurately detect LLM-generated texts. However, recent findings suggest that these techniques often negatively affect the quality of the generated texts, and adversarial attacks can strip the watermarking signals, causing the texts to possibly evade detection. These findings have created resistance in the wide adoption of watermarking by LLM creators. Finally, to encourage adoption, we evaluate the robustness of several watermarking techniques to adversarial attacks by comparing paraphrasing and back translation (i.e., English $\to$ another language $\to$ English) attacks; and their ability to preserve quality and writing style of the unwatermarked texts by using linguistic metrics to capture quality and writing style of texts. Our results suggest that these watermarking techniques preserve semantics, deviate from the writing style of the unwatermarked texts, and are susceptible to adversarial attacks, especially for the back translation attack.

</details>


### [2] [Refine Thought: A Test-Time Inference Method for Embedding Model Reasoning](https://arxiv.org/abs/2511.13726)
*Guangzhi Wang,Kai Li,Yinghao Jiao,Zhi Liu*

Main category: cs.CL

TL;DR: 作者提出了一种称为RT的方法，通过多次推理融合提升文本嵌入模型的语义推理表现，验证其在多项推理任务上有显著效果，对通用任务无负面影响。


<details>
  <summary>Details</summary>
Motivation: 现有文本嵌入模型在特定语义推理任务上的表现仍有提升空间，尤其是在实际应用如人才匹配等场景中。旨在不更改模型架构和权重的前提下提升推理能力。

Method: 提出了RT（Refine Thought）方法，即在推理阶段对文本嵌入模型进行多次前向计算，并融合多轮输出以获得最终语义表示。以此方式激活模型预训练期间习得的语义推理能力。

Result: RT方法在BRIGHT和PJBenchmark1等语义推理基准上显著优于原模型性能；在通用任务（如C-MTEB）上性能无显著波动，验证了其测试时可直接应用的有效性。

Conclusion: RT方法能够有效提升文本嵌入模型的语义推理能力，尤其在特定推理任务上表现显著提升，同时对通用语义理解任务无负面影响。

Abstract: We propose RT (Refine Thought), a method that can enhance the semantic rea-soning ability of text embedding models. The method obtains the final semanticrepresentation by running multiple forward passes of the text embedding model.Experiments show that RT achieves significant improvements on semantic reason-ing tasks in BRIGHT and the person job matching benchmark PJBenchmark1, while maintaining consistent performance on general-purpose semantic under-standing tasks such as C-MTEB. Our results indicate that RT is effective becauseit further activates the semantic reasoning ability learned during pretraining bydecoder-only text embedding models(e.g., Qwen3-Embedding-8B). RT canbe seen as a test-time inference method.

</details>


### [3] [Can QE-informed (Re)Translation lead to Error Correction?](https://arxiv.org/abs/2511.13884)
*Govardhan Padmanabhan*

Main category: cs.CL

TL;DR: 论文提出两种基于质量估测的免训练机器翻译纠错方法，分别为重翻译和局部纠错。重翻译方案显著优于局部纠错，最终获得任务冠军。


<details>
  <summary>Details</summary>
Motivation: 自动化后编辑系统虽能提升机器翻译质量，但仍面临过度修正问题。当前关注如何结合质量估测，提升翻译质量的同时减少无谓编辑。

Method: 两种方法均为免训练方法：一种为QE引导的重翻译，从多个LLM生成的翻译候选中选取质量最高者；另一种则让LLM根据QE标注的错误位置仅替换出错片段，并采用启发式以减少编辑次数，提高编辑收益率。

Result: 第一种方法的Delta COMET分值为0.0201，第二种方法为-0.0108，显著证明第一种方法在实际任务中有效性更高，获得子任务冠军。

Conclusion: 研究提出的第一种方法（从多组候选中选拔最高质量翻译）取得了子任务榜首，表现优于另一种基于QE解释的局部纠错方法。

Abstract: The paper presents two approaches submitted to the WMT 2025 Automated Translation Quality Evaluation Systems Task 3 - Quality Estimation (QE)-informed Segment-level Error Correction. While jointly training QE systems with Automatic Post-Editing (APE) has shown improved performance for both tasks, APE systems are still known to overcorrect the output of Machine Translation (MT), leading to a degradation in performance. We investigate a simple training-free approach - QE-informed Retranslation, and compare it with another within the same training-free paradigm. Our winning approach selects the highest-quality translation from multiple candidates generated by different LLMs. The second approach, more akin to APE, instructs an LLM to replace error substrings as specified in the provided QE explanation(s). A conditional heuristic was employed to minimise the number of edits, with the aim of maximising the Gain-to-Edit ratio. The two proposed approaches achieved a Delta COMET score of 0.0201 and -0.0108, respectively, leading the first approach to achieve the winning position on the subtask leaderboard.

</details>


### [4] [What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations](https://arxiv.org/abs/2511.13900)
*Mihir Gupte,Eshan Dixit,Muhammad Tayyab,Arun Adiththan*

Main category: cs.CL

TL;DR: 本文提出用于LLM长距离检索能力评测的新基准集和双指标系统，系统分析了7-8B模型在多文档任务上的行为，并实证对比缓解“失落中间”现象的各类方法，强调了方法效用的复杂性和上下文表达的关键作用。


<details>
  <summary>Details</summary>
Motivation: 长文本处理能力不足已成为大语言模型检索类应用中的核心难题，需要在真实应用任务中诊断与分析性能瓶颈，并验证现有缓解策略的实际效益。

Method: 1. 构建新的检索基准数据集GM-Extract，用于评估控制变量检索能力。
2. 提出针对空间和语义两个维度的评估指标（Document Metric和Variable Extraction Metric）。
3. 在两个多文档任务上系统性地评估7-8B参数规模LLMs，并探究不同数据表达对检索性能的影响。
4. 调查并分类主流的缓解方法，并在基准任务中进行实证测试。

Result: 改变上下文窗口中的数据呈现方式会显著影响模型检索表现，但未能稳定观测到经典U型性能趋势。评测发现，不同方法的提升效果依赖具体场景，有的情况下甚至适得其反，揭示了方法选择的实践风险和复杂性。

Conclusion: LLMs在长距离上下文检索中存在“失落中间”现象，主流的缓解方法（黑盒、白盒两类）对改进检索性能效果不一，部分场景甚至会产生负面影响。

Abstract: The diminishing ability of large language models (LLMs) to effectively utilize long-range context-the "lost-in-the-middle" phenomenon-poses a significant challenge in retrieval-based LLM applications. To study the impact of this phenomenon in a real-world application setting, we introduce GM-Extract, a novel benchmark dataset meticulously designed to evaluate LLM performance on retrieval of control variables. To accurately diagnose failure modes, we propose a simple yet elegant evaluation system using two distinct metrics: one for spatial retrieval capability (Document Metric) and the other for semantic retrieval capability (Variable Extraction Metric). We conduct a systematic evaluation of 7-8B parameter models on two multi-document tasks (key-value extraction and question-answering), demonstrating a significant change in retrieval performance simply by altering how the data is represented in the context window. While a distinct U-shaped curve was not consistently observed, our analysis reveals a clear pattern of performance across models, which we further correlate with perplexity scores. Furthermore, we perform a literature survey of mitigation methods, which we categorize into two distinct approaches: black-box and white-box methods. We then apply these techniques to our benchmark, finding that their efficacy is highly nuanced. Our evaluation highlights scenarios where these strategies successfully improve performance, as well as surprising cases where they lead to a negative impact, providing a comprehensive understanding of their utility in a practical context.

</details>


### [5] [Hint-Augmented Re-ranking: Efficient Product Search using LLM-Based Query Decomposition](https://arxiv.org/abs/2511.13994)
*Yilun Zhu,Nikhita Vedula,Shervin Malmasi*

Main category: cs.CL

TL;DR: 该文提出了基于LLM的超语义解析框架，提升了电商搜索中最高级查询的效果，并通过知识迁移兼顾了实际部署效率，取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 在电商搜索中，带有最高级修饰（如best、most popular）的查询需要对候选项进行多维度比较，这对语言理解和领域知识提出了挑战。现有系统难以高效处理此类查询。

Method: 提出了一个由LLM驱动的框架，即在检索过程中同步提取结构化属性-值提示，分解查询背后的潜在语义。通过高效迁移LLM的超语义解释至轻量模型，以突破直接LLM重排序存在的高延迟问题。

Result: 新方法在MAP指标上提升了10.9分，在MRR排名指标上提升了5.9分，显著优于基线方法。

Conclusion: 方法显著提升了对含最高级表达电商搜索查询的检索和排序性能，实现了超语义表达在高效、实用部署环境下的传递，推进了检索系统的语言解释能力提升。

Abstract: Search queries with superlatives (e.g., best, most popular) require comparing candidates across multiple dimensions, demanding linguistic understanding and domain knowledge. We show that LLMs can uncover latent intent behind these expressions in e-commerce queries through a framework that extracts structured interpretations or hints. Our approach decomposes queries into attribute-value hints generated concurrently with retrieval, enabling efficient integration into the ranking pipeline. Our method improves search performanc eby 10.9 points in MAP and ranking by 5.9 points in MRR over baselines. Since direct LLM-based reranking faces prohibitive latency, we develop an efficient approach transferring superlative interpretations to lightweight models. Our findings provide insights into how superlative semantics can be represented and transferred between models, advancing linguistic interpretation in retrieval systems while addressing practical deployment constraints.

</details>


### [6] [Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports](https://arxiv.org/abs/2511.14010)
*Chenchen Kuai,Zihao Li,Braden Rosen,Stephanie Paan,Navid Jafari,Jean-Louis Briaud,Yunlong Zhang,Youssef M. A. Hashash,Yang Zhou*

Main category: cs.CL

TL;DR: MoRA-RAG框架通过多数据库检索、智能分块和证据验证，显著提升灾后报告的多灾害问答准确率，减少幻觉，同时使开源大模型达到专有模型水平，为灾害韧性提供可操作情报新范式。


<details>
  <summary>Details</summary>
Motivation: 灾后调查报告中蕴含多灾害交互的重要线索，但由于文本非结构化，难以实现系统化知识转移。当前大语言模型缺乏领域知识支撑时容易生成不可靠内容。

Method: 提出Mixture-of-Retrieval Agentic RAG（MoRA-RAG）框架，包含动态跨灾种数据库检索机制、agentic chunking保持上下文一致性、证据校验循环以补全信息。并构建HazardRecQA数据集用于验证。

Result: MoRA-RAG在GEER报告生成的HazardRecQA问答任务上准确率达94.5%，比零样本LLM提升30%、比主流RAG提升10%，显著减少不同模型的幻觉输出；开源大模型表现逼近专有模型。

Conclusion: MoRA-RAG框架能够有效提升对灾后调查报告的结构化理解和多灾害推理能力，大幅提高问答准确率，减少幻觉现象，并使开源大模型表现逼近专有模型，推动后灾害信息及时转化为可操作的可靠情报。

Abstract: Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.

</details>


### [7] [HiEAG: Evidence-Augmented Generation for Out-of-Context Misinformation Detection](https://arxiv.org/abs/2511.14027)
*Junjie Wu,Yumeng Fu,Nan Yu,Guohong Fu*

Main category: cs.CL

TL;DR: 提出HiEAG新方法，通过分层结合多模态大模型与自动证据操作，显著提升图文虚假信息检测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 以往OOC虚假信息检测方法重视内部一致性，忽略了外部从其他证据源获取一致性约束的价值，导致结果可靠性不足。

Method: 本文提出了一种分层证据增强生成（HiEAG）框架，将外部一致性检测流程分解为检索、重排序和重写，分别利用AESP自动证据选择提示和AEGP自动证据生成提示，与多模态大语言模型结合。

Result: HiEAG在多个基准数据集上获得了准确率上的显著增益，超越现有最先进方法，并支持判别过程解释。

Conclusion: 提出的HiEAG方法有效提升了多模态图文OOC虚假信息的外部一致性检测能力，并显著优于以往SOTA方法。

Abstract: Recent advancements in multimodal out-of-context (OOC) misinformation detection have made remarkable progress in checking the consistencies between different modalities for supporting or refuting image-text pairs. However, existing OOC misinformation detection methods tend to emphasize the role of internal consistency, ignoring the significant of external consistency between image-text pairs and external evidence. In this paper, we propose HiEAG, a novel Hierarchical Evidence-Augmented Generation framework to refine external consistency checking through leveraging the extensive knowledge of multimodal large language models (MLLMs). Our approach decomposes external consistency checking into a comprehensive engine pipeline, which integrates reranking and rewriting, apart from retrieval. Evidence reranking module utilizes Automatic Evidence Selection Prompting (AESP) that acquires the relevant evidence item from the products of evidence retrieval. Subsequently, evidence rewriting module leverages Automatic Evidence Generation Prompting (AEGP) to improve task adaptation on MLLM-based OOC misinformation detectors. Furthermore, our approach enables explanation for judgment, and achieves impressive performance with instruction tuning. Experimental results on different benchmark datasets demonstrate that our proposed HiEAG surpasses previous state-of-the-art (SOTA) methods in the accuracy over all samples.

</details>


### [8] [Based on Data Balancing and Model Improvement for Multi-Label Sentiment Classification Performance Enhancement](https://arxiv.org/abs/2511.14073)
*Zijin Su,Huanzhu Lv,Yuren Niu,Yiming Liu*

Main category: cs.CL

TL;DR: 构建了类别均衡的多标签情感数据集，并提出融合多种深度学习技术的模型，显著提升了多标签情感分类性能，尤其改善了对罕见情感类别的识别。


<details>
  <summary>Details</summary>
Motivation: 现有多标签情感分类数据集（如GoEmotions）存在类别极度不平衡现象，致使模型对少数类别的识别能力较弱。因此亟需构建类别均衡的数据集并改进模型结构，以提升对各类别的全面识别能力。

Method: 构建了一个包含28类情感的平衡多标签情感数据集。该数据集整合了原始GoEmotions数据、由RoBERTa-base-GoEmotions模型在Sentiment140数据集上情感标注的样本，以及GPT-4 mini生成文本的人工标注样本。模型设计上结合了预训练FastText词向量、卷积层提取局部特征、双向LSTM学习上下文信息、注意力机制突出情感相关词汇，通过sigmoid激活输出实现多标签预测，并采用混合精度训练提升效率。

Result: 实验表明，基于新平衡数据集和改进模型，分类准确率、精度、召回率、F1分数和AUC均有显著提升，尤其是在处理原本少见情感类别时性能改善明显。

Conclusion: 本文提出的数据平衡策略和改进模型能够显著提升多标签情感分类任务在各项指标上的表现，特别是在精度、召回率、F1分数和AUC方面，优于传统使用不平衡数据训练的模型，验证了方法的有效性。

Abstract: Multi-label sentiment classification plays a vital role in natural language processing by detecting multiple emotions within a single text. However, existing datasets like GoEmotions often suffer from severe class imbalance, which hampers model performance, especially for underrepresented emotions. To address this, we constructed a balanced multi-label sentiment dataset by integrating the original GoEmotions data, emotion-labeled samples from Sentiment140 using a RoBERTa-base-GoEmotions model, and manually annotated texts generated by GPT-4 mini. Our data balancing strategy ensured an even distribution across 28 emotion categories. Based on this dataset, we developed an enhanced multi-label classification model that combines pre-trained FastText embeddings, convolutional layers for local feature extraction, bidirectional LSTM for contextual learning, and an attention mechanism to highlight sentiment-relevant words. A sigmoid-activated output layer enables multi-label prediction, and mixed precision training improves computational efficiency. Experimental results demonstrate significant improvements in accuracy, precision, recall, F1-score, and AUC compared to models trained on imbalanced data, highlighting the effectiveness of our approach.

</details>


### [9] [Stealth Fine-Tuning: Efficiently Breaking Alignment in RVLMs Using Self-Generated CoT](https://arxiv.org/abs/2511.14106)
*Le Yu,Zhengyue Zhao,Yawen Zheng,Yunhao Liu*

Main category: cs.CL

TL;DR: Stealth Fine-Tuning是一种低成本且高效的新型攻击方法，能够突破理由增强视觉-语言模型的安全防线，大幅提升恶意输出成功率，并且不影响模型原有的推理表现。


<details>
  <summary>Details</summary>
Motivation: 由于理由增强的视觉-语言模型（RVLMs）的展开推理链条暴露了新的攻击面，原有安全对齐机制容易被突破，亟需研究攻击及防御的新方式。

Method: 该方法通过分段级干预（segment-level interference）和自监督式微调，将模型自生成有害推理过程作为微调数据，并采用回合加权损失设计，使微调既轻量又与原分布一致。实验中用499个样本，单卡A100运行3小时，即得到显著性能提升。

Result: 与IDEATOR等方法相比，Stealth Fine-Tuning能够在攻击成功率（ASR）上提升38.52%，且调优后模型的推理能力几乎无损。此外，该方法在主流的AdvBench和通用基准测试上，都是成本低、有效性高的防御绕过手段。

Conclusion: 提出的Stealth Fine-Tuning方法能够显著突破RVLMs的安全对齐，提升攻击成功率，同时保持推理能力不减。实验结果证明该方法可以有效绕过现有防御机制。

Abstract: Reasoning-augmented Vision-Language Models (RVLMs) rely on safety alignment to prevent harmful behavior, yet their exposed chain-of-thought (CoT) traces introduce new attack surfaces. In this work, we find that the safety alignment of RVLMs can be easily break through a novel attack method termed \textbf{Stealth Fine-Tuning}. Our method elicits harmful reasoning traces through \textbf{segment-level interference} and reuses the self-generated outputs as supervised fine-tuning data. Through a \textbf{turn-based weighted} loss design, yielding a lightweight, distribution-consistent finetuning method. In our experiment, with only 499 samples and under 3 hours on a single A100 (QLoRA), Stealth Fine-Tuning outperforms IDEATOR by 38.52\% ASR while preserving general reasoning ability, as the tuned model retains the original representation distribution. Experiments on AdvBench and several general benchmarks demonstrate that Stealth Fine-Tuning is a low-cost and highly effective way to bypass alignment defenses. \textcolor{red}{\textbf{Disclaimer: This paper contains content that may be disturbing or offensive.}}

</details>


### [10] [Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding](https://arxiv.org/abs/2511.14112)
*Truong Vo,Weiyi Wu,Kaize Ding*

Main category: cs.CL

TL;DR: 通过引入覆盖罕见ICD编码的高质量合成临床文本，扩充训练集，可以小幅推高自动ICD编码的宏F1分数，并提升long-tail罕见编码的预测公平性。


<details>
  <summary>Details</summary>
Motivation: 自动ICD编码任务极端长尾分布导致大量罕见及零次出现的ICD用于训练的数据极度不足，造成整体宏F1分数低、预测公平性差，亟需突破。

Method: 提出了一个以数据为中心的框架，通过真实共现模式、ICD描述、同义词、层级结构、相似临床案例等信息生成结构化提示，并利用这些提示生成包含7,902个ICD编码、共计90,000条的合成临床文本；再将这部分合成数据与原始数据结合，对主流Transformer模型（PLM-ICD与GKI-ICD）进行微调。

Result: 采用合成数据辅助训练可以在宏F1方面取得温和提升，同时保持较高微F1，且优于此前SOTA方法，实验证明构建精细的合成数据能实质缓解编码不公平问题。

Conclusion: 通过生成高质量的合成出院小结来改善诊断编码极端长尾分布带来的预测不公平性，能够提升自动ICD编码系统在稀有码上的表现，提升了long-tail ICD code的预测公平性。

Abstract: Automatic ICD coding from clinical text is a critical task in medical NLP but remains hindered by the extreme long-tail distribution of diagnostic codes. Thousands of rare and zero-shot ICD codes are severely underrepresented in datasets like MIMIC-III, leading to low macro-F1 scores. In this work, we propose a data-centric framework that generates high-quality synthetic discharge summaries to mitigate this imbalance. Our method constructs realistic multi-label code sets anchored on rare codes by leveraging real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and similar clinical notes. Using these structured prompts, we generate 90,000 synthetic notes covering 7,902 ICD codes, significantly expanding the training distribution. We fine-tune two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, on both the original and extended datasets. Experiments show that our approach modestly improves macro-F1 while maintaining strong micro-F1, outperforming prior SOTA. While the gain may seem marginal relative to the computational cost, our results demonstrate that carefully crafted synthetic data can enhance equity in long-tail ICD code prediction.

</details>


### [11] [From Graphs to Hypergraphs: Enhancing Aspect-Based Sentiment Analysis via Multi-Level Relational Modeling](https://arxiv.org/abs/2511.14142)
*Omkar Mahesh Kashyap,Padegal Amit,Madhav Kashyap,Ashwini M Joshi,Shylaja SS*

Main category: cs.CL

TL;DR: HyperABSA提出动态超图方法解决ABSA的关系建模瓶颈，在多项基准上超越现有主流方法，适用于短文本NLP任务。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的方法无法有效表达多方面、复杂关系，且多图融合存在冗余、参数消耗和错误传播问题，影响模型在短文本或低资源场景的表现。

Method: 提出了一种动态超图结构，通过sample-specific层级聚类生成超边，并设计了加速回退的分割机制自适应确定粒度。方法在三个基准（Lap14、Rest14、MAMS）上评估，并与RoBERTa融合。

Result: HyperABSA在各基准数据集上均优于强图算法，尤其在结合RoBERTa主干时，性能提升更为显著。

Conclusion: 动态超图框架HyperABSA能显著提升ABSA任务的性能，尤其在短文本和低资源场景下展现更强鲁棒性。

Abstract: Aspect-Based Sentiment Analysis (ABSA) predicts sentiment polarity for specific aspect terms, a task made difficult by conflicting sentiments across aspects and the sparse context of short texts. Prior graph-based approaches model only pairwise dependencies, forcing them to construct multiple graphs for different relational views. These introduce redundancy, parameter overhead, and error propagation during fusion, limiting robustness in short-text, low-resource settings. We present HyperABSA, a dynamic hypergraph framework that induces aspect-opinion structures through sample-specific hierarchical clustering. To construct these hyperedges, we introduce a novel acceleration-fallback cutoff for hierarchical clustering, which adaptively determines the level of granularity. Experiments on three benchmarks (Lap14, Rest14, MAMS) show consistent improvements over strong graph baselines, with substantial gains when paired with RoBERTa backbones. These results position dynamic hypergraph construction as an efficient, powerful alternative for ABSA, with potential extensions to other short-text NLP tasks.

</details>


### [12] [Applying Relation Extraction and Graph Matching to Answering Multiple Choice Questions](https://arxiv.org/abs/2511.14144)
*Naoki Shimoda,Akihiro Yamamoto*

Main category: cs.CL

TL;DR: 利用Transformer关系抽取动态生成并校验知识图谱解决填空式选择题，答题准确率约70%，方法支持过程可追溯；题目类别影响结果表现。


<details>
  <summary>Details</summary>
Motivation: 知识图谱构建成本高且通常作为静态数据库，传统方法难以灵活利用文本信息。用Transformer关系抽取方法动态生成KG，能够提高对输入语句含义理解。还需解决关系抽取方法可能因输入错误信息而生成虚假KG的问题。

Method: 采用基于Transformer的关系抽取技术生成知识图谱，并将输入句子转化为关系图，然后与事实性知识图谱进行匹配和验证，保证输出答案的真实性和可追溯性。

Result: 方法能正确回答约70%的测试题目，具备全过程可追溯性，同时指出不同题型准确率差异较大。

Conclusion: 本文提出的方法能在保持输出过程可追溯性的情况下，有效回答约70%的填空式多项选择题，且题目类别对准确性影响显著。

Abstract: In this research, we combine Transformer-based relation extraction with matching of knowledge graphs (KGs) and apply them to answering multiple-choice questions (MCQs) while maintaining the traceability of the output process. KGs are structured representations of factual knowledge consisting of entities and relations. Due to the high construction cost, they had been regarded as static databases with validated links. However, the recent development of Transformer-based relation extraction (RE) methods has enabled us to generate KGs dynamically by giving them natural language texts, and thereby opened the possibility for representing the meaning of the input sentences with the created KGs. Using this effect, we propose a method that answers MCQs in the "fill-in-the-blank" format, taking care of the point that RE methods generate KGs that represent false information if provided with factually incorrect texts. We measure the truthfulness of each question sentence by (i) converting the sentence into a relational graph using an RE method and (ii) verifying it against factually correct KGs under the closed-world assumption. The experimental results demonstrate that our method correctly answers up to around 70% of the questions, while providing traceability of the procedure. We also highlight that the question category has a vast influence on the accuracy.

</details>


### [13] [Selective Weak-to-Strong Generalization](https://arxiv.org/abs/2511.14166)
*Hao Lang,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: 本文提出选择性弱到强泛化方法，通过智能筛选和优化弱标签，改善大模型对齐效果，有更强性能与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有W2SG方法始终使用弱监督，导致部分弱标签对模型有害，且缺乏高质量对齐数据，因此需要更加鲁棒且高效的数据利用方式。

Method: 训练一个二分类器P(IK)来判断强模型能否回答问题，并对能够回答的问题使用模型自己生成的标签进行对齐；同时利用图平滑方法对弱标签进行优化。

Result: 在三个基准测试中，所提方法在性能上持续优于主流对比方法，且二分类器P(IK)在任务和难度间具备较好的泛化能力，验证了选择性W2SG框架对超对齐任务的益处。

Conclusion: 提出了一种选择性弱到强泛化（W2SG）框架，通过仅在必要时使用弱监督，从而提升模型对齐的效果，实验结果优于现有方法。

Abstract: Future superhuman models will surpass the ability of humans and humans will only be able to \textit{weakly} supervise superhuman models. To alleviate the issue of lacking high-quality data for model alignment, some works on weak-to-strong generalization (W2SG) finetune a strong pretrained model with a weak supervisor so that it can generalize beyond weak supervision. However, the invariable use of weak supervision in existing methods exposes issues in robustness, with a proportion of weak labels proving harmful to models. In this paper, we propose a selective W2SG framework to avoid using weak supervision when unnecessary. We train a binary classifier P(IK) to identify questions that a strong model can answer and use its self-generated labels for alignment. We further refine weak labels with a graph smoothing method. Extensive experiments on three benchmarks show that our method consistently outperforms competitive baselines. Further analyses show that P(IK) can generalize across tasks and difficulties, which indicates selective W2SG can help superalignment.

</details>


### [14] [SymLoc: Symbolic Localization of Hallucination across HaluEval and TruthfulQA](https://arxiv.org/abs/2511.14172)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: LLMs still struggle with hallucination, especially when confronted with symbolic triggers like modifiers, negation, numbers, exceptions, and named entities. Yet, we lack a clear understanding of where these symbolic hallucinations originate, making it crucial to systematically handle such triggers and localize the emergence of hallucination inside the model. While prior work explored localization using statistical techniques like LSC and activation variance analysis, these methods treat all tokens equally and overlook the role symbolic linguistic knowledge plays in triggering hallucinations. So far, no approach has investigated how symbolic elements specifically drive hallucination failures across model layers, nor has symbolic linguistic knowledge been used as the foundation for a localization framework. We propose the first symbolic localization framework that leverages symbolic linguistic and semantic knowledge to meaningfully trace the development of hallucinations across all model layers. By focusing on how models process symbolic triggers, we analyze five models using HaluEval and TruthfulQA. Our symbolic knowledge approach reveals that attention variance for these linguistic elements explodes to critical instability in early layers (2-4), with negation triggering catastrophic variance levels, demonstrating that symbolic semantic processing breaks down from the very beginning. Through the lens of symbolic linguistic knowledge, despite larger model sizes, hallucination rates remain consistently high (78.3%-83.7% across Gemma variants), with steep attention drops for symbolic semantic triggers throughout deeper layers. Our findings demonstrate that hallucination is fundamentally a symbolic linguistic processing failure, not a general generation problem, revealing that symbolic semantic knowledge provides the key to understanding and localizing hallucination mechanisms in LLMs.

</details>


### [15] [Harnessing Deep LLM Participation for Robust Entity Linking](https://arxiv.org/abs/2511.14181)
*Jiajun Hou,Chenyu Zhang,Rui Meng*

Main category: cs.CL

TL;DR: 提出了全环节集成LLM的实体链接新框架DeepEL，并引入自验证机制，实验证明其在多个数据集上均取得了显著领先的效果。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM已被用于提升实体消歧和输入表征，但过去研究仅限于EL任务的部分阶段，尚未实现LLM在整个流程的深度融合。此外，仅对单一实体进行消歧未能充分优化整体性能。

Method: 提出了DeepEL框架，全面整合LLM于实体链接任务的每个环节，并设计了一种利用全局上下文进行自验证的新机制，使LLM能够自我修正，提升实体之间关系的识别能力。

Result: 在十个基准数据集上实证验证，DeepEL平均提升整体F1分数2.6%，在域外数据集上提升4%，全面优于现有最佳方法。

Conclusion: 深度集成LLM于每阶段的实体链接过程（DeepEL框架）能够显著提升实体链接任务的性能，特别是在各种基准数据集上都优于现有方法。

Abstract: Entity Linking (EL), the task of mapping textual entity mentions to their corresponding entries in knowledge bases, constitutes a fundamental component of natural language understanding. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable potential for enhancing EL performance. Prior research has leveraged LLMs to improve entity disambiguation and input representation, yielding significant gains in accuracy and robustness. However, these approaches typically apply LLMs to isolated stages of the EL task, failing to fully integrate their capabilities throughout the entire process.
  In this work, we introduce DeepEL, a comprehensive framework that incorporates LLMs into every stage of the entity linking task. Furthermore, we identify that disambiguating entities in isolation is insufficient for optimal performance. To address this limitation, we propose a novel self-validation mechanism that utilizes global contextual information, enabling LLMs to rectify their own predictions and better recognize cohesive relationships among entities within the same sentence.
  Extensive empirical evaluation across ten benchmark datasets demonstrates that DeepEL substantially outperforms existing state-of-the-art methods, achieving an average improvement of 2.6\% in overall F1 score and a remarkable 4% gain on out-of-domain datasets. These results underscore the efficacy of deep LLM integration in advancing the state-of-the-art in entity linking.

</details>


### [16] [ArbESC+: Arabic Enhanced Edit Selection System Combination for Grammatical Error Correction Resolving conflict and improving system combination in Arabic GEC](https://arxiv.org/abs/2511.14230)
*Ahlam Alrehili,Areej Alhothali*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Grammatical Error Correction (GEC) is an important aspect of natural language processing. Arabic has a complicated morphological and syntactic structure, posing a greater challenge than other languages. Even though modern neural models have improved greatly in recent years, the majority of previous attempts used individual models without taking into account the potential benefits of combining different systems. In this paper, we present one of the first multi-system approaches for correcting grammatical errors in Arabic, the Arab Enhanced Edit Selection System Complication (ArbESC+). Several models are used to collect correction proposals, which are represented as numerical features in the framework. A classifier determines and implements the appropriate corrections based on these features. In order to improve output quality, the framework uses support techniques to filter overlapping corrections and estimate decision reliability. A combination of AraT5, ByT5, mT5, AraBART, AraBART+Morph+GEC, and Text editing systems gave better results than a single model alone, with F0.5 at 82.63% on QALB-14 test data, 84.64% on QALB-15 L1 data, and 65.55% on QALB-15 L2 data. As one of the most significant contributions of this work, it's the first Arab attempt to integrate linguistic error correction. Improving existing models provides a practical step towards developing advanced tools that will benefit users and researchers of Arabic text processing.

</details>


### [17] [MuCPT: Music-related Natural Language Model Continued Pretraining](https://arxiv.org/abs/2511.14245)
*Kai Tian,Yirong Mao,Wendong Bi,Hanjie Wang,Que Wenhui*

Main category: cs.CL

TL;DR: 该论文通过构建高质量大规模音乐语料和基于参考模型的软评分机制，大幅提升了音乐专用大语言模型的训练及对齐能力，并开发了自动化事实性评测基准，推动了音乐领域LLM的发展。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在通用任务中表现突出，但在音乐娱乐等专用领域受制于训练语料规模、数据纯净度和目标匹配度。本研究旨在通过更优质的音乐语料和任务对齐机制提升音乐领域LLM的性能。

Method: 作者构建了一个包含400亿token的大规模音乐相关自然语言语料库，该语料库整合了开源数据和内部数据。数据管道采用轻量级分类器进行领域内文本筛选与加权，在多阶段清洗、去重和隐私保护处理后集成多源音乐文本及关联元数据。训练方面，创新地引入基于参考模型（RM）的token级软评分质量控制机制，并通过统一损失比准则进行数据选择与动态降权，从而提升优化效果。同时，设计了MusicSimpleQA基准，用于评测模型事实性。

Result: 基于所提出的数据管道和优化机制，研究人员显著提升了音乐领域大模型的预训练效果与任务对齐程度，并通过MusicSimpleQA基准实现了事实性自动化评测，对数据组成的影响也进行了系统比较。最终，在音乐领域的LLM构建方面获得了可观的迭代进展。

Conclusion: 本研究提出了一套可扩展的数据训练框架以及可复用的评测工具，为在音乐领域构建大语言模型（LLMs）提供了有效的方法和资源。其方法对音乐领域大模型的预训练与对齐效果有明显提升。

Abstract: Large language models perform strongly on general tasks but remain constrained in specialized settings such as music, particularly in the music-entertainment domain, where corpus scale, purity, and the match between data and training objectives are critical. We address this by constructing a large, music-related natural language corpus (40B tokens) that combines open source and in-house data, and by implementing a domain-first data pipeline: a lightweight classifier filters and weights in-domain text, followed by multi-stage cleaning, de-duplication, and privacy-preserving masking. We further integrate multi-source music text with associated metadata to form a broader, better-structured foundation of domain knowledge. On the training side, we introduce reference-model (RM)-based token-level soft scoring for quality control: a unified loss-ratio criterion is used both for data selection and for dynamic down-weighting during optimization, reducing noise gradients and amplifying task-aligned signals, thereby enabling more effective music-domain continued pretraining and alignment. To assess factuality, we design the MusicSimpleQA benchmark, which adopts short, single-answer prompts with automated agreement scoring. Beyond the benchmark design, we conduct systematic comparisons along the axes of data composition. Overall, this work advances both the right corpus and the right objective, offering a scalable data-training framework and a reusable evaluation tool for building domain LLMs in the music field.

</details>


### [18] [Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning](https://arxiv.org/abs/2511.14249)
*Rui Liu,Yuan Zhao,Zhenqi Jia*

Main category: cs.CL

TL;DR: Authentic-Dubber通过引入导演-演员互动、情感检索增强和逐步图生成机制，显著提升了自动配音的情感表现力，并在公开数据集上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有自动配音方法过于简化真实配音流程，忽视了导演与演员之间在情感传递上的动态互动，因此无法很好地实现高情感表现力的配音。

Method: 主要方法包括：1) 构建多模态参考素材库，并引入大语言模型以深入理解多模态情感信息；2) 提出基于情感相似性的检索增强机制，从素材库中检索最相关的多模态信息；3) 发展逐步图机制实现情感知识融合，模拟真实的配音流程。

Result: 在V2C Animation基准集上的主客观评测均显示，该模型在情感表达等方面均取得了全面提升。

Conclusion: 本文提出的Authentic-Dubber模型能够更真实地模拟电影配音中导演与演员的互动，显著提升了配音的情感表现力。

Abstract: The automatic movie dubbing model generates vivid speech from given scripts, replicating a speaker's timbre from a brief timbre prompt while ensuring lip-sync with the silent video. Existing approaches simulate a simplified workflow where actors dub directly without preparation, overlooking the critical director-actor interaction. In contrast, authentic workflows involve a dynamic collaboration: directors actively engage with actors, guiding them to internalize the context cues, specifically emotion, before performance. To address this issue, we propose a new Retrieve-Augmented Director-Actor Interaction Learning scheme to achieve authentic movie dubbing, termed Authentic-Dubber, which contains three novel mechanisms: (1) We construct a multimodal Reference Footage library to simulate the learning footage provided by directors. Note that we integrate Large Language Models (LLMs) to achieve deep comprehension of emotional representations across multimodal signals. (2) To emulate how actors efficiently and comprehensively internalize director-provided footage during dubbing, we propose an Emotion-Similarity-based Retrieval-Augmentation strategy. This strategy retrieves the most relevant multimodal information that aligns with the target silent video. (3) We develop a Progressive Graph-based speech generation approach that incrementally incorporates the retrieved multimodal emotional knowledge, thereby simulating the actor's final dubbing process. The above mechanisms enable the Authentic-Dubber to faithfully replicate the authentic dubbing workflow, achieving comprehensive improvements in emotional expressiveness. Both subjective and objective evaluations on the V2C Animation benchmark dataset validate the effectiveness. The code and demos are available at https://github.com/AI-S2-Lab/Authentic-Dubber.

</details>


### [19] [AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR](https://arxiv.org/abs/2511.14255)
*Gabrial Zencha Ashungafac,Mardhiyah Sanni,Busayo Awobade,Alex Gichamba,Tobi Olatunji*

Main category: cs.CL

TL;DR: 作者提出并公布了首个大规模非洲英语口音的多域语音识别评测集，对多类ASR和多模态语音模型进行对比，揭示各自优劣，助力非洲本地化语音AI落地。


<details>
  <summary>Details</summary>
Motivation: 现有语音AI研究和评测多集中于主流英语口音，缺乏针对非洲等多口音、多域场景的公开评测标准，导致技术在本地化和多样性应用中效果未知或不佳。

Method: 构建并公开了一个包含多国家、多口音、多应用场景的非洲英语评测数据集，并使用自发与非自发语音对开源/非开源、单模态/多模态语音识别系统进行系统化对比评测。

Result: 开源ASR模型在自发语音表现较好、面对噪音及非母语对话性能下降；多模态大模型对口音更强健但在领域专有实体识别有挑战；专有模型在干净语音表现优但不同国家/域表现波动大；针对非洲英语微调模型取得较低延迟和有竞争力的准确率，但幻觉问题依然普遍。

Conclusion: 本文介绍了AfriSpeech-MultiBench，这是首个专为非洲100多种英语口音、跨10+国家和7类应用场景开发的评测套件，并基于该套件系统评测了各类语音识别模型及多模态大模型，推动了非洲及类似多样性地区的语音AI评测规范和技术选型的进步。

Abstract: Recent advances in speech-enabled AI, including Google's NotebookLM and OpenAI's speech-to-speech API, are driving widespread interest in voice interfaces globally. Despite this momentum, there exists no publicly available application-specific model evaluation that caters to Africa's linguistic diversity. We present AfriSpeech-MultiBench, the first domain-specific evaluation suite for over 100 African English accents across 10+ countries and seven application domains: Finance, Legal, Medical, General dialogue, Call Center, Named Entities and Hallucination Robustness. We benchmark a diverse range of open, closed, unimodal ASR and multimodal LLM-based speech recognition systems using both spontaneous and non-spontaneous speech conversation drawn from various open African accented English speech datasets. Our empirical analysis reveals systematic variation: open-source ASR models excels in spontaneous speech contexts but degrades on noisy, non-native dialogue; multimodal LLMs are more accent-robust yet struggle with domain-specific named entities; proprietary models deliver high accuracy on clean speech but vary significantly by country and domain. Models fine-tuned on African English achieve competitive accuracy with lower latency, a practical advantage for deployment, hallucinations still remain a big problem for most SOTA models. By releasing this comprehensive benchmark, we empower practitioners and researchers to select voice technologies suited to African use-cases, fostering inclusive voice applications for underserved communities.

</details>


### [20] [Entropy-Guided Reasoning Compression](https://arxiv.org/abs/2511.14258)
*Hourun Zhu,Yang Gao,Wenlong Fei,Jiawei Li,Huashan Sun*

Main category: cs.CL

TL;DR: 本文针对大型推理模型推理链过长的问题，分析并解决了压缩训练中的熵冲突，提出熵引导训练框架，在兼顾准确率的同时极大缩短了推理过程，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然表现出色，但推理链条冗长导致计算成本高和部署困难。现有压缩方法忽视了训练过程中的熵冲突现象，导致模型陷入两难困境，亟需设计兼顾压缩和准确率的新训练方法。

Method: 通过分析推理模型中的熵冲突根源，作者设计了一种在压缩训练过程中动态调整熵控制的训练方法，在熵下降时引导模型生成更高效简洁的推理步骤，在熵上升时增强模型的探索能力。

Result: 在六个数学基准测试中，该方法将推理长度压缩至原来的20%，且准确率保持或超越基线方法，显示出优异的实用价值。

Conclusion: 本文提出的熵引导训练框架能够有效缓解推理模型压缩过程中的熵冲突问题，实现大幅缩短推理链条的同时保持甚至提升推理准确率。

Abstract: Large reasoning models have demonstrated remarkable performance on complex reasoning tasks, yet the excessive length of their chain-of-thought outputs remains a major practical bottleneck due to high computation cost and poor deployability. Existing compression methods have achieved partial success but overlook a crucial phenomenon in the training process -- the entropy conflict. During compression training, entropy decreases, leading to shorter reasoning but limited exploration, while accuracy-oriented objectives increase entropy, lengthening reasoning chains. This can cause the model to get stuck in a local dilemma. Our analysis further reveals the origin of the entropy conflict: many high-entropy tokens are logical connectors that receive larger gradients and are encouraged under the performance objective, while the compression objective simultaneously penalizes these potentially redundant connectors. This opposing pressure creates a direct source of entropy conflict. To address these issues, we adopt an entropy-guided training framework. As entropy descends, the model is guided toward efficient reasoning by encouraging concise thought steps; as entropy rises, exploration is reinforced under the compact reasoning mode to improve robustness. Experiments on six mathematical benchmarks show that our method compresses reasoning length to 20% of the original while maintaining or even surpassing baseline accuracy. Code and models will be released publicly.

</details>


### [21] [Don't Miss the Forest for the Trees: In-Depth Confidence Estimation for LLMs via Reasoning over the Answer Space](https://arxiv.org/abs/2511.14275)
*Ante Wang,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: 论文提出通过语言化概率分布预测促进深度推理，提高置信度估计，在多任务多模型下效果显著，推理过程更贴近人类习惯。


<details>
  <summary>Details</summary>
Motivation: 当前模型的可靠性评估依赖“语言化信心”，但推理策略对信心估计的影响未深入探讨，需要一种能激发更深层推理、提升可靠性的估计方法。

Method: 通过大模型生成语言化概率分布，要求模型考虑所有备选项，并分配置信分数，且方法在已知或未知答案空间、不同任务中均进行验证。

Result: 该方法在各种模型和任务上均显示出优势，且即使经过强化学习优化，效果依然保持，具体推理模式与人类一致。

Conclusion: 以语言化概率分布预测作为信心估计方法，能够有效促进模型进行更深入的推理，并且在不同模型与任务下均表现优异，推理模式与人类期望一致。

Abstract: Knowing the reliability of a model's response is essential in application. With the strong generation capabilities of LLMs, research has focused on generating verbalized confidence. This is further enhanced by combining chain-of-thought reasoning, which provides logical and transparent estimation. However, how reasoning strategies affect the estimated confidence is still under-explored. In this work, we demonstrate that predicting a verbalized probability distribution can effectively encourage in-depth reasoning for confidence estimation. Intuitively, it requires an LLM to consider all candidates within the answer space instead of basing on a single guess, and to carefully assign confidence scores to meet the requirements of a distribution. This method shows an advantage across different models and various tasks, regardless of whether the answer space is known. Its advantage is maintained even after reinforcement learning, and further analysis shows its reasoning patterns are aligned with human expectations.

</details>


### [22] [AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models](https://arxiv.org/abs/2511.14295)
*Mohammad Zbib,Hasan Abed Al Kader Hammoud,Sina Mukalled,Nadine Rizk,Fatima Karnib,Issam Lakkis,Ammar Mohanna,Bernard Ghanem*

Main category: cs.CL

TL;DR: 建立了评估阿拉伯语LLM结构性语言能力的人类标注基准，发现现有模型多靠模式识别而非真实理解，AraLingBench有助于完善模型开发和评测体系。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语LLM评测多聚焦知识类，而缺少对基本语言结构理解的诊断，导致模型高分但缺乏语言实际掌控力。该研究旨在填补这一差距，系统评估和提升阿拉伯语LLM的语言能力。

Method: 构建了AraLingBench基准，包括150道人类专家设计的多项选择题，涵盖语法、形态学、拼写、阅读理解及句法五大类别，对35个阿拉伯语和双语大型语言模型进行评测。

Result: AraLingBench揭示了知识型基准得分高与真实语言能力之间仍有差距，为开发具备真正语言理解能力的阿拉伯语LLM提供了更诊断性框架；评测及源码已公开。

Conclusion: 当前的大型语言模型在阿拉伯语表层语言能力上表现良好，但在深入的语法和句法推理方面存在明显不足；许多模型主要依靠记忆或模式识别而非真正理解。

Abstract: We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.

</details>


### [23] [ConInstruct: Evaluating Large Language Models on Conflict Detection and Resolution in Instructions](https://arxiv.org/abs/2511.14342)
*Xingwei He,Qianru Zhang,Pengfei Chen,Guanhua Chen,Linlin Yu,Yuan Yuan,Siu-Ming Yiu*

Main category: cs.CL

TL;DR: 本文提出了评估LLM指令矛盾处理能力的ConInstruct基准，发现主流模型能检测冲突却缺乏主动提示，未来需增强交互透明度。


<details>
  <summary>Details</summary>
Motivation: 以往关于大型语言模型（LLM）遵循指令的研究，主要关注其是否按用户指令行事，然而现实中复杂指令常常包含矛盾约束，目前对LLM在此类情况下的应对能力仍缺乏研究。为弥补这一研究空白，作者提出了旨在评估LLM处理指令冲突能力的基准测试。

Method: 作者构建了ConInstruct基准，专门用于测试LLM对用户指令中冲突的检测与处理能力，通过该数据集系统评估了多种LLM在冲突检测及冲突化解方面的表现。

Result: （1）多数闭源LLM具有较强冲突检测能力，开源模型中仅DeepSeek-R1表现突出；DeepSeek-R1和Claude-4.5-Sonnet分别以91.5%和87.3%的平均F1分数位居前列。（2）LLM虽能检测到冲突，却极少明确告知用户或请求进一步澄清。

Conclusion: 当前主流LLM虽在识别指令冲突上表现优异，但存在很少主动告知用户或寻求澄清的缺陷，这为提升智能交互与用户体验指明了改进方向。

Abstract: Instruction-following is a critical capability of Large Language Models (LLMs). While existing works primarily focus on assessing how well LLMs adhere to user instructions, they often overlook scenarios where instructions contain conflicting constraints-a common occurrence in complex prompts. The behavior of LLMs under such conditions remains under-explored. To bridge this gap, we introduce ConInstruct, a benchmark specifically designed to assess LLMs' ability to detect and resolve conflicts within user instructions. Using this dataset, we evaluate LLMs' conflict detection performance and analyze their conflict resolution behavior. Our experiments reveal two key findings: (1) Most proprietary LLMs exhibit strong conflict detection capabilities, whereas among open-source models, only DeepSeek-R1 demonstrates similarly strong performance. DeepSeek-R1 and Claude-4.5-Sonnet achieve the highest average F1-scores at 91.5% and 87.3%, respectively, ranking first and second overall. (2) Despite their strong conflict detection abilities, LLMs rarely explicitly notify users about the conflicts or request clarification when faced with conflicting constraints. These results underscore a critical shortcoming in current LLMs and highlight an important area for future improvement when designing instruction-following LLMs.

</details>


### [24] [ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning](https://arxiv.org/abs/2511.14366)
*Hongwei Liu,Junnan Liu,Shudong Liu,Haodong Duan,Yuqiang Li,Mao Su,Xiaohong Liu,Guangtao Zhai,Xinyu Fang,Qianhong Ma,Taolin Zhang,Zihan Ma,Yufeng Zhao,Peiheng Zhou,Linchen Xiao,Wenlong Zhang,Shijie Zhou,Xingjian Ma,Siqi Sun,Jiaye Ge,Meng Li,Yuhong Liu,Jianxin Dong,Jiaying Li,Hui Wu,Hanwen Liang,Jintai Lin,Yanting Wang,Jie Dong,Tong Zhu,Tianfan Fu,Conghui He,Qi Zhang,Songyang Zhang,Lei Bai,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出跨学科高难度科学推理评测集ATLAS，并展示其有区分度地测评现代大模型能力，可为通向AGI提供新标尺。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评测基准普遍趋于“满分”，难以区分前沿模型，大量高难度基准又存在学科单一、答题格式简单、易被数据污染等问题，与真实科学问题不符，因此需新型高质量评测体系。

Method: 由领域专家开发原始题目，多学科、复杂开放式问题设计，采用多阶段专家复审和攻防式测试保障信度，评测时引入LLM专家组自动细致判分。

Result: 开发出涵盖7大学科、约800题目的原创新型高难度评测集ATLAS，通过专家和算法判分，初步结果显示其对模型能力区分度高，拟继续扩展至开放式社区平台。

Conclusion: ATLAS能够有效区分和衡量当今前沿大模型在科学推理等高难度任务上的能力，有助于推动通用人工智能评价标准的发展。

Abstract: The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable "ruler" for progress toward Artificial General Intelligence.

</details>


### [25] [Mitigating Label Length Bias in Large Language Models](https://arxiv.org/abs/2511.14385)
*Mario Sanz-Guerrero,Katharina von der Wense*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.

</details>


### [26] [Unified Defense for Large Language Models against Jailbreak and Fine-Tuning Attacks in Education](https://arxiv.org/abs/2511.14423)
*Xin Yi,Yue Li,Dongsheng Shi,Linlin Wang,Xiaoling Wang,Liang He*

Main category: cs.CL

TL;DR: 本文针对教育场景大语言模型的安全漏洞，构建了EduHarm基准，并提出三阶段防护框架，有效防御攻击同时保证正常问题的处理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在教育场景应用广泛，但面临jailbreak和微调攻击带来的安全风险。而现有研究多关注通用场景的安全性，缺乏针对教育领域的系统性安全评估和防护方案。本文旨在填补这一空白，提升模型在教育应用中的安全适用性。

Method: 1. 构建EduHarm基准数据集，覆盖五种代表性教育场景的安全-不安全指令对，用于系统评估教育类大语言模型的安全性；2. 提出三阶段防护框架（TSSF）：（1）安全感知注意力重定向，区分关键不安全token；（2）分层安全判断，融合多层安全特征识别有害输入；（3）基于防护的双路处理，对安全和不安全请求分别处理，保障良性请求的正常输出和有害请求的安全响应。3. 在8种jailbreak攻击和3个微调攻击数据集上进行实验评估。

Result: 提出的TSSF框架能有效抵御多种jailbreak和微调攻击，显著提升了模型在教育领域的安全性，同时还能保持对正常输入的良好效用。实验证实TSSF在不影响模型有用性的前提下，实现更健壮的攻击防御能力。

Conclusion: 作者提出的三阶段防护框架（TSSF）在防止大语言模型在教育场景下受到jailbreak和微调攻击方面表现出显著效果。不仅提升了模型的安全性，还有效规避了对正常无害请求的过度拒绝。

Abstract: Large Language Models (LLMs) are increasingly integrated into educational applications. However, they remain vulnerable to jailbreak and fine-tuning attacks, which can compromise safety alignment and lead to harmful outputs. Existing studies mainly focus on general safety evaluations, with limited attention to the unique safety requirements of educational scenarios. To address this gap, we construct EduHarm, a benchmark containing safe-unsafe instruction pairs across five representative educational scenarios, enabling systematic safety evaluation of educational LLMs. Furthermore, we propose a three-stage shield framework (TSSF) for educational LLMs that simultaneously mitigates both jailbreak and fine-tuning attacks. First, safety-aware attention realignment redirects attention toward critical unsafe tokens, thereby restoring the harmfulness feature that discriminates between unsafe and safe inputs. Second, layer-wise safety judgment identifies harmfulness features by aggregating safety cues across multiple layers to detect unsafe instructions. Finally, defense-driven dual routing separates safe and unsafe queries, ensuring normal processing for benign inputs and guarded responses for harmful ones. Extensive experiments across eight jailbreak attack strategies demonstrate that TSSF effectively strengthens safety while preventing over-refusal of benign queries. Evaluations on three fine-tuning attack datasets further show that it consistently achieves robust defense against harmful queries while maintaining preserving utility gains from benign fine-tuning.

</details>


### [27] [MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents](https://arxiv.org/abs/2511.14439)
*Jinru Ding,Lu Lu,Chao Ding,Mouxiao Bian,Jiayuan Chen,Renjie Lu,Wenrao Pang,Xiaoqin Wu,Zhiqiang Liu,Luyi Jiang,Bing Han,Yunqiu Wang,Jie Xu*

Main category: cs.CL

TL;DR: MedBench v4建立了覆盖广泛专科的云端医学AI评测平台，揭示多数基础模型存在跨模态推理和安全性短板，而具备治理机制的智能体能显著提升临床应用准备度。研究有助于中国医学AI的合规审查与落地。


<details>
  <summary>Details</summary>
Motivation: 当前医学AI模型与临床实际需求和安全标准未充分对齐，亟需覆盖广泛领域、反映真实临床流程与安全约束的基准测试体系。

Method: 构建云端基准测试基础设施，覆盖24个主要和91个次要专科，超70万个专家任务，经多轮临床专家筛选与匿名审查。采用LLM辅助评分系统并校准至人工标准，对不同类型模型（LLM、多模态、智能体）进行多维评估。

Result: 基础LLM整体得分均值为54.1/100（最佳模型为Claude Sonnet 4.5，得分62.5），安全性和伦理得分仅18.4；多模态模型得分更低（均值47.5/100，GPT-5最佳54.9），但智能体在同样骨干下显著提升（均值79.8/100，Claude Sonnet 4.5为85.3，安全性最高88.9）。平台可用于医院、开发者及政策制定者参考。

Conclusion: MedBench v4揭示了基础LLM和多模态模型在临床安全性等方面存在明显短板，但基于治理机制的智能体显著提升了端到端绩效和安全表现。平台符合中国临床实际，有助于医疗AI审查与落地。

Abstract: Recent advances in medical large language models (LLMs), multimodal models, and agents demand evaluation frameworks that reflect real clinical workflows and safety constraints. We present MedBench v4, a nationwide, cloud-based benchmarking infrastructure comprising over 700,000 expert-curated tasks spanning 24 primary and 91 secondary specialties, with dedicated tracks for LLMs, multimodal models, and agents. Items undergo multi-stage refinement and multi-round review by clinicians from more than 500 institutions, and open-ended responses are scored by an LLM-as-a-judge calibrated to human ratings. We evaluate 15 frontier models. Base LLMs reach a mean overall score of 54.1/100 (best: Claude Sonnet 4.5, 62.5/100), but safety and ethics remain low (18.4/100). Multimodal models perform worse overall (mean 47.5/100; best: GPT-5, 54.9/100), with solid perception yet weaker cross-modal reasoning. Agents built on the same backbones substantially improve end-to-end performance (mean 79.8/100), with Claude Sonnet 4.5-based agents achieving up to 85.3/100 overall and 88.9/100 on safety tasks. MedBench v4 thus reveals persisting gaps in multimodal reasoning and safety for base models, while showing that governance-aware agentic orchestration can markedly enhance benchmarked clinical readiness without sacrificing capability. By aligning tasks with Chinese clinical guidelines and regulatory priorities, the platform offers a practical reference for hospitals, developers, and policymakers auditing medical AI.

</details>


### [28] [Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning](https://arxiv.org/abs/2511.14445)
*Trishala Jayesh Ahalpara*

Main category: cs.CL

TL;DR: Tell Me系统利用大模型构建心理健康辅助平台，集对话、数据扩充与个性化计划于一体，通过技术创新降低心理支持门槛，实验验证有效性并促进跨领域合作。


<details>
  <summary>Details</summary>
Motivation: 情绪健康需求增长，但专业心理疗愈资源有限，且真实情感数据紧缺。希望利用大模型，创新技术，以更低门槛和更智能方式扩大心理健康支持和相关资源的获取范围。

Method: 系统由三大模块组成：（1）用于个性化和知识支撑对话的RAG助手，（2）基于客户画像生成合成客户-治疗师对话以辅助研究与数据扩充，（3）基于CrewAI的Well-being AI团队，能动态制定每周自我关怀计划和生成冥想音频。通过架构展示、功能演示，并在选定情境下以自动化LLM评分和真实用户实验评估系统。

Result: 系统功能涵盖个性化对话、数据合成、主动关怀计划，用示例和实验验证了RAG助手的有效性，展现了AI团队协作下创新、个性化自护流程，有助突破传统工具静态局限。

Conclusion: 本文展示了Tell Me系统作为情感自省空间，能够为用户和研究者提供更便利的情感支持，并证实了基于大模型的对话助手在心理健康领域具备较高的应用潜力。还强调了NLP和心理健康领域间的跨学科合作前景。

Abstract: We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.

</details>


### [29] [Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning](https://arxiv.org/abs/2511.14460)
*Mingyue Cheng,Jie Ouyang,Shuo Yu,Ruiran Yan,Yucong Luo,Zirui Liu,Daoyu Wang,Qi Liu,Enhong Chen*

Main category: cs.CL

TL;DR: 论文提出了系统化的强化学习方法用于大型语言模型Agent，并开发了易扩展的Agent-R1训练框架，在多跳问答任务上初步验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有将强化学习应用于LLM Agent领域的探索不足，缺乏针对性算法和灵活的训练框架，限制了智能体解决复杂交互任务的能力。

Method: 系统扩展马尔可夫决策过程（MDP）以适配LLM Agent，定义其关键组成部分，并设计了Agent-R1框架，进行多任务和多环境的实验验证。

Result: 提出了具有高度模块化和可扩展性的Agent-R1框架，实验证明在Multihop QA等基准任务上，其有效提升了LLM Agent的训练和任务表现。

Conclusion: 本论文提出了将强化学习（RL）方法系统性地应用于大型语言模型（LLM）Agent，并验证了所提出的新型训练框架Agent-R1的有效性。

Abstract: Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.

</details>


### [30] [LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation](https://arxiv.org/abs/2511.14531)
*David Carmel,Simone Filice,Guy Horowitz,Yoelle Maarek,Alex Shtoff,Oren Somekh,Ran Tavory*

Main category: cs.CL

TL;DR: LiveRAG基准集为RAG问答系统评估提供系统性工具，具有题目多样性、区分性和科学量化分析，有利于推动相关研究和系统开发。


<details>
  <summary>Details</summary>
Motivation: RAG在生成式AI中日益重要，但其评估缺乏系统性与标准化，需要一个权威的基准来衡量和对比RAG问答系统的有效性。

Method: 构建并公开了一个包含895个合成问题与答案的数据集，该数据集基于SIGIR'2025 LiveRAG Challenge的竞赛数据，不仅包含问题和答案，还补充了真实答案与支持性论据，并通过项目反应理论模型分析出每道题的难度与区分度。

Result: 实验证明该基准涵盖了问题的多样性，难度分布广泛，能够有效区分不同系统的能力，有助于规范RAG问答系统的评测过程。

Conclusion: LiveRAG基准数据集的提出有助于推动RAG技术的系统性评估，为研究与开发更鲁棒的问答系统提供了有力工具。

Abstract: With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&A systems.

</details>


### [31] [Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak](https://arxiv.org/abs/2511.14566)
*Lucia Makaiová,Martin Fajčík,Antonín Jarolím*

Main category: cs.CL

TL;DR: 本文提出文档级主张集对齐方法评估主张提取性能，并在高挑战性数据集上实验证明现有评价标准不足。


<details>
  <summary>Details</summary>
Motivation: 文档级主张提取在事实核查领域仍是一大挑战，相应的主张评价方法研究不足。缺乏有效的评估框架影响模型性能比较与主张标注质量测评。

Method: 提出并探索对两组同一源文档相关主张的比对与相似性计算方法，通过主张集对齐与打分，作为主张提取模型性能的衡量指标，并可用于人工标注者间一致性的度量。

Result: 提出的主张集对齐和评价框架在捷克语、斯洛伐克语新闻评论中新数据集上进行实验，揭示了当前评价体系的不足，对捕捉复杂语义和主张本质属性存在障碍，呼吁更精细的评价标准和方法。

Conclusion: 目前的评价方法在文档级主张提取任务中存在明显局限性，尤其是在捕捉语义相似性和判断主张关键属性（原子性、可查证性、去语境化）方面不够充分。需要研发更先进的评价框架。

Abstract: Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.

</details>


### [32] [A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease](https://arxiv.org/abs/2511.14603)
*Yilu Fang,Jordan G. Nestor,Casey N. Ta,Jerard Z. Kneifati-Hayek,Chunhua Weng*

Main category: cs.CL

TL;DR: 本研究通过EHR数据挖掘，识别了AKI患者多种临床轨迹及其向CKD进展的风险因素，为高危人群筛选和早期干预提供了数据支撑。


<details>
  <summary>Details</summary>
Motivation: AKI患者向CKD进展风险高，但缺乏有效方法识别高风险亚群，因此需开发精准识别与干预的临床工具。

Method: 利用电子健康记录中的纵向医疗编码和肌酐测量，构建患者时序特征向量，结合聚类方法识别出AKI后的临床状态，并通过多状态模型计算不同状态间转移概率及CKD风险，随后应用生存分析找出不同亚群的关键危险因素。

Result: 在20,699例AKI入院患者中，3,491例(17%)进展为CKD。共识别出15种不同的AKI后状态，各状态转归和CKD风险不同。75%的患者在研究期间未出现多次状态转换。除了传统危险因素外，还发现了新的风险因素，并在不同状态下作用各异。

Conclusion: 通过对电子健康记录(EHR)大规模分析，研究展示了动力学追踪和无监督聚类能揭示AKI患者进展到CKD的路径，为高危人群预警和早期干预提供了新方法。

Abstract: Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.

</details>


### [33] [Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models](https://arxiv.org/abs/2511.14606)
*Shreya Adrita Banik,Niaz Nafi Rahman,Tahsina Moiukh,Farig Sadeque*

Main category: cs.CL

TL;DR: 本文比较了人类和多种LLM在新闻政治偏见检测上的表现，发现RoBERTa与人类标签一致性最佳，GPT零样本一致性最强，强调需结合人类与模型优势。


<details>
  <summary>Details</summary>
Motivation: 尽管NLP技术已能自动分类媒体偏见，但LLM与人类判断的吻合度尚未充分探究，因此本文提出对多模型与人类标注进行对比分析。

Method: 通过构建人工标注的新闻数据集，比较人类标注与多种LLM（GPT、BERT、RoBERTa、FLAN）的偏见检测表现，分析标注一致性、偏见极性及模型间一致性。

Result: 实验结果显示，传统Transformer模型中RoBERTa与人工标注一致性最高，GPT在零样本情况下与人类一致性最强，经微调的RoBERTa表现出最高准确率和一致性。

Conclusion: 本文强调了人类与大型语言模型（LLM）在政治偏见感知上的系统性差异，指出自动化媒体偏见检测需结合人类解释性与模型可扩展性的混合评估框架。

Abstract: Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.

</details>


### [34] [Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities](https://arxiv.org/abs/2511.14631)
*Kahaan Gandhi,Boris Bolliet,Inigo Zubeldia*

Main category: cs.CL

TL;DR: 本研究提出将视觉-语言模型作为评判者引入多智能体系统，用于自动化科学数据探索。案例证明该方法在数据分析过程中具有自纠错和自适应新数据的能力，在相关任务上取得了远优于传统方法的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前科学发现自动化探索存在错误路径恢复能力弱、难以适应新数据集且可解释性不足等问题，旨在用VLM提升多智能体探索的智能性和可靠性。

Method: 将VLM作为裁判，用于对绘图结果进行依据领域专属动态评分标准的评价，使智能体能够自我纠错并实时调整数据探索路径。

Result: 在10项数据驱动科学发现任务基准测试中，VLM增强系统通过率达到0.7-0.8，远高于代码-only（0.2-0.3）、代码+文本（0.4-0.5）方法。同时，系统可提供更具可审计性的推理过程，提升理解与分析能力。

Conclusion: 多智能体系统结合视觉-语言模型（VLMs）能够显著提升科学数据探索的自动化与准确性，相较于传统方法，表现更优且具有更高可解释性。

Abstract: We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent

</details>


### [35] [A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases](https://arxiv.org/abs/2511.14638)
*Tao Yang,Dandan Huang,Yunting Lin,Pengfei Wu,Zhikun Wu,Gangyuan Ma,Yulan Lu,Xinran Dong,Dingpeng Li,Junshuang Ge,Zhiyan Zhang,Xuanzhao Huang,Wenyan Nong,Yao Zhou,Hui Tang,Hongxi Yang,Shijie Zhang,Juan Li,Xiaojun Cao,Lin Yang,Xia Gao,Kaishou Xu,Xiaoqiong Gu,Wen Zhang,Huimin Xia,Li Liu,Wenhao Zhou,Mulin Jun Li*

Main category: cs.CL

TL;DR: RareSeek R1通过融合知识和推理技术，提升了罕见病诊断的准确率和透明度，缩短了诊疗过程，为临床实践提供了可追溯的辅助决策工具。


<details>
  <summary>Details</summary>
Motivation: 罕见病诊断困难，传统流程受限于数据噪音、领域知识陈旧和大模型幻觉。现有医学/通用大模型缺乏高质量电子健康记录(EHR)训练，影响诊断准确性与可推广性，因此需创新集成知识与推理范式。

Method: 作者构建了一个大规模、医学领域专用的临床语料库和医生验证的推理数据集，并通过分阶段的指令微调、思维链学习、图谱增强检索，开发了RareSeek R1系统。

Result: RareSeek R1在多中心EHR和公开基准上表现出领先的准确性与泛化能力，尤以结合临床叙事和遗传变异信息时表现最佳。系统在模拟人类医生辅助环境中与资深医生表现相当，并能逻辑透明地揭示多种非表型证据对诊断的关键作用。

Conclusion: RareSeek R1在罕见病诊断中实现了可靠性高、准确性强的推理，有望缩短漫长的诊疗过程，并为临床决策提供透明、可审计的支持。

Abstract: Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.

</details>


### [36] [Graded strength of comparative illusions is explained by Bayesian inference](https://arxiv.org/abs/2511.14642)
*Yuhan Zhang,Erxiao Wang,Cory Shain*

Main category: cs.CL

TL;DR: 本文通过将统计语言模型与行为数据结合，提出并实证验证了能够精确预测和解释比较幻觉现象强度的噪声通道理论量化模型，扩展了该理论对语言理解幻觉现象的解释力。


<details>
  <summary>Details</summary>
Motivation: 语言中存在类似于视觉加工的幻觉现象，如比较幻觉（CI），即某些无意义的比较句被认为是可接受的。此前理论用噪声通道贝叶斯推断加以解释，但对现象的量化建模和更细粒度效果解释尚不足。本研究旨在改进解释和预测CI幻觉强弱的理论模型。

Method: 作者将统计语言模型与人类行为数据结合，建立了量化模型以预测人们对比较幻觉句子的接受程度，并分析了不同主语形式对幻觉强度的影响。

Result: 所建模型不仅解释了比较幻觉效应强弱的细微差异，还揭示了代词与名词性主语对幻觉强度的新影响。模型预测与实验数据高度吻合，进一步完善和验证了噪声通道理论。

Conclusion: 研究结果支持噪声通道句子理解理论，表明该理论对比较幻觉具有可验证的新预测能力，并能作为多样语言加工现象的统一解释框架。

Abstract: Like visual processing, language processing is susceptible to illusions in which people systematically misperceive stimuli. In one such case--the comparative illusion (CI), e.g., More students have been to Russia than I have--comprehenders tend to judge the sentence as acceptable despite its underlying nonsensical comparison. Prior research has argued that this phenomenon can be explained as Bayesian inference over a noisy channel: the posterior probability of an interpretation of a sentence is proportional to both the prior probability of that interpretation and the likelihood of corruption into the observed (CI) sentence. Initial behavioral work has supported this claim by evaluating a narrow set of alternative interpretations of CI sentences and showing that comprehenders favor interpretations that are more likely to have been corrupted into the illusory sentence. In this study, we replicate and go substantially beyond this earlier work by directly predicting the strength of illusion with a quantitative model of the posterior probability of plausible interpretations, which we derive through a novel synthesis of statistical language models with human behavioral data. Our model explains not only the fine gradations in the strength of CI effects, but also a previously unexplained effect caused by pronominal vs. full noun phrase than-clause subjects. These findings support a noisy-channel theory of sentence comprehension by demonstrating that the theory makes novel predictions about the comparative illusion that bear out empirically. This outcome joins related evidence of noisy channel processing in both illusory and non-illusory contexts to support noisy channel inference as a unified computational-level theory of diverse language processing phenomena.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [37] [Imagine in Space: Exploring the Frontier of Spatial Intelligence and Reasoning Efficiency in Vision Language Models](https://arxiv.org/abs/2511.13782)
*Xiaoxing Lian,Aidong Yang,Jun Zhu,Peng Wang,Yue Zhang*

Main category: cs.AI

TL;DR: 本文系统评测了主流VLM在空间推理上的能力，发现其主要依赖语言表征，难以胜任需要空间感知和3D变换的任务，且推理效率低下。文中提出基于图像驱动的数据框架能提升模型的空间推理能力，为后续改进指出方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模视觉语言模型在逻辑推理和问题求解等方面表现突出，但其在空间推理（如心象旋转、导航、空间关系等）上表现不佳。作者想要分析并解释当前VLM为何空间推理较弱，并探索提升空间推理能力的方法。

Method: 本文设计并引入了SpatiaLite基准，这是一套完全合成的数据集，用于联合评测视觉语言模型的空间推理准确性和推理效率。此外还提出了Imagery Driven Framework（IDF）作为合成训练框架，旨在隐式构建VLMs的空间世界模型。

Result: 实验表明，现有VLM主要依赖语言表达进行想象与推理，导致在需要感知空间关系与3D几何变换的任务上表现明显不足。同时，其空间推理机制在面对复杂空间变换时，Token消耗迅速增长，效率低下。提出的IDF框架可帮助VLM内隐地建立空间世界模型，从而改善空间推理表现。

Conclusion: 本文通过SpatiaLite基准测试，系统性地揭示了当前先进视觉语言模型（VLMs）在空间推理能力上的不足，主要表现为对视觉中心任务的处理缺陷和效率低下，提出的Imagery Driven Framework能在一定程度上提升VLMs的空间推理性能。

Abstract: Large language models (LLMs) and vision language models (VLMs), such as DeepSeek R1,OpenAI o3, and Gemini 2.5 Pro, have demonstrated remarkable reasoning capabilities across logical inference, problem solving, and decision making. However, spatial reasoning:a fundamental component of human cognition that includes mental rotation, navigation, and spatial relationship comprehension remains a significant challenge for current advanced VLMs. We hypothesize that imagination, the internal simulation of spatial states, is the dominant reasoning mechanism within a spatial world model. To test this hypothesis and systematically probe current VLM spatial reasoning mechanisms, we introduce SpatiaLite, a fully synthetic benchmark that jointly measures spatial reasoning accuracy and reasoning efficiency. Comprehensive experiments reveal three key findings. First, advanced VLMs predominantly rely on linguistic representations for reasoning and imagination, resulting in significant deficiencies on visual centric tasks that demand perceptual spatial relations and 3D geometry transformations such as mental rotation or projection prediction. Second, advanced VLMs exhibit severe inefficiency in their current spatial reasoning mechanisms, with token usage growing rapidly as transformation complexity increases. Third, we propose an Imagery Driven Framework (IDF) for data synthesis and training, which can implicitly construct an internal world model that is critical for spatial reasoning in VLMs. Building on SpatiaLite, this work delineates the spatial reasoning limits and patterns of advanced VLMs, identifies key shortcomings, and informs future advances

</details>


### [38] [KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention for 3D Modeling of Complex Structures](https://arxiv.org/abs/2511.13798)
*Mohammad Reza Shafie,Morteza Hajiabadi,Hamed Khosravi,Mobina Noori,Imtiaz Ahmed*

Main category: cs.AI

TL;DR: KANGURA是一种结合函数分解和空间感知的新型3D学习框架，有效提升了对复杂结构的建模和预测能力，在标准数据集和实际MFC应用中均取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前微生物燃料电池（MFC）阳极优化受限于预测模型无法有效捕捉复杂几何结构依赖关系，制约了性能提升和结构优化。

Method: 采用基于Kolmogorov-Arnold Network (KAN)的函数分解方法，实现3D结构的几何关系重建；引入几何解耦表示学习和统一注意力机制以提升对关键结构特征的识别与建模能力。

Result: KANGURA框架在ModelNet40数据集上取得92.7%准确率，并在实际MFC阳极结构识别中取得97%准确率，远超现有主流模型。

Conclusion: KANGURA框架在3D几何建模任务中表现出色，不仅在标准数据集（ModelNet40）上优于15种主流方法，还能高效解决实际MFC阳极结构问题，具有广泛应用前景。

Abstract: Microbial Fuel Cells (MFCs) offer a promising pathway for sustainable energy generation by converting organic matter into electricity through microbial processes. A key factor influencing MFC performance is the anode structure, where design and material properties play a crucial role. Existing predictive models struggle to capture the complex geometric dependencies necessary to optimize these structures. To solve this problem, we propose KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention. KANGURA introduces a new approach to three-dimensional (3D) machine learning modeling. It formulates prediction as a function decomposition problem, where Kolmogorov-Arnold Network (KAN)- based representation learning reconstructs geometric relationships without a conventional multi- layer perceptron (MLP). To refine spatial understanding, geometry-disentangled representation learning separates structural variations into interpretable components, while unified attention mechanisms dynamically enhance critical geometric regions. Experimental results demonstrate that KANGURA outperforms over 15 state-of-the-art (SOTA) models on the ModelNet40 benchmark dataset, achieving 92.7% accuracy, and excels in a real-world MFC anode structure problem with 97% accuracy. This establishes KANGURA as a robust framework for 3D geometric modeling, unlocking new possibilities for optimizing complex structures in advanced manufacturing and quality-driven engineering applications.

</details>


### [39] [Causal computations in Semi Markovian Structural Causal Models using divide and conquer](https://arxiv.org/abs/2511.13852)
*Anna Rodum Bjøru,Rafael Cabañas,Helge Langseth,Antonio Salmerón*

Main category: cs.AI

TL;DR: 本文探讨如何将基于分治的反事实概率界定算法从马尔可夫型推广到能刻画混杂的半马尔可夫型结构因果模型，并提出多种新策略，经理论和实验评估。


<details>
  <summary>Details</summary>
Motivation: 由于原有分治算法仅适用于马尔可夫型SCM，而现实中存在因果混杂的半马尔可夫模型，为丰富因果推断工具，需将该法扩展适用于更一般的情况。

Method: 在理论分析的基础上，作者通过最小例子分析半马尔可夫模型下分治法的挑战，提出一系列新的应对策略，并进行了理论和计算实验评估。

Result: 通过理论和实验，作者展示了备选策略在半马尔可夫SCM中的表现，指出原有算法应用扩展时遇到的难题及潜在的解决路径。

Conclusion: 本文对Bjøru等人提出的分治算法在半马尔可夫结构因果模型(SCMs)中的适用性进行了探讨，并提出了应对挑战的备选策略。作者分析并评估了这些策略的理论基础和计算表现。

Abstract: Recently, Bjøru et al. proposed a novel divide-and-conquer algorithm for bounding counterfactual probabilities in structural causal models (SCMs). They assumed that the SCMs were learned from purely observational data, leading to an imprecise characterization of the marginal distributions of exogenous variables. Their method leveraged the canonical representation of structural equations to decompose a general SCM with high-cardinality exogenous variables into a set of sub-models with low-cardinality exogenous variables. These sub-models had precise marginals over the exogenous variables and therefore admitted efficient exact inference. The aggregated results were used to bound counterfactual probabilities in the original model. The approach was developed for Markovian models, where each exogenous variable affects only a single endogenous variable. In this paper, we investigate extending the methodology to \textit{semi-Markovian} SCMs, where exogenous variables may influence multiple endogenous variables. Such models are capable of representing confounding relationships that Markovian models cannot. We illustrate the challenges of this extension using a minimal example, which motivates a set of alternative solution strategies. These strategies are evaluated both theoretically and through a computational study.

</details>


### [40] [Jailbreaking Large Vision Language Models in Intelligent Transportation Systems](https://arxiv.org/abs/2511.13892)
*Badhan Chandra Das,Md Tasnim Jawad,Md Jueal Mia,M. Hadi Amini,Yanzhao Wu*

Main category: cs.AI

TL;DR: 本文揭示了大视觉语言模型在智能交通系统应用中的越狱安全风险，提出图像文案与多轮交互联合的创新攻击方式，并用多层次过滤防御提升安全性。研究结果表明，当前LVLMs在面对高级越狱攻击时依然存在明显安全漏洞。


<details>
  <summary>Details</summary>
Motivation: LVLMs在实际交通应用中具备强大的多模态推理能力，但其易受越狱攻击，可能导致模型输出违规或有害信息，亟需分析其安全漏洞并提出有效防御方案。

Method: （1）构建针对交通领域的有害查询数据集，参照OpenAI的禁令类别；（2）提出基于图像文案操控和对话多轮引导的新型越狱攻击方法；（3）提出多层次响应过滤防御技术；（4）采用GPT-4和人工评审结合评测攻击有效性和防御性能，并与现有越狱方法做对比。

Result: 所提新型越狱攻击方法在SOTA LVLMs上效果显著，利用图像排版和多轮交互手法可绕过现有安全措施。所提多层次响应过滤防御对减少不当回答有较好的效果，但攻击和对抗仍是持续性问题。此外，研究揭示了交通相关LVLMs在处理复杂攻击时的安全短板。

Conclusion: LVLMs集成在智能交通系统中面临通过图像文字操作和多轮提示进行的越狱攻击，现有系统存在严重安全隐患。多层次的结果过滤可以提升防御效果，但模型仍需持续优化以应对新型攻击。

Abstract: Large Vision Language Models (LVLMs) demonstrate strong capabilities in multimodal reasoning and many real-world applications, such as visual question answering. However, LVLMs are highly vulnerable to jailbreaking attacks. This paper systematically analyzes the vulnerabilities of LVLMs integrated in Intelligent Transportation Systems (ITS) under carefully crafted jailbreaking attacks. First, we carefully construct a dataset with harmful queries relevant to transportation, following OpenAI's prohibited categories to which the LVLMs should not respond. Second, we introduce a novel jailbreaking attack that exploits the vulnerabilities of LVLMs through image typography manipulation and multi-turn prompting. Third, we propose a multi-layered response filtering defense technique to prevent the model from generating inappropriate responses. We perform extensive experiments with the proposed attack and defense on the state-of-the-art LVLMs (both open-source and closed-source). To evaluate the attack method and defense technique, we use GPT-4's judgment to determine the toxicity score of the generated responses, as well as manual verification. Further, we compare our proposed jailbreaking method with existing jailbreaking techniques and highlight severe security risks involved with jailbreaking attacks with image typography manipulation and multi-turn prompting in the LVLMs integrated in ITS.

</details>


### [41] [CORGI: Efficient Pattern Matching With Quadratic Guarantees](https://arxiv.org/abs/2511.13942)
*Daniel Weitekamp*

Main category: cs.AI

TL;DR: 针对高延迟和高空间复杂度的规则匹配难题，本文提出并验证了新算法CORGI，其能以更低资源开销快速匹配，适合自动学习产生规则的实时智能系统。


<details>
  <summary>Details</summary>
Motivation: 当前基于规则的系统如AI规划、数据库低延迟查询等对实时性和复杂模式匹配有极高需求。然而现有如RETE等主流匹配引擎遇到变量较多或部分约束不严的模式时，容易引发指数级的时间和空间复杂度，影响系统可用性。

Method: 提出并实现了新的匹配算法CORGI，其机制为：先正向构建/维护一个基于关系的图，然后通过逆向迭代遍历该图以按需生成匹配项，不用存储完整冲突集。与RETE基于β-记忆收集部分匹配不同，CORGI避免了高空间消耗和中间态爆炸。

Result: 实验证明CORGI算法在简单组合型匹配任务上明显优于SOAR、OPS5等RETE实现，达到了更优的时间效率和空间利用。

Conclusion: 本文提出的CORGI匹配算法较传统RETE算法在复杂匹配问题上能更有效地避免高延迟和内存溢出，特别适合实时性要求高的系统。

Abstract: Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $β$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.

</details>


### [42] [Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios](https://arxiv.org/abs/2511.13970)
*Sanjay Acharjee,Abir Khan Ratul,Diego Patino,Md Nazmus Sakib*

Main category: cs.AI

TL;DR: 本文提出用场景图和生成式AI自动合成基于历史事故的危险场景图像，并用新设计的VQA Graph Score有效验证其语义和真实度，优于现有主流指标。


<details>
  <summary>Details</summary>
Motivation: 准确检测安全隐患需要真实、具有危险性的场景图像数据，但现实中事故场景难以拍摄，数据采集受限。为解决该难题，亟需高质量合成危险场景图像的可用方法。

Method: 采用GPT-4o分析OSHA历史事故叙述并抽取结构化信息，转化为对象级场景图，随后利用扩散模型在场景图引导下生成与真实工地危险场景高度一致的图像。最后，通过设计视觉问答（VQA）系统引入VQA Graph Score作为评价指标，并在多种生成模型上与CLIP/BLIP对比评估。

Result: 提出方法能够根据历史事故数据自动生成符合语义与空间关系、视觉真实的危险场景图像，并在VQA Graph Score评价下显示出比CLIP和BLIP更好的判别性能和语义一致性。

Conclusion: 文中提出的场景图引导生成AI框架能够合成高度真实且具有危险语义的场景图像，并通过新的VQA Graph Score指标确定生成数据的高区分度和语义一致性。

Abstract: Training vision models to detect workplace hazards accurately requires realistic images of unsafe conditions that could lead to accidents. However, acquiring such datasets is difficult because capturing accident-triggering scenarios as they occur is nearly impossible. To overcome this limitation, this study presents a novel scene graph-guided generative AI framework that synthesizes photorealistic images of hazardous scenarios grounded in historical Occupational Safety and Health Administration (OSHA) accident reports. OSHA narratives are analyzed using GPT-4o to extract structured hazard reasoning, which is converted into object-level scene graphs capturing spatial and contextual relationships essential for understanding risk. These graphs guide a text-to-image diffusion model to generate compositionally accurate hazard scenes. To evaluate the realism and semantic fidelity of the generated data, a visual question answering (VQA) framework is introduced. Across four state-of-the-art generative models, the proposed VQA Graph Score outperforms CLIP and BLIP metrics based on entropy-based validation, confirming its higher discriminative sensitivity.

</details>


### [43] [Artificial Intelligence Agents in Music Analysis: An Integrative Perspective Based on Two Use Cases](https://arxiv.org/abs/2511.13987)
*Antonio Manuel Martínez-Heredia,Dolores Godrid Rodríguez,Andrés Ortiz García*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents an integrative review and experimental validation of artificial intelligence (AI) agents applied to music analysis and education. We synthesize the historical evolution from rule-based models to contemporary approaches involving deep learning, multi-agent architectures, and retrieval-augmented generation (RAG) frameworks. The pedagogical implications are evaluated through a dual-case methodology: (1) the use of generative AI platforms in secondary education to foster analytical and creative skills; (2) the design of a multiagent system for symbolic music analysis, enabling modular, scalable, and explainable workflows.
  Experimental results demonstrate that AI agents effectively enhance musical pattern recognition, compositional parameterization, and educational feedback, outperforming traditional automated methods in terms of interpretability and adaptability. The findings highlight key challenges concerning transparency, cultural bias, and the definition of hybrid evaluation metrics, emphasizing the need for responsible deployment of AI in educational environments.
  This research contributes to a unified framework that bridges technical, pedagogical, and ethical considerations, offering evidence-based guidance for the design and application of intelligent agents in computational musicology and music education.

</details>


### [44] [ALEX:A Light Editing-knowledge Extractor](https://arxiv.org/abs/2511.14018)
*Minghu Wang,Shuliang Zhao,Yuanyuan Zhao,Hongxia Xu*

Main category: cs.AI

TL;DR: ALEX框架用层次化记忆结构提高了大模型知识编辑的效率与准确性，实验证明其能极大提升复杂推理问答的表现，并显著降低检索成本。


<details>
  <summary>Details</summary>
Motivation: 大模型知识静态，难以应对知识更新需求，且现有知识编辑方法在处理多步复杂问题时存在扩展性和检索效率瓶颈。

Method: 提出了层次化内存结构，将知识编辑按语义聚类，结合推理式查询生成模块与动态证据裁决引擎，实现高效双阶段检索。

Result: 在MQUAKE基准上，ALEX提升了多跳问答的准确度和推理链可靠性，并将检索空间缩小80%以上，展示了高可扩展、高效且精确的知识编辑能力。

Conclusion: ALEX显著提升了多跳问答的准确率和推理路径的可靠性，同时大幅压缩检索空间，为大模型的知识编辑带来了更高的可扩展性和效率。

Abstract: The static nature of knowledge within Large Language Models (LLMs) makes it difficult for them to adapt to evolving information, rendering knowledge editing a critical task. However, existing methods struggle with challenges of scalability and retrieval efficiency, particularly when handling complex, multi-hop questions that require multi-step reasoning. To address these challenges, this paper introduces ALEX (A Light Editing-knowledge Extractor), a lightweight knowledge editing framework. The core innovation of ALEX is its hierarchical memory architecture, which organizes knowledge updates (edits) into semantic clusters. This design fundamentally reduces retrieval complexity from a linear O(N) to a highly scalable O(K+N/C). Furthermore, the framework integrates an Inferential Query Synthesis (IQS) module to bridge the semantic gap between queries and facts , and a Dynamic Evidence Adjudication (DEA) engine that executes an efficient two-stage retrieval process. Experiments on the MQUAKE benchmark demonstrate that ALEX significantly improves both the accuracy of multi-hop answers (MultiHop-ACC) and the reliability of reasoning paths (HopWise-ACC). It also reduces the required search space by over 80% , presenting a promising path toward building scalable, efficient, and accurate knowledge editing systems.

</details>


### [45] [Syn-STARTS: Synthesized START Triage Scenario Generation Framework for Scalable LLM Evaluation](https://arxiv.org/abs/2511.14023)
*Chiharu Hagiwara,Naoki Nonaka,Yuhta Hashimoto,Ryu Uchimido,Jun Seita*

Main category: cs.AI

TL;DR: 提出并论证了利用LLM生成高质量医疗分诊合成数据的方案，有效缓解真实数据匮乏难题，对促进AI分诊系统发展意义重大。


<details>
  <summary>Details</summary>
Motivation: 大规模伤亡事件(MCIs)的数据稀缺阻碍了AI分诊系统的发展，高质量、足量的标注数据难以获取，因此亟需采用新方法生成可靠的模拟分诊数据用于AI模型的开发和评估。

Method: 开发了Syn-STARTS框架，利用大型语言模型(LLMs)生成大规模分诊案例，并通过与手工整理的TRIAGE开源数据集进行质量对比，进一步基于大量标准分诊等级案例评估LLM表现的准确性和稳定性。

Result: 生成的模拟案例与人工数据集在质量上无显著差异。基于START分诊标准各等级的几百案例中，LLM判别准确性极高，数据表现稳健，支持合成数据可用于AI模型训练。

Conclusion: 生成的Syn-STARTS模拟分诊案例在质量和实际手工整理数据几乎无法区分，验证了其在应对真实大规模伤亡事件数据不足问题时的有效性。合成数据有助于推进AI在紧急医疗分诊领域的应用。

Abstract: Triage is a critically important decision-making process in mass casualty incidents (MCIs) to maximize victim survival rates. While the role of AI in such situations is gaining attention for making optimal decisions within limited resources and time, its development and performance evaluation require benchmark datasets of sufficient quantity and quality. However, MCIs occur infrequently, and sufficient records are difficult to accumulate at the scene, making it challenging to collect large-scale realworld data for research use. Therefore, we developed Syn-STARTS, a framework that uses LLMs to generate triage cases, and verified its effectiveness. The results showed that the triage cases generated by Syn-STARTS were qualitatively indistinguishable from the TRIAGE open dataset generated by manual curation from training materials. Furthermore, when evaluating the LLM accuracy using hundreds of cases each from the green, yellow, red, and black categories defined by the standard triage method START, the results were found to be highly stable. This strongly indicates the possibility of synthetic data in developing high-performance AI models for severe and critical medical situations.

</details>


### [46] [Making Evidence Actionable in Adaptive Learning](https://arxiv.org/abs/2511.14052)
*Amirreza Mehrabi,Jason W. Morphew,Breejha Quezada,N. Sanjay Rebello*

Main category: cs.AI

TL;DR: 本文提出了一种兼具充分性、时间预算和干预多样性的自适应学习反馈系统，基于概念级诊断和优化分配算法，实现了大规模课堂环境下的负载感知公平特性干预，显著提升了教学精准性和系统可用性。


<details>
  <summary>Details</summary>
Motivation: 当前自适应学习系统反馈与干预偏弱，存在响应时间、内容匹配度和资源利用单一的问题，需要更精细且可审计的算法实现高效、均衡的微粒度干预。

Method: 采用基于二元整数规划的干预分配建模，结合贪心、梯度松弛及混合求解方法，利用概念矩阵编码先修知识、难度窗口限制与去冗余机制。通过仿真及1204名学生的物理课程部署进行验证。

Result: 两类求解器都能实现在限定观看时长下的技能全覆盖，梯度法能减少约12%的冗余并优化难度分配，而贪心法更适合资源匮乏场景；Slack变量支持靶向内容优化，整体系统兼具可操作性和公平个性化能力。

Conclusion: 本文提出的自适应学习干预反馈系统能够在实际教学中有效闭合诊断-教学环路，实现了更均衡的个性化干预，保障了技能覆盖和学习公平性，算法针对不同场景表现出良好的可行性与可解释性。

Abstract: Adaptive learning often diagnoses precisely yet intervenes weakly, yielding help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted micro-interventions. The adaptive learning algorithm contains three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted constraint for time and redundancy, and diversity as protection against overfitting to a single resource. We formalize intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows informed by ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy enforced through diversity. Greedy selection serves low-richness and tight-latency regimes, gradient-based relaxation serves rich repositories, and a hybrid method transitions along a richness-latency frontier. In simulation and in an introductory physics deployment with one thousand two hundred four students, both solvers achieved full skill coverage for essentially all learners within bounded watch time. The gradient-based method reduced redundant coverage by approximately twelve percentage points relative to greedy and harmonized difficulty across slates, while greedy delivered comparable adequacy with lower computational cost in scarce settings. Slack variables localized missing content and supported targeted curation, sustaining sufficiency across subgroups. The result is a tractable and auditable controller that closes the diagnostic-pedagogical loop and delivers equitable, load-aware personalization at classroom scale.

</details>


### [47] [APD-Agents: A Large Language Model-Driven Multi-Agents Collaborative Framework for Automated Page Design](https://arxiv.org/abs/2511.14101)
*Xinpeng Chen,Xiaofeng Han,Kaihao Zhang,Guochao Ren,Yujie Wang,Wenhao Cao,Yang Zhou,Jianfeng Lu,Zhenbo Song*

Main category: cs.AI

TL;DR: 该论文提出了基于大语言模型的多智能体自动页面设计系统APD-agents，在移动App页面布局自动化上实现了领先性能，有效解决了设计耗时、协同难和软件学习门槛高的问题。


<details>
  <summary>Details</summary>
Motivation: 移动应用页面的布局设计通常非常耗时，设计师不仅要考虑控件和内容的选择，还要不断调整尺寸、位置和风格以达到美观和结构化的要求。虽然存在辅助设计的软件，但学习成本高且跨页面协作难以标准统一。

Method: 提出了一套由大语言模型驱动的多智能体（APD-agents）自动化页面设计框架。该框架包含OrchestratorAgent、SemanticParserAgent、PrimaryLayoutAgent、TemplateRetrievalAgent和RecursiveComponentAgent，每个智能体负责特定模块。通过OrchestratorAgent动态协调其他智能体，联合完成从描述解析、初步布局、模板检索到细粒度元素生成的整个设计流程。

Result: 在RICO数据集上进行了实验，APD-agents展现出优异的性能，达到了自动化页面布局设计的最新水平。

Conclusion: APD-agents能够自动地将用户页面描述高效转化为结构良好、风格统一的移动应用页面布局，显著简化了传统人工设计与协作流程，实现了多智能体系统协同设计的可能性。

Abstract: Layout design is a crucial step in developing mobile app pages. However, crafting satisfactory designs is time-intensive for designers: they need to consider which controls and content to present on the page, and then repeatedly adjust their size, position, and style for better aesthetics and structure. Although many design software can now help to perform these repetitive tasks, extensive training is needed to use them effectively. Moreover, collaborative design across app pages demands extra time to align standards and ensure consistent styling. In this work, we propose APD-agents, a large language model (LLM) driven multi-agent framework for automated page design in mobile applications. Our framework contains OrchestratorAgent, SemanticParserAgent, PrimaryLayoutAgent, TemplateRetrievalAgent, and RecursiveComponentAgent. Upon receiving the user's description of the page, the OrchestratorAgent can dynamically can direct other agents to accomplish users' design task. To be specific, the SemanticParserAgent is responsible for converting users' descriptions of page content into structured data. The PrimaryLayoutAgent can generate an initial coarse-grained layout of this page. The TemplateRetrievalAgent can fetch semantically relevant few-shot examples and enhance the quality of layout generation. Besides, a RecursiveComponentAgent can be used to decide how to recursively generate all the fine-grained sub-elements it contains for each element in the layout. Our work fully leverages the automatic collaboration capabilities of large-model-driven multi-agent systems. Experimental results on the RICO dataset show that our APD-agents achieve state-of-the-art performance.

</details>


### [48] [Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation](https://arxiv.org/abs/2511.14131)
*Yu Zhong,Zihao Zhang,Rui Zhang,Lingdong Huang,Haihan Gao,Shuo Wang,Da Li,Ruijian Han,Jiaming Guo,Shaohui Peng,Di Huang,Yunji Chen*

Main category: cs.AI

TL;DR: R3为视觉语言导航任务提出创新的双系统思维框架，融合大语言模型推理优势和专家模型高效性，实验显著优于现有方法，解决了效率与空间推理瓶颈。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然具备推理与泛化能力，但在复杂空间理解和实际推理效率上仍显不足，同时引入此类模型会带来高计算代价和延迟；因此亟需一种结合大语言模型泛化与领域专家高效性的创新框架。

Method: 论文构建了一个包含Runner、Ruminator和Regulator三大模块的双系统思维框架：Runner为高效精准的轻量专家模型处理常规任务，Ruminator为具备结构化推理能力的强大多模态大语言模型，Regulator负责监控和切换任务执行的思维模式，从而实现高效与复杂推理的兼顾。整个系统以零样本（zero-shot）方式集成。

Result: R3框架在REVERIE基准任务上取得了比当前最优方法更优的指标表现，显著提升了在挑战性VLN任务的完成率与导航效率。

Conclusion: 提出的R3框架在视觉-语言导航任务中显著提升了导航表现，优于现有主流方法，在REVERIE基准上SPL和RGSPL分别提升3.28%和3.30%。

Abstract: Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.

</details>


### [49] [Beyond Accuracy: A Multi-Dimensional Framework for Evaluating Enterprise Agentic AI Systems](https://arxiv.org/abs/2511.14136)
*Sushant Mehta*

Main category: cs.AI

TL;DR: 该论文指出当前AI agent评测方法无法满足企业需求，提出CLEAR框架系统性解决了评估中的成本控制、可靠性等问题，并通过实证和专家评估验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 推动AI agent评估从单一的准确率标准转向多维度、企业导向的综合评测，解决当前评测缺失成本控制、可靠性和多指标问题。

Method: 系统性分析了12个主流基准，并实证测评了当前最先进的AI agent，提出并验证了CLEAR（成本、时延、效能、保障、可靠性）评估框架。通过对6个主流agent在300个企业任务上的测评，并结合专家评价验证其效果。

Result: 以准确率为唯一优化目标的agent相比于考虑成本的agent，价格高4.4-10.8倍但性能相近。专家评价显示CLEAR框架在预测agent生产成功率方面优于仅看准确率的方法。

Conclusion: 传统的AI agent评测方法主要关注任务完成的准确率，却忽略了企业级需求如成本、稳定性和可靠性。新提出的CLEAR框架能更全面地评估agent，预测其在企业生产中的实际表现。

Abstract: Current agentic AI benchmarks predominantly evaluate task completion accuracy, while overlooking critical enterprise requirements such as cost-efficiency, reliability, and operational stability. Through systematic analysis of 12 main benchmarks and empirical evaluation of state-of-the-art agents, we identify three fundamental limitations: (1) absence of cost-controlled evaluation leading to 50x cost variations for similar precision, (2) inadequate reliability assessment where agent performance drops from 60\% (single run) to 25\% (8-run consistency), and (3) missing multidimensional metrics for security, latency, and policy compliance. We propose \textbf{CLEAR} (Cost, Latency, Efficacy, Assurance, Reliability), a holistic evaluation framework specifically designed for enterprise deployment. Evaluation of six leading agents on 300 enterprise tasks demonstrates that optimizing for accuracy alone yields agents 4.4-10.8x more expensive than cost-aware alternatives with comparable performance. Expert evaluation (N=15) confirms that CLEAR better predicts production success (correlation $ρ=0.83$) compared to accuracy-only evaluation ($ρ=0.41$).

</details>


### [50] [Do Large Language Models (LLMs) Understand Chronology?](https://arxiv.org/abs/2511.14214)
*Pattaraphon Kenny Wongchamcharoen,Paul Glasserman*

Main category: cs.AI

TL;DR: 本文系统测试了LLM在时间排序等任务的能力，发现明确推理预算能大幅提高效果。高推理强度下的新一代模型（如GPT-5）对年代排序和条件排序表现接近完美，但模型仍对输入长度和任务复杂度敏感，对实时金融应用具有现实启示。


<details>
  <summary>Details</summary>
Motivation: 当前金融和经济领域越来越多使用大语言模型，但大量研究默认模型能够正确理解时间顺序，忽略了模型在处理时序排序任务时的实际能力与局限，因此本文系统性检验LLMs对时间、顺序等相关任务的能力。

Method: 本文设计了三类逐步增加复杂度的时序排序任务：（1）年代顺序排序；（2）条件排序（需先过滤再排序）；（3）时代错误检测。评估对象为GPT-4.1、Claude-3.7 Sonnet（含/不含Extended Thinking）、GPT-5，且实验覆盖多种推理努力强度，通过精确匹配率和秩相关性等指标分析表现差异。

Result: 随着序列长度增加，准确匹配率急剧下降，但局部排序仍较好，表明模型能够保留局部顺序但难以全局一致排序。在条件排序任务中，主要失败原因是过滤环节而非排序本身；但GPT-5和强化推理设置下的Claude-3.7 Sonnet表现优异。时代错误识别相对容易，但若时间线或实体高度重叠，模型性能仍会下降。中强/高推理强度下的GPT-5能实现几乎完美的排序和过滤。

Conclusion: 分配明确的推理预算能显著提升LLMs在年代顺序等排序类任务中的表现，尤其是在使用GPT-5和高推理强度时可达几乎完美效果。不同模型与不同推理设置在处理更长序列和复杂任务时表现分化，揭示了当前大模型的能力边界。

Abstract: Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low/minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.

</details>


### [51] [Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation](https://arxiv.org/abs/2511.14219)
*Kumud Tripathi,Aditya Srinivas Menon,Aman Gaurav,Raj Prakash Gohil,Pankaj Wasnik*

Main category: cs.AI

TL;DR: 本文针对Whisper语音识别模型在嘈杂环境下幻觉频发问题，提出基于适应性层注意力和多目标知识蒸馏的两阶段方法，有效提升鲁棒性并降低错误。


<details>
  <summary>Details</summary>
Motivation: 虽然Whisper等ASR模型在多语种和零样本场景中表现优异，但在嘈杂环境下常出现幻觉错误，现有方法多关注于前后处理，鲜有对模型本体直接改进。作者旨在直接提升模型结构以根本缓解幻觉问题。

Method: 提出两阶段架构：第一阶段通过适应性层注意力（ALA），对编码器层做关联性分析并组块，将多头注意力用于块间特征融合，提高鲁棒性；第二阶段通过多目标知识蒸馏（KD），用教师模型（处理干净输入）指导学生模型（处理噪声输入）对齐语义和注意分布，抑制幻觉。

Result: 在多个嘈杂语音基准上，提出的方法显著降低了幻觉和词错误率，并在干净语音上保持了执行性能。

Conclusion: 引入适应性层注意力（ALA）和多目标知识蒸馏（KD）能够有效提升Whisper模型在嘈杂环境下的鲁棒性，并降低幻觉（hallucination）和词错误率，同时不影响干净语音的识别效果。

Abstract: The Whisper model, an open-source automatic speech recognition system, is widely adopted for its strong performance across multilingual and zero-shot settings. However, it frequently suffers from hallucination errors, especially under noisy acoustic conditions. Previous works to reduce hallucinations in Whisper-style ASR systems have primarily focused on audio preprocessing or post-processing of transcriptions to filter out erroneous content. However, modifications to the Whisper model itself remain largely unexplored to mitigate hallucinations directly. To address this challenge, we present a two-stage architecture that first enhances encoder robustness through Adaptive Layer Attention (ALA) and further suppresses hallucinations using a multi-objective knowledge distillation (KD) framework. In the first stage, ALA groups encoder layers into semantically coherent blocks via inter-layer correlation analysis. A learnable multi-head attention module then fuses these block representations, enabling the model to jointly exploit low- and high-level features for more robust encoding. In the second stage, our KD framework trains the student model on noisy audio to align its semantic and attention distributions with a teacher model processing clean inputs. Our experiments on noisy speech benchmarks show notable reductions in hallucinations and word error rates, while preserving performance on clean speech. Together, ALA and KD offer a principled strategy to improve Whisper's reliability under real-world noisy conditions.

</details>


### [52] [DevPiolt: Operation Recommendation for IoT Devices at Xiaomi Home](https://arxiv.org/abs/2511.14227)
*Yuxiang Wang,Siwen Wang,Haowei Han,Ao Wang,Boya Liu,Yong Zhao,Chengbo Wu,Bin Zhu,Bin Qin,Xiaokai Zhou,Xiao Yan,Jiawei Jiang,Bo Du*

Main category: cs.AI

TL;DR: DevPiolt通过大模型方法，显著提升IoT设备操作推荐的准确性和实用性，已在大型平台成功落地，并取得明显业绩提升。


<details>
  <summary>Details</summary>
Motivation: 现有推荐模型难以应对IoT设备操作中的复杂逻辑、多样化用户偏好，以及对不良建议的敏感性，限制了设备智能操作的普及和提升空间。

Method: 1. 持续预训练和多任务微调的方法赋予大模型领域知识；2. 利用直接偏好优化以对齐用户个性化需求；3. 设计置信度控制机制以减少低质量建议带来的负面体验。

Result: 离线实验中DevPiolt在各类指标上平均提升69.5%；实际部署于小米家App，覆盖25.5万日活用户，在线实验显示设备覆盖率上升21.6%，页面接受率提升29.1%。

Conclusion: DevPiolt模型显著提升了IoT设备操作推荐的效果，并已实际应用，带来了用户和企业双赢的显著改进。

Abstract: Operation recommendation for IoT devices refers to generating personalized device operations for users based on their context, such as historical operations, environment information, and device status. This task is crucial for enhancing user satisfaction and corporate profits. Existing recommendation models struggle with complex operation logic, diverse user preferences, and sensitive to suboptimal suggestions, limiting their applicability to IoT device operations. To address these issues, we propose DevPiolt, a LLM-based recommendation model for IoT device operations. Specifically, we first equip the LLM with fundamental domain knowledge of IoT operations via continual pre-training and multi-task fine-tuning. Then, we employ direct preference optimization to align the fine-tuned LLM with specific user preferences. Finally, we design a confidence-based exposure control mechanism to avoid negative user experiences from low-quality recommendations. Extensive experiments show that DevPiolt significantly outperforms baselines on all datasets, with an average improvement of 69.5% across all metrics. DevPiolt has been practically deployed in Xiaomi Home app for one quarter, providing daily operation recommendations to 255,000 users. Online experiment results indicate a 21.6% increase in unique visitor device coverage and a 29.1% increase in page view acceptance rates.

</details>


### [53] [Enhancing Regional Airbnb Trend Forecasting Using LLM-Based Embeddings of Accessibility and Human Mobility](https://arxiv.org/abs/2511.14248)
*Hongju Lee,Youngjun Park,Jisun An,Dongman Lee*

Main category: cs.AI

TL;DR: 提出了一种结合大语言模型与时序预测的新框架，能大幅提升Airbnb区域市场指标的预测准确性，实验显示误差降低48%，对城市管理和政策制定具有实用价值。


<details>
  <summary>Details</summary>
Motivation: 短租平台（如Airbnb）对当地住房市场带来扰动，加剧租金上涨和住房负担问题。区域层面准确预测短租平台趋势，对于政策制定者和城市规划者具有重要意义，有助于缓解上述负面影响。

Method: 本研究采用滑动窗口方法预测未来1到3个月的Airbnb主要指标（收入、预订天数、预订量）。关键方法为：融合房源特征与外部环境因素（如城市可达性、人类流动性），将结构化数据转化为Prompt输入至大语言模型，生成区域嵌入向量，再输入RNN、LSTM、Transformer等高级时序模型进行预测。

Result: 在首尔Airbnb数据集上的实验表明，所提框架在RMSE和MAE的平均水平上，相较传统统计及机器学习基线模型降低了约48%。模型能够有效检测到供给过剩区域，并对城市政策制定有实际指导意义。

Conclusion: 提出的时序预测框架能够有效提升对区域Airbnb市场主要指标的预测精度，有助于识别供给过剩地区，并为数据驱动的城市政策决策提供支持。

Abstract: The expansion of short-term rental platforms, such as Airbnb, has significantly disrupted local housing markets, often leading to increased rental prices and housing affordability issues. Accurately forecasting regional Airbnb market trends can thus offer critical insights for policymakers and urban planners aiming to mitigate these impacts. This study proposes a novel time-series forecasting framework to predict three key Airbnb indicators -- Revenue, Reservation Days, and Number of Reservations -- at the regional level. Using a sliding-window approach, the model forecasts trends 1 to 3 months ahead. Unlike prior studies that focus on individual listings at fixed time points, our approach constructs regional representations by integrating listing features with external contextual factors such as urban accessibility and human mobility. We convert structured tabular data into prompt-based inputs for a Large Language Model (LLM), producing comprehensive regional embeddings. These embeddings are then fed into advanced time-series models (RNN, LSTM, Transformer) to better capture complex spatio-temporal dynamics. Experiments on Seoul's Airbnb dataset show that our method reduces both average RMSE and MAE by approximately 48% compared to conventional baselines, including traditional statistical and machine learning models. Our framework not only improves forecasting accuracy but also offers practical insights for detecting oversupplied regions and supporting data-driven urban policy decisions.

</details>


### [54] [PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2511.14256)
*Yu Liu,Xixun Lin,Yanmin Shang,Yangxi Li,Shi Wang,Yanan Cao*

Main category: cs.AI

TL;DR: PathMind 通过优先选取关键推理路径，结合高效训练策略，显著提升了大模型在知识图谱推理上的表现，具备更高的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大模型推理方法未能甄别推理路径的重要性，易引入噪声，且对LLM调用频繁，效率低下。因此需要更高效且可解释的方法优化推理过程。

Method: 提出了“Retrieve-Prioritize-Reason”三阶段流程，包括检索子图、语义感知的路径优先机制以及双阶段训练（任务微调和路径对齐），以指导大模型选择重要推理路径。

Result: 大量基准数据实验证明，PathMind 能够持续领先主流方法，尤其在复杂推理和精简输入场景下，通过识别关键推理路径，取得更优表现。

Conclusion: PathMind 框架能够有效提升知识图谱推理的准确性和解释性，尤其在输入较少的复杂推理任务中表现优越。

Abstract: Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a "Retrieve-Prioritize-Reason" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.

</details>


### [55] [DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning](https://arxiv.org/abs/2511.14299)
*Xiaochuan Liu,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.AI

TL;DR: 本文提出DataSage多智能体框架，通过引入知识检索、角色辩论和多路径推理，显著提升自动化数据洞察效果，在多个测试集上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的数据洞察智能体存在知识利用不足、分析深度不够以及代码生成易出错的问题，亟需通过结构创新提升自动化数据分析性能。

Method: 提出了一套由多智能体驱动的数据分析框架DataSage，融合了外部知识检索、多角色辩论和多路径推理机制，并在InsightBench进行了实验评估。

Result: DataSage通过引入外部知识、多角色辩论和多路径推理，有效提升了分析深度和代码准确性。实验结果表明，在InsightBench上DataSage在所有难度下都优于已有方案。

Conclusion: DataSage能够在自动化数据洞察发现任务中，整体优于现有的数据洞察智能体，并在各种难度下表现更佳。

Abstract: In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.

</details>


### [56] [When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling](https://arxiv.org/abs/2511.14334)
*Alessio Pellegrino,Jacopo Mauro*

Main category: cs.AI

TL;DR: 本文发现，现有大语言模型在自动生成约束编程模型时对语言表述极为敏感，表现优异可能源于训练数据污染，语境和措辞稍有变化后能力骤降，说明模型理解能力尚浅。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在约束编程自动建模中的真正推理能力，以及其优异表现是否源于数据污染（如训练集已含标准问题），以检验其泛化和理解能力。

Method: 系统性地重述和扰动一组著名CSPLib问题，改变语境和引入干扰元素，并将三种主流大语言模型在原始与修改描述下生成的模型进行比较和定性分析。

Result: 结果显示，LLMs在原始问题描述下能生成语法正确且语义合理的模型，但在经过语境和措辞变化的问题上表现大幅下降，暴露出理解的肤浅和对限定语言的依赖。

Conclusion: 大语言模型在自动生成约束编程模型方面表现出能力，但对问题表述有高度敏感，语境和措辞变化导致其表现明显下降，说明理解深度有限。

Abstract: One of the long-standing goals in optimisation and constraint programming is to describe a problem in natural language and automatically obtain an executable, efficient model. Large language models appear to bring this vision closer, showing impressive results in automatically generating models for classical benchmarks. However, much of this apparent success may derive from data contamination rather than genuine reasoning: many standard CP problems are likely included in the training data of these models. To examine this hypothesis, we systematically rephrased and perturbed a set of well-known CSPLib problems to preserve their structure while modifying their context and introducing misleading elements. We then compared the models produced by three representative LLMs across original and modified descriptions. Our qualitative analysis shows that while LLMs can produce syntactically valid and semantically plausible models, their performance drops sharply under contextual and linguistic variation, revealing shallow understanding and sensitivity to wording.

</details>


### [57] [Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior](https://arxiv.org/abs/2511.14476)
*Dalia Ali,Dora Zhao,Allison Koenecke,Orestis Papakyriakopoulos*

Main category: cs.AI

TL;DR: 探讨多元社会价值对大语言模型对齐效果的影响，实证发现不同群体偏好和技术参数能显著影响模型输出行为和安全性，提供多元对齐及公平性的关键参考。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法往往忽略人类社会多样性带来的价值差异，研究动机在于探索赋予模型多元价值观对其行为的影响，为模型安全与公平对齐提供参考。

Method: 收集美国和德国共1095位参与者的对齐数据，涵盖五个评分维度，通过不同社会群体偏好微调大语言模型，并系统考察评分量表、分歧处理、优化方法对模型表现的影响。

Result: 1）不同人口统计群体评分标准差异显著；2）模型微调时保留分歧能显著降低有害性，5分制优于二元评分方案；3）DPO在多价值对齐中优于GRPO；4）按照不同群体偏好微调的模型展现出明显的行为特征。

Conclusion: 不同社会群体参与的大型语言模型对齐会产生显著行为差异，设计参数对对齐结果也有较大影响，研究为对齐过程如何平衡安全性与公平性提供了实证依据。

Abstract: Although large language models (LLMs) are increasingly trained using human feedback for safety and alignment with human values, alignment decisions often overlook human social diversity. This study examines how incorporating pluralistic values affects LLM behavior by systematically evaluating demographic variation and design parameters in the alignment pipeline. We collected alignment data from US and German participants (N = 1,095, 27,375 ratings) who rated LLM responses across five dimensions: Toxicity, Emotional Awareness (EA), Sensitivity, Stereotypical Bias, and Helpfulness. We fine-tuned multiple Large Language Models and Large Reasoning Models using preferences from different social groups while varying rating scales, disagreement handling methods, and optimization techniques. The results revealed systematic demographic effects: male participants rated responses 18% less toxic than female participants; conservative and Black participants rated responses 27.9% and 44% more emotionally aware than liberal and White participants, respectively. Models fine-tuned on group-specific preferences exhibited distinct behaviors. Technical design choices showed strong effects: the preservation of rater disagreement achieved roughly 53% greater toxicity reduction than majority voting, and 5-point scales yielded about 22% more reduction than binary formats; and Direct Preference Optimization (DPO) consistently outperformed Group Relative Policy Optimization (GRPO) in multi-value optimization. These findings represent a preliminary step in answering a critical question: How should alignment balance expert-driven and user-driven signals to ensure both safety and fair representation?

</details>


### [58] [Rate-Distortion Guided Knowledge Graph Construction from Lecture Notes Using Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2511.14595)
*Yuan An,Ruhma Hashmi,Michelle Rogers,Jane Greenberg,Brian K. Smith*

Main category: cs.AI

TL;DR: 提出一种基于率失真理论和最优传输的知识图谱自动构建和优化方法，在数据科学讲义应用中显著提升了自动生成选择题的质量，并为知识图谱优化提供了信息论基础。


<details>
  <summary>Details</summary>
Motivation: 将非结构化教育资料（如讲义、幻灯片）自动转化为适用于生成高质量选择题的知识图谱难度较大，需要一种信息保存且结构紧凑的方法来提升题目质量与知识图谱可用性。

Method: 以率失真理论（rate-distortion theory）和最优传输几何为基础，将讲义内容建模为度量-测度空间，通过Fused Gromov-Wasserstein（FGW）方式对候选知识图谱进行对齐，并用多种操作(增/合并/拆分/移除/重连)最小化率失真拉格朗日量，优化知识图谱结构。

Result: 原型系统在数据科学讲义上实验证明，优化后的知识图谱生成的选择题在十五项质量标准上优于直接利用原始讲义内容制作的选择题，同时该方法可生成可解释的率失真曲线。

Conclusion: 本文提出的方法能够构建更紧凑且信息丰富的知识图谱，生成的选择题在多个质量维度上均优于直接基于原始讲义制作的题目，为AI辅助教育中知识图谱的优化提供了理论基础。

Abstract: Task-oriented knowledge graphs (KGs) enable AI-powered learning assistant systems to automatically generate high-quality multiple-choice questions (MCQs). Yet converting unstructured educational materials, such as lecture notes and slides, into KGs that capture key pedagogical content remains difficult. We propose a framework for knowledge graph construction and refinement grounded in rate-distortion (RD) theory and optimal transport geometry. In the framework, lecture content is modeled as a metric-measure space, capturing semantic and relational structure, while candidate KGs are aligned using Fused Gromov-Wasserstein (FGW) couplings to quantify semantic distortion. The rate term, expressed via the size of KG, reflects complexity and compactness. Refinement operators (add, merge, split, remove, rewire) minimize the rate-distortion Lagrangian, yielding compact, information-preserving KGs. Our prototype applied to data science lectures yields interpretable RD curves and shows that MCQs generated from refined KGs consistently surpass those from raw notes on fifteen quality criteria. This study establishes a principled foundation for information-theoretic KG optimization in personalized and AI-assisted education.

</details>


### [59] [AutoTool: Efficient Tool Selection for Large Language Model Agents](https://arxiv.org/abs/2511.14650)
*Jingyi Jia,Qinbin Li*

Main category: cs.AI

TL;DR: AutoTool通过建模工具使用惯性，减少了工具选择时对LLM的调用，在保证性能的情况下显著降低了推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型Agent在工具选择时频繁进行推理，尤其是如ReAct方法，每一步都需调用LLM，导致推理成本高。作者观察到工具使用具有路径惯性，期望通过建模该惯性来降低推理次数。

Method: 提出了一种基于有向图的AutoTool框架，利用历史轨迹构建工具选择的转移图，并整合参数级信息以高效生成工具输入，从而减少对LLM推理的依赖。

Result: AutoTool在多项Agent任务中，实现了30%的推理成本降低，并在任务完成率上与现有方法持平，证明其效率与有效性。

Conclusion: AutoTool框架能在降低推理成本的同时保持任务完成率，提升了大型语言模型Agent框架的实用性与可扩展性。

Abstract: Large Language Model (LLM) agents have emerged as powerful tools for automating complex tasks by leveraging the reasoning and decision-making abilities of LLMs. However, a major bottleneck in current agent frameworks lies in the high inference cost of tool selection, especially in approaches like ReAct that repeatedly invoke the LLM to determine which tool to use at each step. In this work, we propose AutoTool, a novel graph-based framework that bypasses repeated LLM inference by exploiting a key empirical observation: tool usage inertia - the tendency of tool invocations to follow predictable sequential patterns. AutoTool constructs a directed graph from historical agent trajectories, where nodes represent tools and edges capture transition probabilities, effectively modeling the inertia in tool selection. It further integrates parameter-level information to refine tool input generation. By traversing this structured representation, AutoTool efficiently selects tools and their parameters with minimal reliance on LLM inference. Extensive experiments across diverse agent tasks demonstrate that AutoTool reduces inference costs by up to 30% while maintaining competitive task completion rates, offering a practical and scalable enhancement for inference-heavy frameworks. Our work highlights the promise of integrating statistical structure into LLM agent design for greater efficiency without sacrificing performance.

</details>
