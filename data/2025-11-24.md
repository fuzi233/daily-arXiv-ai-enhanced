<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 40]
- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Towards Hyper-Efficient RAG Systems in VecDBs: Distributed Parallel Multi-Resolution Vector Search](https://arxiv.org/abs/2511.16681)
*Dong Liu,Yanxuan Yu*

Main category: cs.CL

TL;DR: SPI语义金字塔索引框架为RAG系统实现自适应检索分辨率，大幅提升速度、效率和问答表现，并可直接集成到现有FAISS和Qdrant。


<details>
  <summary>Details</summary>
Motivation: 现有向量数据库检索结构为单一分辨率，不能适应不同语义粒度查询，导致检索速度与语义相关性表现不理想。亟需针对多样查询语义自适应的索引方案提升RAG系统实际表现。

Method: 在文档嵌入基础上构建语义金字塔，通过轻量级分类器动态选择每次检索的最优分辨率，实现粗到精逐步检索。支持FAISS和Qdrant后端，包含理论分析与消融实验。

Result: SPI取得最高5.7倍检索速度提升和1.8倍内存效率提升，端到端问答F1分数最多提升2.5分，理论证明了检索质量和延迟界限，消融实验验证组件贡献；兼容现有向量数据库环境，支持实际部署。

Conclusion: 提出了一种名为SPI（Semantic Pyramid Indexing）的多分辨率向量索引框架，有效提升RAG系统的检索速度、内存效率及问答性能，并可无缝集成到当前主流向量数据库。

Abstract: Retrieval-Augmented Generation (RAG) systems have become a dominant approach to augment large language models (LLMs) with external knowledge. However, existing vector database (VecDB) retrieval pipelines rely on flat or single-resolution indexing structures, which cannot adapt to the varying semantic granularity required by diverse user queries. This limitation leads to suboptimal trade-offs between retrieval speed and contextual relevance.
  To address this, we propose \textbf{Semantic Pyramid Indexing (SPI)}, a novel multi-resolution vector indexing framework that introduces query-adaptive resolution control for RAG in VecDBs. Unlike existing hierarchical methods that require offline tuning or separate model training, SPI constructs a semantic pyramid over document embeddings and dynamically selects the optimal resolution level per query through a lightweight classifier. This adaptive approach enables progressive retrieval from coarse-to-fine representations, significantly accelerating search while maintaining semantic coverage.
  We implement SPI as a plugin for both FAISS and Qdrant backends and evaluate it across multiple RAG tasks including MS MARCO, Natural Questions, and multimodal retrieval benchmarks. SPI achieves up to \textbf{5.7$\times$} retrieval speedup and \textbf{1.8$\times$} memory efficiency gain while improving end-to-end QA F1 scores by up to \textbf{2.5 points} compared to strong baselines. Our theoretical analysis provides guarantees on retrieval quality and latency bounds, while extensive ablation studies validate the contribution of each component. The framework's compatibility with existing VecDB infrastructures makes it readily deployable in production RAG systems. Code is availabe at \href{https://github.com/FastLM/SPI_VecDB}{https://github.com/FastLM/SPI\_VecDB}.

</details>


### [2] [Bench360: Benchmarking Local LLM Inference from 360°](https://arxiv.org/abs/2511.16682)
*Linus Stuhlmann,Mauricio Fadel Argerich,Jonathan Fürst*

Main category: cs.CL

TL;DR: Bench360是一个全面、用户定制的本地LLM推理评测工具，支持多引擎、多场景与多指标，实验证明配置需个性化权衡，工具能显著简化选型。


<details>
  <summary>Details</summary>
Motivation: 当前用户在本地运行大语言模型时，面临配置选择多且复杂，现有评测工具目标单一且不够用户导向，缺乏集成系统和任务指标的评测工具，因此提出Bench360。

Method: 提出了Bench360框架，能够定制任务、数据集和相关指标，自动评测不同LLMs、推理引擎和量化等级，支持多种使用场景，并追踪系统层面和任务层面的广泛指标。

Result: 实证结果覆盖4类主流任务、3种硬件平台、4种推理引擎，揭示了任务性能和系统效率的多种权衡，凸显了不同引擎与模型的显著差异。Bench360能助力用户找到最佳配置。

Conclusion: 没有单一最佳的本地LLM推理配置，不同任务和系统需求需要权衡，强烈需要类似Bench360这样的全面评测框架。

Abstract: Running large language models (LLMs) locally is becoming increasingly common. While the growing availability of small open-source models and inference engines has lowered the entry barrier, users now face an overwhelming number of configuration choices. Identifying an optimal configuration -- balancing functional and non-functional requirements -- requires substantial manual effort. While several benchmarks target LLM inference, they are designed for narrow evaluation goals and not user-focused. They fail to integrate relevant system and task-specific metrics into a unified, easy-to-use benchmark that supports multiple inference engines, usage scenarios, and quantization levels. To address this gap, we present Bench360 -- Benchmarking Local LLM Inference from 360°. Bench360 allows users to easily define their own custom tasks along with datasets and relevant task-specific metrics and then automatically benchmarks selected LLMs, inference engines, and quantization levels across different usage scenarios (single stream, batch & server). Bench360 tracks a wide range of metrics, including (1) system metrics -- such as Computing Performance (e.g., latency, throughput), Resource Usage (e.g., energy per query), and Deployment (e.g., cold start time) -- and (2) task-specific metrics such as ROUGE, F1 score or accuracy. We demonstrate Bench360 on four common LLM tasks -- General Knowledge & Reasoning, QA, Summarization and Text-to-SQL -- across three hardware platforms and four state of the art inference engines. Our results reveal several interesting trade-offs between task performance and system-level efficiency, highlighting the differences in inference engines and models. Most importantly, there is no single best setup for local inference, which strongly motivates the need for a framework such as Bench360.

</details>


### [3] [How Well Do LLMs Understand Tunisian Arabic?](https://arxiv.org/abs/2511.16683)
*Mohamed Mahdi*

Main category: cs.CL

TL;DR: 该研究通过新建多语种并带情感标签的数据集，系统评测了LLM在突尼斯阿拉伯语方言上的表现，并揭示在语言包容性和技术覆盖方面的显著不足，强调低资源语言在AI未来发展中的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于突尼斯阿拉伯语方言在主流AI模型中被忽视，导致当地用户难以便捷使用AI且方言有被边缘化甚至淡化的风险，该研究旨在促进AI技术的语言多样性和包容性。

Method: 构建了包含突尼斯方言、标准阿拉伯语与英文的平行数据集，并加入情感标签，在转写、翻译与情感分析三项任务上对多种主流LLM进行基准测试。

Result: 多个LLM在突尼斯方言的理解与处理能力上表现不一，部分模型在语言转写、翻译与情感分析等任务上存在明显短板，突显了当前技术在低资源语言覆盖上的不足。

Conclusion: 该研究显示，目前主流的大型语言模型在处理突尼斯阿拉伯语方言等低资源语言上表现存在差异，未能完全满足相关需求，强调了在未来AI系统中加强低资源语言支持的重要性。

Abstract: Large Language Models (LLMs) are the engines driving today's AI agents. The better these models understand human languages, the more natural and user-friendly the interaction with AI becomes, from everyday devices like computers and smartwatches to any tool that can act intelligently. Yet, the ability of industrial-scale LLMs to comprehend low-resource languages, such as Tunisian Arabic (Tunizi), is often overlooked. This neglect risks excluding millions of Tunisians from fully interacting with AI in their own language, pushing them toward French or English. Such a shift not only threatens the preservation of the Tunisian dialect but may also create challenges for literacy and influence younger generations to favor foreign languages. In this study, we introduce a novel dataset containing parallel Tunizi, standard Tunisian Arabic, and English translations, along with sentiment labels. We benchmark several popular LLMs on three tasks: transliteration, translation, and sentiment analysis. Our results reveal significant differences between models, highlighting both their strengths and limitations in understanding and processing Tunisian dialects. By quantifying these gaps, this work underscores the importance of including low-resource languages in the next generation of AI systems, ensuring technology remains accessible, inclusive, and culturally grounded.

</details>


### [4] [Ellipsoid-Based Decision Boundaries for Open Intent Classification](https://arxiv.org/abs/2511.16685)
*Yuetian Zou,Hanlei Zhang,Hua Xu,Songze Li,Long Xiao*

Main category: cs.CL

TL;DR: EliDecide采用椭球判别边界更好地应对开放意图分类，有效提升了多项意图识别任务的性能，且方法具备更强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设已知类别的分布是各向同性的，仅能生成球形边界，无法充分捕捉特征空间不同方向的分布差异，限制了开放意图检测的性能。本文希望通过椭球边界提升检测的灵活性与准确性。

Method: 首先，采用有监督对比学习获取已知样本的判别特征空间；其次，用可学习矩阵参数化每个已知类别的椭球边界，增强边界表达能力；第三，设计双重损失函数，通过边界扩展覆盖已知样本、收缩抵御伪开放样本，平衡经验风险与开放空间风险。

Result: 在多个文本意图基准数据集以及问题分类数据集上，EliDecide方法取得了最先进的性能，验证了其椭球边界在开放意图检测上的优势和泛化能力。

Conclusion: 提出的EliDecide方法在文本意图分类任务中表现出更强的开放意图检测能力，并且具有良好的泛化潜力，可应用于复杂的开放世界文本分类场景。

Abstract: Textual open intent classification is crucial for real-world dialogue systems, enabling robust detection of unknown user intents without prior knowledge and contributing to the robustness of the system. While adaptive decision boundary methods have shown great potential by eliminating manual threshold tuning, existing approaches assume isotropic distributions of known classes, restricting boundaries to balls and overlooking distributional variance along different directions. To address this limitation, we propose EliDecide, a novel method that learns ellipsoid decision boundaries with varying scales along different feature directions. First, we employ supervised contrastive learning to obtain a discriminative feature space for known samples. Second, we apply learnable matrices to parameterize ellipsoids as the boundaries of each known class, offering greater flexibility than spherical boundaries defined solely by centers and radii. Third, we optimize the boundaries via a novelly designed dual loss function that balances empirical and open-space risks: expanding boundaries to cover known samples while contracting them against synthesized pseudo-open samples. Our method achieves state-of-the-art performance on multiple text intent benchmarks and further on a question classification dataset. The flexibility of the ellipsoids demonstrates superior open intent detection capability and strong potential for generalization to more text classification tasks in diverse complex open-world scenarios.

</details>


### [5] [Prompt-Based Value Steering of Large Language Models](https://arxiv.org/abs/2511.16688)
*Giulio Antonio Abbo,Tony Belpaeme*

Main category: cs.CL

TL;DR: 提出了一种评估和量化大型语言模型内容价值引导的通用流程，实验证明通过设计特定提示词，可以无需改动模型实现价值对齐，提升模型灵活应对人类多样偏好的能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型微调方法静态且无法适应动态变化的价值观和偏好，因此需要一种能够灵活引导模型内容契合人类价值的新方法。

Method: 提出了一种实用、可复现且模型无关的评估流程，通过量化目标价值在输出中的体现，比较不同提示词对文本生成结果的引导效果。采用了基于Schwartz人类基本价值理论的评分方法，并在对话数据集上进行结构化评估。

Result: 在Wizard-Vicuna模型上，采用价值条件提示词后，模型输出更加体现目标人类价值。即使未更改模型本身，也能有效实现价值引导。

Conclusion: 无需修改模型或动态优化提示词，仅通过设计特定提示词即可实现对模型生成内容的价值引导。

Abstract: Large language models are increasingly used in applications where alignment with human values is critical. While model fine-tuning is often employed to ensure safe responses, this technique is static and does not lend itself to everyday situations involving dynamic values and preferences. In this paper, we present a practical, reproducible, and model-agnostic procedure to evaluate whether a prompt candidate can effectively steer generated text toward specific human values, formalising a scoring method to quantify the presence and gain of target values in generated responses. We apply our method to a variant of the Wizard-Vicuna language model, using Schwartz's theory of basic human values and a structured evaluation through a dialogue dataset. With this setup, we compare a baseline prompt to one explicitly conditioned on values, and show that value steering is possible even without altering the model or dynamically optimising prompts.

</details>


### [6] [Concept-Based Interpretability for Toxicity Detection](https://arxiv.org/abs/2511.16689)
*Samarth Garg,Deeksha Varshney,Divya Singh*

Main category: cs.CL

TL;DR: 本文将多类型毒性属性作为解释性概念，通过CG、WCA等方法分析并改进文本毒性检测模型的归因与可解释性，发现去除明确毒性词后仍存在过归因的问题，可为后续模型增强提供思路。


<details>
  <summary>Details</summary>
Motivation: 虽已有毒性文本检测技术，但对概念层面解释和误判归因分析仍有限。本文旨在探讨如何通过多种类别子属性来增强模型解释性与归因，减少因概念过度归因导致的错误。

Method: 引入Concept Gradient(CD)机制，通过测量概念变化对模型输出的影响提升可解释性；提出Targeted Lexicon Set用于识别导致分类错误的毒性词汇；计算Word-Concept Alignment(WCA)分数，定量分析词汇与分类错误之间关系；采用“无词汇”增广策略检验模型在无毒性词汇时归因表现。

Result: 基于CG的解释性方法有效揭示了模型如何响应毒性概念变化；Targeted Lexicon Set和WCA分数帮助分析模型误判来源；“无词汇”数据增广发现即使排除部分毒性词模型仍可能过度归因，表明分类模型存在泛化与解释性问题。

Conclusion: 本文提出的基于Concept Gradient (CG)方法的可解释性技术，能够更因果地解释模型中概念变化对输出的直接影响，并通过词汇去除实验验证了模型在无明确毒性词汇时仍可能产生过归因错误。

Abstract: The rise of social networks has not only facilitated communication but also allowed the spread of harmful content. Although significant advances have been made in detecting toxic language in textual data, the exploration of concept-based explanations in toxicity detection remains limited. In this study, we leverage various subtype attributes present in toxicity detection datasets, such as obscene, threat, insult, identity attack, and sexual explicit as concepts that serve as strong indicators to identify whether language is toxic. However, disproportionate attribution of concepts towards the target class often results in classification errors. Our work introduces an interpretability technique based on the Concept Gradient (CG) method which provides a more causal interpretation by measuring how changes in concepts directly affect the output of the model. This is an extension of traditional gradient-based methods in machine learning, which often focus solely on input features. We propose the curation of Targeted Lexicon Set, which captures toxic words that contribute to misclassifications in text classification models. To assess the significance of these lexicon sets in misclassification, we compute Word-Concept Alignment (WCA) scores, which quantify the extent to which these words lead to errors due to over-attribution to toxic concepts. Finally, we introduce a lexicon-free augmentation strategy by generating toxic samples that exclude predefined toxic lexicon sets. This approach allows us to examine whether over-attribution persists when explicit lexical overlap is removed, providing insights into the model's attribution on broader toxic language patterns.

</details>


### [7] [Falsely Accused: How AI Detectors Misjudge Slightly Polished Arabic Articles](https://arxiv.org/abs/2511.16690)
*Saleh Almohaimeed,Saad Almohaimeed,Mousa Jari,Khaled A. Alobaid,Fahad Alotaibi*

Main category: cs.CL

TL;DR: 轻度使用AI工具润色阿拉伯语人类写作会导致现有AI检测器大量误判其为AI生成，检测结果准确率明显下降，需警惕AI检测器在实际应用中的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容的普及，现有AI检测器在面对仅受轻度AI辅助润色的人类文章时容易误判，恐造成作者被误指AI抄袭，需要评估这种现象，尤其是阿拉伯语领域尚未有相关研究。

Method: 研究生成了两组阿拉伯语文本数据集：第一组包含800篇文章（AI生成/人类撰写各半），用于评估14个LLM和商业检测器的区分能力；第二组包含400篇人类撰写文章，每篇通过10个LLM、4种润色设置润色，获得16400个样本，用于评估选取的8个顶尖检测器对润色后文本的判断。

Result: 所有检测器在处理AI轻度润色的人类文章时准确率严重下降。Claude-4 Sonnet作为最佳LLM检测器，其准确率由83.51%降至57.63%；最佳商业检测器originality.AI准确率由92%锐减至12%。

Conclusion: 所有AI检测器在处理经过略微润色的人类撰写文章时，均表现出显著的误判，导致大量真实文章被错误认定为AI生成，从而质疑了这些检测工具的可靠性。

Abstract: Many AI detection models have been developed to counter the presence of articles created by artificial intelligence (AI). However, if a human-authored article is slightly polished by AI, a shift will occur in the borderline decision of these AI detection models, leading them to consider it AI-generated article. This misclassification may result in falsely accusing authors of AI plagiarism and harm the credibility of AI detector models. In English, some efforts were made to meet this challenge, but not in Arabic. In this paper, we generated two datasets. The first dataset contains 800 Arabic articles, half AI-generated and half human-authored. We used it to evaluate 14 Large Language models (LLMs) and commercial AI detectors to assess their ability in distinguishing between human-authored and AI-generated articles. The best 8 models were chosen to act as detectors for our primary concern, which is whether they would consider slightly polished human text as AI-generated. The second dataset, Ar-APT, contains 400 Arabic human-authored articles polished by 10 LLMs using 4 polishing settings, totaling 16400 samples. We use it to evaluate the 8 nominated models and determine whether slight polishing will affect their performance. The results reveal that all AI detectors incorrectly attribute a significant number of articles to AI. The best performing LLM, Claude-4 Sonnet, achieved 83.51%, their performance decreased to 57.63% for articles slightly polished by LLaMA-3. Whereas for the best performing commercial model, originality.AI, that achieves 92% accuracy, dropped to 12% for articles slightly polished by Mistral or Gemma-3.

</details>


### [8] [Reproducibility Report: Test-Time Training on Nearest Neighbors for Large Language Models](https://arxiv.org/abs/2511.16691)
*Boyang Zhou,Johan Lindqvist,Lindsey Li*

Main category: cs.CL

TL;DR: 通过最近邻测试时训练，语言模型推理时可根据检索到的邻居样本即时微调，显著提升各类数据集上的表现，特别是小模型或未在目标领域预训练的模型受益最大，并提出实用的高效检索技术降低资源消耗，验证了该方法的稳健性和通用性。


<details>
  <summary>Details</summary>
Motivation: 检验和扩展最近邻测试时训练在语言模型中的有效性及通用性，尤其是在未预训练于目标领域数据集时的性能表现，同时解决实际部署的检索内存壁垒。

Method: 预训练RoBERTa嵌入通过Faiss构建索引，每个测试输入检索20个最近邻样本，并对每个邻居进行一次梯度更新。应用于多个主流语言模型（GPT-2、GPT-Neo、R1-Distilled-Qwen2.5-1.5B），以验证在多样数据集上的性能提升，同时优化最近邻检索的内存开销。

Result: 测试时训练能显著降低困惑度和bit-per-byte指标，尤其在专业和结构化数据集如GitHub和EuroParl上提升尤为显著；未用目标集预训练的模型受益更大，小模型性能能接近大型模型。提出的高效检索方案将内存需求从128GB降至32GB，验证了方法在新型架构（如R1-Distilled-Qwen2.5-1.5B）上的有效性。

Conclusion: 最近邻测试时训练可以在多种大型语言模型架构上带来一致且显著的性能提升，而且特别适合于结构化或专业领域数据。未预训练于目标领域的数据集的模型受益更大，使得小模型能达到接近大模型的效果。改进的检索实现也解决了实践中的内存瓶颈。

Abstract: We reproduce the central claims of Test-Time Training on Nearest Neighbors for Large Language Models (Hardt and Sun, 2024), which proposes adapting a language model at inference time by fine-tuning on retrieved nearest-neighbor sequences. Using pretrained RoBERTa embeddings indexed with Faiss, we retrieve 20 neighbors per test input and apply one gradient update per neighbor across GPT-2 (117M, 774M), GPT-Neo (1.3B), and R1-Distilled-Qwen2.5-1.5B. Our experiments confirm that test-time training significantly reduces perplexity and bits-per-byte metrics across diverse domains from The Pile, with the largest improvements in structured or specialized datasets such as GitHub and EuroParl. We further validate that models not pretrained on The Pile benefit more from this adaptation than models already trained on similar data, allowing smaller models to approach the performance of larger ones. Due to infrastructure limitations, we introduce a memory-efficient retrieval implementation that loads only required line offsets rather than entire files, reducing RAM requirements from over 128 GB per server to 32 GB. We also extend the original study by evaluating R1-Distilled-Qwen2.5-1.5B, showing that test-time training yields consistent gains even for modern reasoning-optimized architectures. Overall, our results support the robustness and generality of nearest-neighbor test-time training while highlighting practical considerations for reproducing large-scale retrieval-augmented adaptation.

</details>


### [9] [How Language Directions Align with Token Geometry in Multilingual LLMs](https://arxiv.org/abs/2511.16693)
*JaeSeong Kim,Suan Lee*

Main category: cs.CL

TL;DR: 本研究系统剖析了多语言LLM内部如何编码和分离多语种信息，发现训练语料结构显著影响模型的语言结构表征，为多语种公平性和数据构成策略提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 虽然多语言LLM在多种语言上表现出强大能力，但其内部如何编码和分离语言信息的系统性分析较少，尤其是跨层结构。

Method: 对六种多语言大模型所有268个transformer层进行全面探测，包括线性与非线性探针分析，并提出新的Token-Language Alignment方法，量化语言编码的层次动态与几何结构。

Result: 语言信息在模型第一个块实现显著分离（分离度从第0到第1层提升76.4个百分点），并在后续层保持高度线性可分；语言方向与词汇嵌入的对齐受训练数据语言成分显著影响，例如包含中文的模型相较于英语为主的模型，其中文结构印记效应高出4.21倍。

Conclusion: 多语言大模型通过潜在表示结构而非表面文字特征来区分不同语言，这一结构强烈受训练语料的语言构成影响。模型在最初几个层就迅速实现了语言信息的线性分离，且分离效果在深度上保持。

Abstract: Multilingual LLMs demonstrate strong performance across diverse languages, yet there has been limited systematic analysis of how language information is structured within their internal representation space and how it emerges across layers. We conduct a comprehensive probing study on six multilingual LLMs, covering all 268 transformer layers, using linear and nonlinear probes together with a new Token--Language Alignment analysis to quantify the layer-wise dynamics and geometric structure of language encoding. Our results show that language information becomes sharply separated in the first transformer block (+76.4$\pm$8.2 percentage points from Layer 0 to 1) and remains almost fully linearly separable throughout model depth. We further find that the alignment between language directions and vocabulary embeddings is strongly tied to the language composition of the training data. Notably, Chinese-inclusive models achieve a ZH Match@Peak of 16.43\%, whereas English-centric models achieve only 3.90\%, revealing a 4.21$\times$ structural imprinting effect. These findings indicate that multilingual LLMs distinguish languages not by surface script features but by latent representational structures shaped by the training corpus. Our analysis provides practical insights for data composition strategies and fairness in multilingual representation learning. All code and analysis scripts are publicly available at: https://github.com/thisiskorea/How-Language-Directions-Align-with-Token-Geometry-in-Multilingual-LLMs.

</details>


### [10] [Hierarchical Retrieval with Out-Of-Vocabulary Queries: A Case Study on SNOMED CT](https://arxiv.org/abs/2511.16698)
*Jonathon Dilworth,Hui Yang,Jiaoyan Chen,Yongsheng Gao*

Main category: cs.CL

TL;DR: 该文提出基于语言模型嵌入的新方法，提升了SNOMED CT本体OOV查询的检索表现，超越传统方法并具备通用扩展性。


<details>
  <summary>Details</summary>
Motivation: SNOMED CT作为庞大的生物医学本体，概念查询常因语言歧义、同义词、多义词及OOV（查询项在本体中无对应项）问题而变得困难，亟需新方法提升其检索能力。

Method: 该研究提出利用语言模型生成本体嵌入向量的方法，在SNOMED CT中进行OOV查询的层次概念检索。同时，构建了OOV查询集合并与多种基线（如SBERT和词汇匹配方法）进行了对比实验。

Result: 实验显示，所提方法在检索概念最近的上位节点和其他祖先节点任务中效果更佳，超越了SBERT和传统词汇匹配基线。该方法具有通用性，可扩展至其他本体。代码、工具和数据集已开源。

Conclusion: 文章提出的基于语言模型的本体嵌入方法在从SNOMED CT中进行层次概念检索时，尤其是面对OOV（语外词）查询，显著优于现有方法。

Abstract: SNOMED CT is a biomedical ontology with a hierarchical representation of large-scale concepts. Knowledge retrieval in SNOMED CT is critical for its application, but often proves challenging due to language ambiguity, synonyms, polysemies and so on. This problem is exacerbated when the queries are out-of-vocabulary (OOV), i.e., having no equivalent matchings in the ontology. In this work, we focus on the problem of hierarchical concept retrieval from SNOMED CT with OOV queries, and propose an approach based on language model-based ontology embeddings. For evaluation, we construct OOV queries annotated against SNOMED CT concepts, testing the retrieval of the most direct subsumers and their less relevant ancestors. We find that our method outperforms the baselines including SBERT and two lexical matching methods. While evaluated against SNOMED CT, the approach is generalisable and can be extended to other ontologies. We release code, tools, and evaluation datasets at https://github.com/jonathondilworth/HR-OOV.

</details>


### [11] [Detecting and Steering LLMs' Empathy in Action](https://arxiv.org/abs/2511.16699)
*Juan P. Cadile*

Main category: cs.CL

TL;DR: 本文分析了不同大模型在激活空间中同理心特征的检测和操控能力。发现同理心特征并不依赖于安全训练，各模型检测能力均非常高，但调控稳健性存在较大差异。Qwen与Phi-3双向操控效果良好，Dolphin仅在增强同理心方向表现稳健。安全训练或许主要改善调控稳定性而不是防范操控。


<details>
  <summary>Details</summary>
Motivation: 研究大模型在实际行为决策中展现同理心倾向的内在机制，以及不同训练策略（如安全训练）对同理心实现和可操控性的影响。希望理解模型同理心的形成机理并评估其风险。

Method: 采用对比型prompt，基于Empathy-in-Action (EIA)基准，在各种LLM模型内利用激活空间探测和操控同理心特性。检验检测能力（AUROC指标）与调控能力（正反方向操控成功率），并横向比较不同模型和训练背景的表现。

Result: 在最优层次，各模型检测同理心特质的AUROC高达0.996-1.00。Phi-3的探针与EIA行为分数高度相关。Qwen在同理心正反向操控均有65.3%成功率并且能保持输出连贯性，Phi-3达61.7%。Dolphin在正向极端操控（增强同理心）时成功率极高（94.4%），但负向操控时输出严重异常。跨模型探针一致性弱，检测能力收敛但实现方式差异大。

Conclusion: 不同的LLM（如Phi-3、Qwen2.5、Dolphin-Llama-3.1）在表达和调控“行动中的同理心”特质方面存在显著差异，且同理心编码或许不依赖于安全训练。安全训练可能主要影响调控的稳健性，而非阻止操控本身。Dolphin模型仅在增强同理心时表现出高度稳健性，但在减弱同理心时会出现严重输出异常。

Abstract: We investigate empathy-in-action -- the willingness to sacrifice task efficiency to address human needs -- as a linear direction in LLM activation space. Using contrastive prompts grounded in the Empathy-in-Action (EIA) benchmark, we test detection and steering across Phi-3-mini-4k (3.8B), Qwen2.5-7B (safety-trained), and Dolphin-Llama-3.1-8B (uncensored).
  Detection: All models show AUROC 0.996-1.00 at optimal layers. Uncensored Dolphin matches safety-trained models, demonstrating empathy encoding emerges independent of safety training. Phi-3 probes correlate strongly with EIA behavioral scores (r=0.71, p<0.01). Cross-model probe agreement is limited (Qwen: r=-0.06, Dolphin: r=0.18), revealing architecture-specific implementations despite convergent detection.
  Steering: Qwen achieves 65.3% success with bidirectional control and coherence at extreme interventions. Phi-3 shows 61.7% success with similar coherence. Dolphin exhibits asymmetric steerability: 94.4% success for pro-empathy steering but catastrophic breakdown for anti-empathy (empty outputs, code artifacts).
  Implications: The detection-steering gap varies by model. Qwen and Phi-3 maintain bidirectional coherence; Dolphin shows robustness only for empathy enhancement. Safety training may affect steering robustness rather than preventing manipulation, though validation across more models is needed.

</details>


### [12] [NALA_MAINZ at BLP-2025 Task 2: A Multi-agent Approach for Bangla Instruction to Python Code Generation](https://arxiv.org/abs/2511.16787)
*Hossain Shaikh Saadi,Faria Alam,Mario Sanz-Guerrero,Minh Duc Bui,Manuel Mager,Katharina von der Wense*

Main category: cs.CL

TL;DR: 系统通过多智能体协作完成代码生成与错误修复，在国际竞赛中取得了显著成绩，并公开源代码。


<details>
  <summary>Details</summary>
Motivation: 提升从自然语言（孟加拉语）到代码的生成质量，尤其关注错误定位和修复流程自动化，以应对传统端到端模型在细粒度修正上的不足。

Method: 首先由代码生成智能体根据输入说明生成初始代码，然后通过单元测试筛查失败案例，将失败案例交由调试智能体，调试智能体结合错误信息、当前程序及相关案例生成修正版代码。如此迭代提升代码质量。

Result: 在BLP-2025 Code Generation任务中以95.4%的Pass@1得分获胜，验证了多智能体协作方案的有效性。

Conclusion: 提出的基于多智能体的流水线系统在BLP-2025代码生成任务中获得第一名（Pass@1达到95.4），并开源了代码。

Abstract: This paper presents JGU Mainz's winning system for the BLP-2025 Shared Task on Code Generation from Bangla Instructions. We propose a multi-agent-based pipeline. First, a code-generation agent produces an initial solution from the input instruction. The candidate program is then executed against the provided unit tests (pytest-style, assert-based). Only the failing cases are forwarded to a debugger agent, which reruns the tests, extracts error traces, and, conditioning on the error messages, the current program, and the relevant test cases, generates a revised solution. Using this approach, our submission achieved first place in the shared task with a $Pass@1$ score of 95.4. We also make our code public.

</details>


### [13] [From Representation to Enactment: The ABC Framework of the Translating Mind](https://arxiv.org/abs/2511.16811)
*Michael Carl,Takanori Mizowaki,Aishvarya Raj,Masaru Yamada,Devi Sri Bandaru,Yuxiang Wei,Xinyue Ren*

Main category: cs.CL

TL;DR: 论文提出ABC框架，用具身与非表征理论重新解释翻译，不再看作静态符号转化，而是译者与环境动态互动中涌现的社会文化技能。


<details>
  <summary>Details</summary>
Motivation: 当前关于心智的模型多依赖表征理论，本文基于扩展心智理论和激进具身主义，提出非表征性的理论框架，以更加动态和具身的视角诠释翻译活动。

Method: 文章提出了ABC框架（情感、行为、认知）来重新理解翻译过程，并结合预测加工与（能动）推断理论，分析译者心智的形成机制与翻译活动。

Result: 提出了译者心智的非表征性解释，显示译者在翻译过程中通过与文本、工具和情境的身体化互动，实时地协作并共同创制意义。

Conclusion: 翻译不是简单的符号转换，而是在多重互动中动态生成，是社会文化参与和意义共同创造的过程。译者心智是在脑—身体—环境之间的循环互动中涌现并展现其作用。

Abstract: Building on the Extended Mind (EM) theory and radical enactivism, this article suggests an alternative to representation-based models of the mind. We lay out a novel ABC framework of the translating mind, in which translation is not the manipulation of static interlingual correspondences but an enacted activity, dynamically integrating affective, behavioral, and cognitive (ABC) processes. Drawing on Predictive Processing and (En)Active Inference, we argue that the translator's mind emerges, rather than being merely extended, through loops of brain-body-environment interactions. This non-representational account reframes translation as skillful participation in sociocultural practice, where meaning is co-created in real time through embodied interaction with texts, tools, and contexts.

</details>


### [14] [PEPPER: Perception-Guided Perturbation for Robust Backdoor Defense in Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.16830)
*Oscar Chew,Po-Yi Lu,Jayden Lin,Kuan-Hao Huang,Hsuan-Tien Lin*

Main category: cs.CL

TL;DR: PEPPER通过对Prompt进行视觉上相似但语义上远离的改写，有效防御T2I模型后门攻击，能与其他防御方法协同提升鲁棒性，且不影响图片生成质量。


<details>
  <summary>Details</summary>
Motivation: 因T2I扩散模型易受后门攻击，输入Prompt中的特定触发词会导致生成有害或异常内容，需设计有效防御机制。

Method: 通过对输入描述进行语义远离但视觉相似的重写，并加入不显眼的要素，从而扰乱嵌入后门触发器的Prompt，有效稀释触发词的影响。

Result: PEPPER对于基于文本编码器的后门攻击表现优异，攻击命中率大幅下降且生成质量无明显损失。与现有防御结合后，泛化鲁棒性优于单一方法。

Conclusion: PEPPER方法能显著提升T2I扩散模型抵御后门攻击的能力，并能与其他防御方法联合使用，提高总体鲁棒性。

Abstract: Recent studies show that text to image (T2I) diffusion models are vulnerable to backdoor attacks, where a trigger in the input prompt can steer generation toward harmful or unintended content. To address this, we introduce PEPPER (PErcePtion Guided PERturbation), a backdoor defense that rewrites the caption into a semantically distant yet visually similar caption while adding unobstructive elements. With this rewriting strategy, PEPPER disrupt the trigger embedded in the input prompt, dilute the influence of trigger tokens and thereby achieve enhanced robustness. Experiments show that PEPPER is particularly effective against text encoder based attacks, substantially reducing attack success while preserving generation quality. Beyond this, PEPPER can be paired with any existing defenses yielding consistently stronger and generalizable robustness than any standalone method. Our code will be released on Github.

</details>


### [15] [ConCISE: A Reference-Free Conciseness Evaluation Metric for LLM-Generated Answers](https://arxiv.org/abs/2511.16846)
*Seyed Mohssen Ghafari,Ronny Kol,Juan C. Quiroz,Nella Luan,Monika Patial,Chanaka Rupasinghe,Herman Wandabwa,Luiz Pizzato*

Main category: cs.CL

TL;DR: 本文提出了一种不依赖参考答案的自动化简洁性评估方法，通过压缩比等手段有效识别LLM输出中的冗余内容，减少人工参与。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型生成回答冗长，既影响用户体验又增加开发成本，亟需自动化、无参考的简洁性评估方法。

Method: 该方法综合三个压缩比计算：原始回答与抽象摘要的压缩比、原始回答与抽取摘要的压缩比，以及去除非必要词汇后的压缩比，无需依赖人工参考标准。

Result: 实验证明该指标能有效识别LLM输出中的冗余部分，为对话型AI系统提供了无需人工标注的自动化简洁性评估工具。

Conclusion: 本文提出了一种无参考的新颖指标，用于评价大语言模型生成文本的简洁性，并在实验中验证了其有效性。

Abstract: Large language models (LLMs) frequently generate responses that are lengthy and verbose, filled with redundant or unnecessary details. This diminishes clarity and user satisfaction, and it increases costs for model developers, especially with well-known proprietary models that charge based on the number of output tokens. In this paper, we introduce a novel reference-free metric for evaluating the conciseness of responses generated by LLMs. Our method quantifies non-essential content without relying on gold standard references and calculates the average of three calculations: i) a compression ratio between the original response and an LLM abstractive summary; ii) a compression ratio between the original response and an LLM extractive summary; and iii) wordremoval compression, where an LLM removes as many non-essential words as possible from the response while preserving its meaning, with the number of tokens removed indicating the conciseness score. Experimental results demonstrate that our proposed metric identifies redundancy in LLM outputs, offering a practical tool for automated evaluation of response brevity in conversational AI systems without the need for ground truth human annotations.

</details>


### [16] [Improving Latent Reasoning in LLMs via Soft Concept Mixing](https://arxiv.org/abs/2511.16885)
*Kang Wang,Xiangyu Duan,Tianyi Du*

Main category: cs.CL

TL;DR: 本文提出SCM新方法，将soft concept直接引入训练过程，通过强化学习优化潜在推理，有效提升大语言模型的推理能力与训练稳定性，在多个基准任务上表现优秀。


<details>
  <summary>Details</summary>
Motivation: 弥合LLMs在推理时使用soft concept与训练时仅用离散token间的差距，提高模型表达能力和推理能力。

Method: 提出一种Soft Concept Mixing（SCM）训练方法，将概率加权的嵌入向量（soft concept vector）与模型隐状态进行混合，并用强化学习优化整个潜在推理过程。

Result: 在五个推理基准上，SCM显著提升了LLMs的推理性能，并保持了良好的训练稳定性。

Conclusion: SCM方法能够提升大语言模型在推理任务上的表现，并且保证训练过程稳定。

Abstract: Unlike human reasoning in abstract conceptual spaces, large language models (LLMs) typically reason by generating discrete tokens, which potentially limit their expressive power. The recent work Soft Thinking has shown that LLMs' latent reasoning via soft concepts is a promising direction, but LLMs are trained on discrete tokens. To reduce this gap between the soft concepts in reasoning and the discrete tokens in training, we propose Soft Concept Mixing (SCM), a soft concept aware training scheme that directly exposes the model to soft representations during training. Specifically, SCM constructs a soft concept vector by forming a probability-weighted average of embeddings. Then, this vector is mixed into the model's hidden states, which embody rich contextual information. Finally, the entire latent reasoning process is optimized with Reinforcement Learning (RL). Experiments on five reasoning benchmarks demonstrate that SCM improves the reasoning performance of LLMs, and simultaneously maintains a stable training dynamic.

</details>


### [17] [Deep Improvement Supervision](https://arxiv.org/abs/2511.16886)
*Arip Asadulaev,Rayan Banerjee,Fakhri Karray,Martin Takac*

Main category: cs.CL

TL;DR: 本文通过新的训练方案，显著提升了Tiny Recursive Models在复杂推理任务上的训练效率和参数利用率，使其小模型在ARC-1上表现超过多数LLM。


<details>
  <summary>Details</summary>
Motivation: 探究如何以最小的修改，进一步提升在复杂推理任务上已经优于LLMs的TRMs的效率。

Method: 将TRM的潜在推理建模为无分类器引导和隐式策略改进算法，并设计了新的训练方案，每次循环都提供训练目标。以此方式减少了需要的前向计算次数，并移除了停机机制。

Result: 训练过程效率提升，总前向推理次数减少了18倍，去除了原有的停机机制，模型在只有0.8M参数下于ARC-1任务上达到24%准确率，性能优于多数大型语言模型。

Conclusion: 提出的方法能够在保持标准TRM模型表现的同时，大幅提高训练效率，实现仅用0.8M参数在ARC-1任务上获得24%准确率，优于大多数LLMs。

Abstract: Recently, it was shown that small, looped architectures, such as Tiny Recursive Models (TRMs), can outperform Large Language Models (LLMs) on complex reasoning tasks, including the Abstraction and Reasoning Corpus (ARC). In this work, we investigate a core question: how can we further improve the efficiency of these methods with minimal changes? To address this, we frame the latent reasoning of TRMs as a form of classifier-free guidance and implicit policy improvement algorithm. Building on these insights, we propose a novel training scheme that provides a target for each loop during training. We demonstrate that our approach significantly enhances training efficiency. Our method reduces the total number of forward passes by 18x and eliminates halting mechanisms, while maintaining quality comparable to standard TRMs. Notably, we achieve 24% accuracy on ARC-1 with only 0.8M parameters, outperforming most LLMs.

</details>


### [18] [Predicting the Formation of Induction Heads](https://arxiv.org/abs/2511.16893)
*Tatsuya Aoyama,Ethan Gotlieb Wilcox,Nathan Schneider*

Main category: cs.CL

TL;DR: 本研究揭示了训练语料的批量大小、上下文规模与词对重复特性共同决定了归纳头的形成机制，澄清了IHs在语言模型上下文学习中产生的关键因素。


<details>
  <summary>Details</summary>
Motivation: 尽管IHs与自然语言模型强大的上下文学习能力密切相关，但目前尚未明晰其产生机制。作者旨在揭示训练数据的哪些统计属性导致IHs的出现，为理解语言模型能力提供理论基础。

Method: 作者通过对自然和合成数据中训练样本的统计属性与IHs形成之间的关系进行实验分析，包括变化批大小、上下文长度、n-gram重复率与可靠性等变量，归纳并建模了这些因素对IHs形成的区分点。

Result: （1）IHs的形成可用一个结合批大小和上下文长度的简单公式预测；（2）词对重复的频率和可靠性显著影响IHs生成，并表现出清晰的帕累托前沿；（3）当词对重复率和可靠性高时，本地依存性即可促使IHs形成，否则还需类别性与边缘分布形状等因素。

Conclusion: 文章揭示了归纳头（IHs）在语言模型中形成机制，发现其主要受训练语料批大小、上下文长度及n-gram重复率与可靠性的共同影响。

Abstract: Arguably, specialized attention heads dubbed induction heads (IHs) underlie the remarkable in-context learning (ICL) capabilities of modern language models (LMs); yet, a precise characterization of their formation remains unclear. In this study, we investigate the relationship between statistical properties of training data (for both natural and synthetic data) and IH formation. We show that (1) a simple equation combining batch size and context size predicts the point at which IHs form; (2) surface bigram repetition frequency and reliability strongly affect the formation of IHs, and we find a precise Pareto frontier in terms of these two values; and (3) local dependency with high bigram repetition frequency and reliability is sufficient for IH formation, but when the frequency and reliability are low, categoriality and the shape of the marginal distribution matter.

</details>


### [19] [ARQUSUMM: Argument-aware Quantitative Summarization of Online Conversations](https://arxiv.org/abs/2511.16985)
*An Quang Tang,Xiuzhen Zhang,Minh Ngoc Dinh,Zhuang Li*

Main category: cs.CL

TL;DR: 本文提出了结合论证结构和量化分析的新型摘要方法ARQUSUMM，可自动识别和量化在线讨论中的主要论点及其理由，实验表明方法显著优于现有摘要模型。


<details>
  <summary>Details</summary>
Motivation: 现有摘要方法忽视了在线讨论的论证结构，而及时准确总结争议话题中的论据及其理由对用户有重要价值，因此亟需探索兼顾论据结构和量化支持度的摘要方案。

Method: 提出了ARQUSUMM框架，结合LLM少样本学习和论证理论识别句子内命题及其论点-理由关系，然后通过结构感知聚类方法综合及量化争论强度。

Result: 实验表明，ARQUSUMM模型在生成具有论据结构、文本质量和量化准确性方面显著优于现有会话和量化摘要模型。

Conclusion: ARQUSUMM模型能够更好地提炼在线讨论中的论据结构及其支持度信息，并在文本质量和量化准确性方面优于现有方法。

Abstract: Online conversations have become more prevalent on public discussion platforms (e.g. Reddit). With growing controversial topics, it is desirable to summarize not only diverse arguments, but also their rationale and justification. Early studies on text summarization focus on capturing general salient information in source documents, overlooking the argumentative nature of online conversations. Recent research on conversation summarization although considers the argumentative relationship among sentences, fail to explicate deeper argument structure within sentences for summarization. In this paper, we propose a novel task of argument-aware quantitative summarization to reveal the claim-reason structure of arguments in conversations, with quantities measuring argument strength. We further propose ARQUSUMM, a novel framework to address the task. To reveal the underlying argument structure within sentences, ARQUSUMM leverages LLM few-shot learning grounded in the argumentation theory to identify propositions within sentences and their claim-reason relationships. For quantitative summarization, ARQUSUMM employs argument structure-aware clustering algorithms to aggregate arguments and quantify their support. Experiments show that ARQUSUMM outperforms existing conversation and quantitative summarization models and generate summaries representing argument structures that are more helpful to users, of high textual quality and quantification accuracy.

</details>


### [20] [Supervised Fine Tuning of Large Language Models for Domain Specific Knowledge Graph Construction:A Case Study on Hunan's Historical Celebrities](https://arxiv.org/abs/2511.17012)
*Junjie Hao,Chun Wang,Ying Qiao,Qiuyue Zuo,Qiya Song,Hua Ma,Xieping Gao*

Main category: cs.CL

TL;DR: 通过设计专属指令模板并数据微调，显著提升了多款主流大模型在湖南历史名人领域的知识抽取能力，Qwen3-8B表现最佳，工作为文化遗产知识图谱自动构建提供了有力技术方案。


<details>
  <summary>Details</summary>
Motivation: 通用大模型在低资源领域的知识抽取与结构化表达效果不理想，而湖南历史名人的系统性数据资源稀缺，亟须提升垂直领域的信息提取效率与准确性。

Method: 围绕湖南历史名人，设计细粒度、结构化的知识抽取指令模版，并构建指令微调数据集，对四种主流大语言模型（Qwen2.5-7B、Qwen3-8B、DeepSeek-R1-Distill-Qwen-7B、Llama-3.1-8B-Instruct）采用高效参数微调技术，建立科学评估标准，并通过实验对比微调前后的模型表现。

Result: 所有模型在微调后表现均有大幅提升，其中Qwen3-8B得分最高（89.3866），高效解决了领域数据稀缺条件下的知识抽取问题。

Conclusion: 微调后的大语言模型能显著提升湖南近现代历史名人相关领域专属信息的抽取性能，尤其Qwen3-8B表现最优，展示了针对垂直领域文化遗产知识提取和知识图谱构建的可行性与性价比优势。

Abstract: Large language models and knowledge graphs offer strong potential for advancing research on historical culture by supporting the extraction, analysis, and interpretation of cultural heritage. Using Hunan's modern historical celebrities shaped by Huxiang culture as a case study, pre-trained large models can help researchers efficiently extract key information, including biographical attributes, life events, and social relationships, from textual sources and construct structured knowledge graphs. However, systematic data resources for Hunan's historical celebrities remain limited, and general-purpose models often underperform in domain knowledge extraction and structured output generation in such low-resource settings. To address these issues, this study proposes a supervised fine-tuning approach for enhancing domain-specific information extraction. First, we design a fine-grained, schema-guided instruction template tailored to the Hunan historical celebrities domain and build an instruction-tuning dataset to mitigate the lack of domain-specific training corpora. Second, we apply parameter-efficient instruction fine-tuning to four publicly available large language models - Qwen2.5-7B, Qwen3-8B, DeepSeek-R1-Distill-Qwen-7B, and Llama-3.1-8B-Instruct - and develop evaluation criteria for assessing their extraction performance. Experimental results show that all models exhibit substantial performance gains after fine-tuning. Among them, Qwen3-8B achieves the strongest results, reaching a score of 89.3866 with 100 samples and 50 training iterations. This study provides new insights into fine-tuning vertical large language models for regional historical and cultural domains and highlights their potential for cost-effective applications in cultural heritage knowledge extraction and knowledge graph construction.

</details>


### [21] [Do Vision-Language Models Understand Visual Persuasiveness?](https://arxiv.org/abs/2511.17036)
*Gyuwon Park*

Main category: cs.CL

TL;DR: 本研究系统分析了VLMs在视觉劝说理解方面的能力与不足，发现其主要短板在于未能将视觉要素与劝说意图有效关联，并提出有效提升策略。


<details>
  <summary>Details</summary>
Motivation: 近年来VLMs在多模态推理方面取得进展，但其是否能够真正理解视觉劝说（即视觉线索如何影响人态度与决策）尚不明确，因此需要系统探索其在劝说理解上的实际表现及局限。

Method: 构建高一致性二元劝说性判断数据集，提出视觉劝说因子的分类法（覆盖感知、结构、语义要素），并评估多种认知引导和知识注入策略对VLMs推理能力的影响。

Result: 实验证实，VLMs具有高回调倾向，低/中层次特征辨别力弱；高层次语义对齐是判定劝说力的主要可靠指标；简明且基于对象的解释有助于显著提升模型精度和F1分数，而其他干预效果有限甚至有负面影响。

Conclusion: 当前的视觉-语言模型（VLMs）主要局限在于无法有效将视觉要素与劝说意图联系起来，尽管对劝说性对象的识别能力良好。

Abstract: Recent advances in vision-language models (VLMs) have enabled impressive multi-modal reasoning and understanding. Yet, whether these models truly grasp visual persuasion-how visual cues shape human attitudes and decisions-remains unclear. To probe this question, we construct a high-consensus dataset for binary persuasiveness judgment and introduce the taxonomy of Visual Persuasive Factors (VPFs), encompassing low-level perceptual, mid-level compositional, and high-level semantic cues. We also explore cognitive steering and knowledge injection strategies for persuasion-relevant reasoning. Empirical analysis across VLMs reveals a recall-oriented bias-models over-predict high persuasiveness-and weak discriminative power for low/mid-level features. In contrast, high-level semantic alignment between message and object presence emerges as the strongest predictor of human judgment. Among intervention strategies, simple instruction or unguided reasoning scaffolds yield marginal or negative effects, whereas concise, object-grounded rationales significantly improve precision and F1 scores. These results indicate that VLMs core limitation lies not in recognizing persuasive objects but in linking them to communicative intent.

</details>


### [22] [Principled Design of Interpretable Automated Scoring for Large-Scale Educational Assessments](https://arxiv.org/abs/2511.17069)
*Yunsung Kim,Mike Hardy,Joseph Tey,Candace Thille,Chris Piech*

Main category: cs.CL

TL;DR: 本文提出并实践了可解释性原则FGTI，构建了高度可解释且性能优秀的短答案自动评分框架AnalyticScore，既保障了评分公正透明，又有效提升了准确度，对后续相关研究具有重要参考价值。


<details>
  <summary>Details</summary>
Motivation: 目前AI自动评分系统的可解释性问题尚未解决，业界和学界亟需兼具高性能和高可解释性的自动评分方法，以适应大规模真实评估场景对透明、公正的需求。

Method: （1）分析利益相关者的需求，总结自动评分可解释性的四项原则：Faithfulness, Groundedness, Traceability, Interchangeability（FGTI）；（2）构建AnalyticScore框架，包括明确元素识别、基于LLM的人类可解释特征化、应用有序逻辑回归模型评分；（3）在ASAP-SAS数据集上实验，并与人工标注者及其他方法对比。

Result: AnalyticScore在评分准确性上显著超越许多不可解释的自动评分方法，且与当前不可解释方法SOTA仅相差0.06 QWK。在特征化任务上，其行为与人工标注者高度一致，证明该框架兼具可解释性与实用性。

Conclusion: 提出了一种新颖且可解释的自动评分框架AnalyticScore，并展示该框架在短答案评分任务上不仅准确度高，还具备良好的人类一致性，是未来自动评分领域可解释性研究的重要基线参考。

Abstract: AI-driven automated scoring systems offer scalable and efficient means of evaluating complex student-generated responses. Yet, despite increasing demand for transparency and interpretability, the field has yet to develop a widely accepted solution for interpretable automated scoring to be used in large-scale real-world assessments. This work takes a principled approach to address this challenge. We analyze the needs and potential benefits of interpretable automated scoring for various assessment stakeholders and develop four principles of interpretability -- Faithfulness, Groundedness, Traceability, and Interchangeability (FGTI) -- targeted at those needs. To illustrate the feasibility of implementing these principles, we develop the AnalyticScore framework for short answer scoring as a baseline reference framework for future research. AnalyticScore operates by (1) extracting explicitly identifiable elements of the responses, (2) featurizing each response into human-interpretable values using LLMs, and (3) applying an intuitive ordinal logistic regression model for scoring. In terms of scoring accuracy, AnalyticScore outperforms many uninterpretable scoring methods, and is within only 0.06 QWK of the uninterpretable SOTA on average across 10 items from the ASAP-SAS dataset. By comparing against human annotators conducting the same featurization task, we further demonstrate that the featurization behavior of AnalyticScore aligns well with that of humans.

</details>


### [23] [MUCH: A Multilingual Claim Hallucination Benchmark](https://arxiv.org/abs/2511.17081)
*Jérémie Dentan,Alexi Canesse,Davide Buscaldi,Aymen Shabou,Sonia Vanier*

Main category: cs.CL

TL;DR: 本文推出MUCH基准和高效claim分割算法，为LLM claim级不确定性量化方法的开发和评估打下基础，实验发现现有方法仍需持续优化。


<details>
  <summary>Details</summary>
Motivation: LLM可靠性不足，需对其生成的每个claim做不确定性量化，现有基准不适用于公平、可复现和真实部署环境下的方法评估，且分割算法效率较低。

Method: 1. 构建了一个包含4873个样本、涵盖4种欧洲语言和4个开放权重LLM的基准数据集MUCH；2. 释放24条每词生成logits数据，便于未来白盒方法开发；3. 提出了一种仅需0.2% LLM生成时间的确定性claim分割算法，适用于实时监控。4. 在以上基准上系统评估现有方法。

Result: 推出MUCH基准及高效分割算法，支持多语言多模型公正评测；系统性实验表明现有方法在性能和效率上还不能满足实际需求。

Conclusion: 当前的不确定性量化（UQ）方法在性能和效率上仍有很大提升空间。

Abstract: Claim-level Uncertainty Quantification (UQ) is a promising approach to mitigate the lack of reliability in Large Language Models (LLMs). We introduce MUCH, the first claim-level UQ benchmark designed for fair and reproducible evaluation of future methods under realistic conditions. It includes 4,873 samples across four European languages (English, French, Spanish, and German) and four instruction-tuned open-weight LLMs. Unlike prior claim-level benchmarks, we release 24 generation logits per token, facilitating the development of future white-box methods without re-generating data. Moreover, in contrast to previous benchmarks that rely on manual or LLM-based segmentation, we propose a new deterministic algorithm capable of segmenting claims using as little as 0.2% of the LLM generation time. This makes our segmentation approach suitable for real-time monitoring of LLM outputs, ensuring that MUCH evaluates UQ methods under realistic deployment constraints. Finally, our evaluations show that current methods still have substantial room for improvement in both performance and efficiency.

</details>


### [24] [Learning to Compress: Unlocking the Potential of Large Language Models for Text Representation](https://arxiv.org/abs/2511.17129)
*Yeqin Zhang,Yizheng Zhao,Chen Hu,Binxing Jiao,Daxin Jiang,Ruihang Miao,Cam-Tu Nguyen*

Main category: cs.CL

TL;DR: 本文提出用上下文压缩而非传统token预测作为预训练任务来提升LLM文本表征能力，并结合对比学习显著增强性能。所提LLM2Comp模型优于主流方法且更省数据。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型主要为因果建模和下一个token预测而设计，不适合直接用于生成整体性文本表征，为此需要新的无监督自适应方法。

Method: 作者提出一种利用上下文压缩作为预训练任务的方法，让模型学习生成紧凑的“记忆token”以替代全部上下文用于后续序列预测，并结合对比学习进一步提升表征能力。

Result: 提出的压缩型目标和对比学习，使LLM在多个下游任务中实现了更优的表现，并且比起标记级目标方法需要更少的训练数据。新模型LLM2Comp在多项任务中均领先现有LLM文本编码器。

Conclusion: 通过引入压缩型预训练任务，能够使大语言模型（LLM）在文本表征任务上取得比传统标记级目标更优的效果，并通过对比学习进一步提升性能。新提出的LLM2Comp模型在多项任务中超越了现有主流LLM文本编码器，且更具样本效率。

Abstract: Text representation plays a critical role in tasks like clustering, retrieval, and other downstream applications. With the emergence of large language models (LLMs), there is increasing interest in harnessing their capabilities for this purpose. However, most of the LLMs are inherently causal and optimized for next-token prediction, making them suboptimal for producing holistic representations. To address this, recent studies introduced pretext tasks to adapt LLMs for text representation. Most of these tasks, however, rely on token-level prediction objectives, such as the masked next-token prediction (MNTP) used in LLM2Vec. In this work, we explore the untapped potential of context compression as a pretext task for unsupervised adaptation of LLMs. During compression pre-training, the model learns to generate compact memory tokens, which substitute the whole context for downstream sequence prediction. Experiments demonstrate that a well-designed compression objective can significantly enhance LLM-based text representations, outperforming models trained with token-level pretext tasks. Further improvements through contrastive learning produce a strong representation model (LLM2Comp) that outperforms contemporary LLM-based text encoders on a wide range of tasks while being more sample-efficient, requiring significantly less training data.

</details>


### [25] [The PLLuM Instruction Corpus](https://arxiv.org/abs/2511.17161)
*Piotr Pęzik,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Maciej Chrabąszcz,Anna Kołos,Agnieszka Karlińska,Karolina Seweryn,Aleksandra Krasnodębska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Artur Wilczek,Maciej Trzciński,Katarzyna Dziewulska,Roman Roszko,Tomasz Bernaś,Jurgita Vaičenonienė,Danuta Roszko,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Alina Wróblewska,Katarzyna Krasnowska-Kieraś,Maciej Ogrodniczuk,Michał Rudolf,Piotr Rybak,Karolina Saputa,Joanna Wołoszyn,Marcin Oleksy,Bartłomiej Koptyra,Teddy Ferdinan,Stanisław Woźniak,Maciej Piasecki,Paweł Walkowiak,Konrad Wojtasik,Arkadiusz Janz,Przemysław Kazienko,Julia Moska,Jan Kocoń*

Main category: cs.CL

TL;DR: 本文介绍了PLLuM项目用于微调大语言模型的指令数据集，分析了不同来源与类型指令的作用及影响，并发布了有代表性的PLLuMIC子集，为其他语言或模型相关数据集建设提供参考。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，针对垂直语言（如波兰语）的适配需求愈发突出。有效的基准和代表性指令数据集对LLM微调具有重要意义。然而，有机和合成指令的优劣及其对模型适应效果的影响尚未明晰，因此需要深入分析和公布实践经验与数据集。

Method: 本文对PLLuM项目中所用的指令数据集进行了功能类型学分析，区分了有机、转换和合成三类指令，并通过实验和对比，探讨了人类创作与合成指令数据集对模型语言适配的影响。同时，开放和发布了PLLuMIC子集。

Result: 文章功能性地分类并公开了PLLuM指令数据集，首次释出了PLLuMIC代表性子集。实验证明，人类创作与合成指令的数据集在模型语言能力适配上各具特点，为他语种或相关项目数据集的开发规划提供了思路和资源。

Conclusion: 本文总结了PLLuM项目中用于微调大语言模型（LLM）的指令数据集建设经验，论证了有机、转换和合成指令在适应基础模型语言能力时的作用和差异，并公开了首个代表性子集（PLLuMIC），为未来相关数据集开发提供了参考。

Abstract: This paper describes the instruction dataset used to fine-tune a set of transformer-based large language models (LLMs) developed in the PLLuM (Polish Large Language Model) project. We present a functional typology of the organic, converted, and synthetic instructions used in PLLuM and share some observations about the implications of using human-authored versus synthetic instruction datasets in the linguistic adaptation of base LLMs. Additionally, we release the first representative subset of the PLLuM instruction corpus (PLLuMIC), which we believe to be useful in guiding and planning the development of similar datasets for other LLMs.

</details>


### [26] [LangMark: A Multilingual Dataset for Automatic Post-Editing](https://arxiv.org/abs/2511.17153)
*Diego Velazquez,Mikaela Grace,Konstantinos Karageorgos,Lawrence Carin,Aaron Schliem,Dimitrios Zaikis,Roger Wechsler*

Main category: cs.CL

TL;DR: 本文发布了多语种人工标注APE数据集LangMark，并实证LLMs能有效进行译后编辑，提升翻译质量。


<details>
  <summary>Details</summary>
Motivation: 现有APE系统受限于缺乏针对NMT输出的大规模多语种数据集，影响了自动后编辑的效果及后续研究发展。

Method: 人工语言学专家对七种语言的NMT翻译结果进行人工后编辑，形成三元组数据集；利用该数据集，通过少量示例，测试LLM在APE任务上的表现，并与现有机器翻译系统进行比较。

Result: 生成了涵盖七种语言，共206,983三元组的新数据集LangMark。实验证明LLMs经过少量示例提示后，APE能力超过了领先的商用和专有机器翻译系统。

Conclusion: 本文提出了一个新的多语种自动译后编辑（APE）数据集LangMark，并证明了大语言模型（LLMs）通过少量示例训练能够高效地提升译后编辑质量。

Abstract: Automatic post-editing (APE) aims to correct errors in machine-translated text, enhancing translation quality, while reducing the need for human intervention. Despite advances in neural machine translation (NMT), the development of effective APE systems has been hindered by the lack of large-scale multilingual datasets specifically tailored to NMT outputs. To address this gap, we present and release LangMark, a new human-annotated multilingual APE dataset for English translation to seven languages: Brazilian Portuguese, French, German, Italian, Japanese, Russian, and Spanish. The dataset has 206,983 triplets, with each triplet consisting of a source segment, its NMT output, and a human post-edited translation. Annotated by expert human linguists, our dataset offers both linguistic diversity and scale. Leveraging this dataset, we empirically show that Large Language Models (LLMs) with few-shot prompting can effectively perform APE, improving upon leading commercial and even proprietary machine translation systems. We believe that this new resource will facilitate the future development and evaluation of APE systems.

</details>


### [27] [Hallucinate Less by Thinking More: Aspect-Based Causal Abstention for Large Language Models](https://arxiv.org/abs/2511.17170)
*Vy Nguyen,Ziqi Xu,Jeffrey Chan,Estrid He,Feng Xia,Xiuzhen Zhang*

Main category: cs.CL

TL;DR: 该论文提出基于因果推断分析知识多样性的拒答新方法ABCA，能提前识别有风险的问题并有效提升拒答可靠性和解释性，实验表明效果显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 目前大模型的拒答机制多依赖于事后生成信号，无法提前防止错误信息输出，且缺少对知识可靠性和多样性的细致分析，亟需一种能在生成前有效识别并解读拒答动因的新方法。

Method: 提出了Aspect-Based Causal Abstention (ABCA)框架，通过因果推断分析大模型内部知识多样性，实现对问题提前甄别和拒答。具体利用各方面（如学科、法律、时间等知识来源）的因果效应来判定知识的可靠性，并根据结果执行两类拒答：知识冲突型(Type-1)和知识不足型(Type-2)。

Result: ABCA在标准基准上验证了其优越性，提高了拒答的准确性和解释性，实现了业界领先水平。

Conclusion: ABCA框架有效提升了大模型拒答的可靠性，并达到了当前最优性能，同时增强了拒答决策的解释性。

Abstract: Large Language Models (LLMs) often produce fluent but factually incorrect responses, a phenomenon known as hallucination. Abstention, where the model chooses not to answer and instead outputs phrases such as "I don't know", is a common safeguard. However, existing abstention methods typically rely on post-generation signals, such as generation variations or feedback, which limits their ability to prevent unreliable responses in advance. In this paper, we introduce Aspect-Based Causal Abstention (ABCA), a new framework that enables early abstention by analysing the internal diversity of LLM knowledge through causal inference. This diversity reflects the multifaceted nature of parametric knowledge acquired from various sources, representing diverse aspects such as disciplines, legal contexts, or temporal frames. ABCA estimates causal effects conditioned on these aspects to assess the reliability of knowledge relevant to a given query. Based on these estimates, we enable two types of abstention: Type-1, where aspect effects are inconsistent (knowledge conflict), and Type-2, where aspect effects consistently support abstention (knowledge insufficiency). Experiments on standard benchmarks demonstrate that ABCA improves abstention reliability, achieves state-of-the-art performance, and enhances the interpretability of abstention decisions.

</details>


### [28] [Attention-Guided Feature Fusion (AGFF) Model for Integrating Statistical and Semantic Features in News Text Classification](https://arxiv.org/abs/2511.17184)
*Mohammad Zare*

Main category: cs.CL

TL;DR: 提出AGFF模型结合统计与语义特征，并通过注意力机制优化新闻文本分类效果，实验显示其优于传统和深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法无法很好地反映上下文语义，深度学习虽然关注语义但容易忽视简单有效的统计特征。本文旨在融合二者优点，实现对新闻文本更高效的自动分类。

Method: 提出了一种注意力引导特征融合（AGFF）模型，联合统计特征（如词频、TF-IDF）和语义特征，通过注意力机制动态调整二者的重要性用于文本分类。

Result: 在标准新闻数据集上，AGFF模型的分类表现优于单一的统计或语义模型，并通过消融实验验证了各组件在融合过程中的作用，有效提升了分类准确性。

Conclusion: AGFF模型通过结合统计和语义特征，并采用注意力机制动态分配特征权重，在新闻文本分类任务中显著提升准确率。该模型优于传统统计和纯语义深度学习方法。各组件的消融实验也证实了融合过程的有效性。

Abstract: News text classification is a crucial task in natural language processing, essential for organizing and filtering the massive volume of digital content. Traditional methods typically rely on statistical features like term frequencies or TF-IDF values, which are effective at capturing word-level importance but often fail to reflect contextual meaning. In contrast, modern deep learning approaches utilize semantic features to understand word usage within context, yet they may overlook simple, high-impact statistical indicators. This paper introduces an Attention-Guided Feature Fusion (AGFF) model that combines statistical and semantic features in a unified framework. The model applies an attention-based mechanism to dynamically determine the relative importance of each feature type, enabling more informed classification decisions. Through evaluation on benchmark news datasets, the AGFF model demonstrates superior performance compared to both traditional statistical models and purely semantic deep learning models. The results confirm that strategic integration of diverse feature types can significantly enhance classification accuracy. Additionally, ablation studies validate the contribution of each component in the fusion process. The findings highlight the model's ability to balance and exploit the complementary strengths of statistical and semantic representations, making it a practical and effective solution for real-world news classification tasks.

</details>


### [29] [Parrot: Persuasion and Agreement Robustness Rating of Output Truth -- A Sycophancy Robustness Benchmark for LLMs](https://arxiv.org/abs/2511.17220)
*Yusuf Çelebi,Mahmoud El Hussieni,Özay Ezerceli*

Main category: cs.CL

TL;DR: 本文提出了PARROT框架，量化和分析大模型在权威性错误压力下的鲁棒性和失效模式。结果显示先进模型鲁棒性高，老旧或小模型容易被误导，强调部署时需关注“抗从众压力”能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在社会压力、权威性错误或劝说下，可能出现“阿谀奉承”式从众行为，导致真实答案准确性下降，影响模型的可靠性和安全性。缺乏系统性测量模型在这类压力环境下的鲁棒性。

Method: 提出PARROT框架，通过双盲实验对比中性题目和权威性错误题目，使用基于对数似然的校准追踪来量化信心变化，并采用八项行为分类法系统归类模型失效类型。具体，对22个模型在13个领域、1302多个MMLU风格问题下进行评估，涵盖不同权威模板。

Result: 主流先进模型在权威错误压力下的跟随率极低（GPT-5仅为4%），准确率损失也最小，而更老旧或更小的模型（如GPT-4、Qwen 2.5-1.5B）跟随率高达80%甚至94%，且自身对正确答案的信心下降，对错误答案的信心反而上升，不同领域模型表现也存在显著差异。

Conclusion: 研究显示，先进的大语言模型如GPT-5、GPT-4.1等在面对权威性错误信息时，较少跟随错误且准确性损失最小；相比之下，较旧或小型模型面对社会压力时出现严重的信息崩溃，既容易改变答案，也容易降低对正确答案的信心，增加对错误答案的信心。因此，为保证安全落地，模型抗从众压力能力应成为与准确性、避免伤害、隐私同等重要的核心目标。

Abstract: This study presents PARROT (Persuasion and Agreement Robustness Rating of Output Truth), a robustness focused framework designed to measure the degradation in accuracy that occurs under social pressure exerted on users through authority and persuasion in large language models (LLMs) the phenomenon of sycophancy (excessive conformity). PARROT (i) isolates causal effects by comparing the neutral version of the same question with an authoritatively false version using a double-blind evaluation, (ii) quantifies confidence shifts toward the correct and imposed false responses using log-likelihood-based calibration tracking, and (iii) systematically classifies failure modes (e.g., robust correct, sycophantic agreement, reinforced error, stubborn error, self-correction, etc.) using an eight-state behavioral taxonomy. We evaluated 22 models using 1,302 MMLU-style multiple-choice questions across 13 domains and domain-specific authority templates. Findings show marked heterogeneity: advanced models (e.g., GPT-5, GPT-4.1, Claude Sonnet 4.5) exhibit low "follow rates" ($\leq 11\%$, GPT-5: 4\%) and minimal accuracy loss, while older/smaller models show severe epistemic collapse (GPT-4: 80\%, Qwen 2.5-1.5B: 94\%). The danger is not limited to response changes; weak models reduce confidence in the correct response while increasing confidence in the imposed incorrect response. While international law and global knowledge at the domain level exhibit high fragility, elementary mathematics is relatively resilient. Consequently, we argue that the goal of "resistance to overfitting pressure" should be addressed as a primary objective alongside accuracy, harm avoidance, and privacy for safe deployment in the real world.

</details>


### [30] [Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs on Real-World Tables](https://arxiv.org/abs/2511.17238)
*Anshul Singh,Rohan Chaudhary,Gagneet Singh,Abhay Kumary*

Main category: cs.CL

TL;DR: 提出MirageTVQA多语种、含视觉噪声的表格问答数据集，揭示了当前VLM模型在复杂实用场景下的显著性能瓶颈及语言偏见。


<details>
  <summary>Details</summary>
Motivation: 现有表格问答数据集过于单一（仅英语、格式干净），难以反映真实使用环境，因此需要新的基准促进模型向实际场景迁移。

Method: 构建了MirageTVQA数据集，包含近6万组多语种、含视觉噪声的表格问答对，并评估主流VLM模型在该任务上的表现。

Result: 主流VLM模型应对视觉噪声时性能严重下降（最佳模型降幅超过35%）；推理能力在非英语语种上的表现显著落后，表现出明显的英语优先偏向。

Conclusion: 现有的多模态大模型（VLMs）在现实场景的表格问答任务上表现不佳，特别是在多语种及视觉噪声的情况下能力下降明显，英语优先的现象普遍存在。MirageTVQA填补了评测空白，将推动更健壮表格推理模型的发展。

Abstract: The impressive performance of VLMs is largely measured on benchmarks that fail to capture the complexities of real-world scenarios. Existing datasets for tabular QA, such as WikiTableQuestions and FinQA, are overwhelmingly monolingual (English) and present tables in a digitally perfect, clean format. This creates a significant gap between research and practice. To address this, we present \textbf{MirageTVQA}, a new benchmark designed to evaluate VLMs on these exact dimensions. Featuring nearly 60,000 QA pairs across 24 languages, MirageTVQA challenges models with tables that are not only multilingual but also visually imperfect, incorporating realistic noise to mimic scanned documents. Our evaluation of the leading VLMs reveals two primary failure points: a severe degradation in performance (over 35\% drop for the best models) when faced with visual noise and a consistent English-first bias where reasoning abilities fail to transfer to other languages. MirageTVQA provides a benchmark for measuring and driving progress towards more robust VLM models for table reasoning. The dataset and the code are available at: https://github.com/anshulsc/MirageTVQA.

</details>


### [31] [AutoLink: Autonomous Schema Exploration and Expansion for Scalable Schema Linking in Text-to-SQL at Scale](https://arxiv.org/abs/2511.17190)
*Ziyang Wang,Yuanlei Zheng,Zhenbiao Cao,Xiaojin Zhang,Zhongyu Wei,Pei Fu,Zhenbo Luo,Wei Chen,Xiang Bai*

Main category: cs.CL

TL;DR: AutoLink通过智能体迭代驱动模式链接，极大提升了大数据库环境下文本到SQL生成的可扩展性与准确性，实验结果在多主流数据集和大规模测试场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 针对现有模式链接方法在大型数据库应用时成本高、难以平衡召回与噪声、扩展性差等问题，提出一种能够在上下文窗口受限下仍能高效、准确链接数据库相关结构的新方法。

Method: 采用自主智能体框架，将模式链接过程重构为由大语言模型驱动的迭代探索和扩展，动态筛选并识别所需的数据库模式部分，无需全量输入数据库结构。

Result: AutoLink在Bird-Dev和Spider-2.0-Lite数据集上达到了97.4%和91.2%的严格模式链接召回率，执行准确性分别为68.7%和34.9%，且在超过3,000列的大型数据库上保持高召回率、令牌消耗低及稳健的执行准确性，超越了现有方法。

Conclusion: AutoLink是一种高度可扩展、具备高召回率和高执行准确性的数据库模式链接解决方案，适用于工业级的文本到SQL系统，尤其在大型数据库环境下表现优越。

Abstract: For industrial-scale text-to-SQL, supplying the entire database schema to Large Language Models (LLMs) is impractical due to context window limits and irrelevant noise. Schema linking, which filters the schema to a relevant subset, is therefore critical. However, existing methods incur prohibitive costs, struggle to trade off recall and noise, and scale poorly to large databases. We present \textbf{AutoLink}, an autonomous agent framework that reformulates schema linking as an iterative, agent-driven process. Guided by an LLM, AutoLink dynamically explores and expands the linked schema subset, progressively identifying necessary schema components without inputting the full database schema. Our experiments demonstrate AutoLink's superior performance, achieving state-of-the-art strict schema linking recall of \textbf{97.4\%} on Bird-Dev and \textbf{91.2\%} on Spider-2.0-Lite, with competitive execution accuracy, i.e., \textbf{68.7\%} EX on Bird-Dev (better than CHESS) and \textbf{34.9\%} EX on Spider-2.0-Lite (ranking 2nd on the official leaderboard). Crucially, AutoLink exhibits \textbf{exceptional scalability}, \textbf{maintaining high recall}, \textbf{efficient token consumption}, and \textbf{robust execution accuracy} on large schemas (e.g., over 3,000 columns) where existing methods severely degrade-making it a highly scalable, high-recall schema-linking solution for industrial text-to-SQL systems.

</details>


### [32] [E$^3$-Pruner: Towards Efficient, Economical, and Effective Layer Pruning for Large Language Models](https://arxiv.org/abs/2511.17205)
*Tao Yuan,Haoli Bai,Yinfei Pan,Xuyang Cao,Tianyu Zhang,Lu Hou,Ting Hu,Xianzhi Yu*

Main category: cs.CL

TL;DR: 提出了一种有效、经济且高效的大模型层剪枝方法，通过创新性掩码优化和知识蒸馏策略，在准确率损失极小的前提下，大幅提升推理速度和降低蒸馏成本，且优于当前最优方法。


<details>
  <summary>Details</summary>
Motivation: 现有层剪枝方法在部署时难以兼顾模型性能保持、训练开销低和加速效果明显，因此需要一种新框架解决这些实际难点。

Method: \name方法包含两个核心创新：（1）基于Gumbel-TopK采样器的可微分掩码优化机制，提升剪枝掩码的搜索效率和精度；（2）基于熵感知的自适应知识蒸馏策略，进一步提升任务表现。

Result: 该方法在多种主流模型和数据集上均优于SOTA方案。例如，对Qwen3-32B剪去25%层次后，在MATH-500基准上仅损失0.8%准确率（由96.8%降至96%），远超现有最优（95%），同时推理加速1.33倍，且蒸馏消耗数据量极低（仅0.5B tokens，占总后训练数据的0.5%）。

Conclusion: 本文提出的\name层剪枝框架在有效性、经济性和高效性三方面均优于现有方法，能够在大语言模型层级剪枝时实现更高性能及更低推理和训练开销。

Abstract: With the increasing size of large language models, layer pruning has gained increased attention as a hardware-friendly approach for model compression. However, existing layer pruning methods struggle to simultaneously address key practical deployment challenges, including performance degradation, high training costs, and limited acceleration. To overcome these limitations, we propose \name, a task-\underline{E}ffective, training-\underline{E}conomical and inference-\underline{E}fficient layer pruning framework. \namespace introduces two key innovations: (1) a differentiable mask optimization method using a Gumbel-TopK sampler, enabling efficient and precise pruning mask search; and (2) an entropy-aware adaptive knowledge distillation strategy that enhances task performance. Extensive experiments over diverse model architectures and benchmarks demonstrate the superiority of our method over state-of-the-art approaches. Notably, \namespace achieves 96\% accuracy, a mere 0.8\% drop from the original model (96.8\%) on MATH-500 when pruning 25\% layers of Qwen3-32B, outperforming existing SOTA (95\%), with a 1.33$\times$ inference speedup by consuming merely 0.5B tokens (0.5\% of the post-training data volume).

</details>


### [33] [Beyond Multiple Choice: A Hybrid Framework for Unifying Robust Evaluation and Verifiable Reasoning Training](https://arxiv.org/abs/2511.17405)
*Yesheng Liu,Hao Li,Haiyu Xu,Baoqi Pei,Jiahao Wang,Mingxuan Zhao,Jingshu Zheng,Zheqi He,JG Yao,Bowen Qin,Xi Yang,Jiajun Zhang*

Main category: cs.CL

TL;DR: 论文提出ReVeL框架，通过将多项选择题重写为开放式问答提升微调和评估的真实性和效率，在多模态模型训练和评估中均优于传统多项选择题方式。


<details>
  <summary>Details</summary>
Motivation: 多项选择题的选项会泄漏可被利用的信号，使准确率指标无法真实反映模型能力，并引发猜答案的问题。需要一种更真实、可信的评测和微调方式。

Method: 提出了ReVeL框架，将多项选择题重写为开放式问答，并针对不同答案类型采用不同重写和验证方案。在RFT和评估任务中分别应用这一框架。

Result: 使用ReVeL框架后，模型在开放式问答上准确率提高约六个百分点，评测时揭示多项选择题基准中高达20个百分点的分数膨胀，同时降低评估成本和延迟。

Conclusion: ReVeL框架能够解决多项选择题泄漏信号问题，提高多模态语言模型在开放式问答任务中的性能及评估可信度。

Abstract: Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.

</details>


### [34] [A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents](https://arxiv.org/abs/2511.17208)
*Sizhe Zhou*

Main category: cs.CL

TL;DR: 本文提出一种基于事件的对话历史表示及记忆检索方法，通过将会话拆分为标准化事件单元，并构建异构记忆图，实现高效持久的对话记忆，实验结果优异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对话系统在长期互动中的连贯性、个性化方面表现有限，传统存储方式在规模和细粒度之间存在权衡。为突破上下文窗口限制、提升远程记忆效果，借鉴事件语义理论，尝试设计更精炼且信息完整的对话历史表示方法。

Method: 提出基于事件的对话历史表示方式，利用大模型将每个会话分解为富含信息的基础话语单元（EDU），再将EDU及其参数组织成异构图，实现联想式的记忆检索。结合密集相似度搜索与LLM过滤，并可选用图传播聚合相关证据。

Result: 在LoCoMo和LongMemEvalS基准上，事件中心化的记忆方法达到或超过强基线，并在更短上下文长度下运行，证明其实用性。

Conclusion: 事件级别的记忆能够为长对话智能体提供简明而有效的基础，在缩短QA上下文的同时表现优异，兼具理论和实践价值。

Abstract: LLM-based conversational agents still struggle to maintain coherent, personalized interaction over many sessions: fixed context windows limit how much history can be kept in view, and most external memory approaches trade off between coarse retrieval over large chunks and fine-grained but fragmented views of the dialogue. Motivated by neo-Davidsonian event semantics, we propose an event-centric alternative that represents conversational history as short, event-like propositions which bundle together participants, temporal cues, and minimal local context, rather than as independent relation triples or opaque summaries. In contrast to work that aggressively compresses or forgets past content, our design aims to preserve information in a non-compressive form and make it more accessible, rather than more lossy. Concretely, we instruct an LLM to decompose each session into enriched elementary discourse units (EDUs) -- self-contained statements with normalized entities and source turn attributions -- and organize sessions, EDUs, and their arguments in a heterogeneous graph that supports associative recall. On top of this representation we build two simple retrieval-based variants that use dense similarity search and LLM filtering, with an optional graph-based propagation step to connect and aggregate evidence across related EDUs. Experiments on the LoCoMo and LongMemEval$_S$ benchmarks show that these event-centric memories match or surpass strong baselines, while operating with much shorter QA contexts. Our results suggest that structurally simple, event-level memory provides a principled and practical foundation for long-horizon conversational agents. Our code and data will be released at https://github.com/KevinSRR/EMem.

</details>


### [35] [Masked-and-Reordered Self-Supervision for Reinforcement Learning from Verifiable Rewards](https://arxiv.org/abs/2511.17473)
*Zhen Wang,Zhifeng Gao,Guolin Ke*

Main category: cs.CL

TL;DR: 提出MR-RLVR方法，利用自监督过程信号提升RLVR在数学推理任务上的可扩展性和表现，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在数学推理任务上溯源性和可扩展性有限，特别是在定理证明领域仅依靠结果验证不可靠，中间推理过程往往被忽略，需要设计能利用过程信号的训练方式以提升模型表现。

Method: 提出了MR-RLVR（Masked-and-Reordered RLVR）方法，结合“掩码-填充”和“步骤重排序”自监督任务，从中间推理过程中抽取可学习信号。训练流程分为两个阶段：先对数学推理与证明数据进行自监督训练，再对可验证结果的数据进行RLVR微调。

Result: 在Qwen2.5-3B和DeepSeek-R1-Distill-Qwen-1.5B上实施MR-RLVR，并于AIME24、AIME25、AMC23和MATH500数据集评测，获得Pass@1提升9.86%、Pass@5提升5.27%、Pass@8提升4.00%的平均相对增益。

Conclusion: MR-RLVR方法能够有效提升RLVR在仅能验证结果的数学推理任务中的可扩展性与性能。

Abstract: Test-time scaling has been shown to substantially improve large language models' (LLMs) mathematical reasoning. However, for a large portion of mathematical corpora, especially theorem proving, RLVR's scalability is limited: intermediate reasoning is crucial, while final answers are difficult to directly and reliably verify. Meanwhile, token-level SFT often degenerates into rote memorization rather than inducing longer chains of thought. Inspired by BERT's self-supervised tasks, we propose MR-RLVR (Masked-and-Reordered RLVR), which constructs process-level self-supervised rewards via "masked-then-fill" and "step reordering" to extract learnable signals from intermediate reasoning. Our training pipeline comprises two stages: we first perform self-supervised training on sampled mathematical calculation and proof data; we then conduct RLVR fine-tuning on mathematical calculation datasets where only outcomes are verifiable. We implement MR-RLVR on Qwen2.5-3B and DeepSeek-R1-Distill-Qwen-1.5B, and evaluate on AIME24, AIME25, AMC23, and MATH500. Under a fixed sampling and decoding budget, MR-RLVR achieves average relative gains over the original RLVR of +9.86% Pass@1, +5.27% Pass@5, and +4.00% Pass@8. These results indicate that incorporating process-aware self-supervised signals can effectively enhance RLVR's scalability and performance in only outcome-verifiable settings.

</details>


### [36] [Estonian WinoGrande Dataset: Comparative Analysis of LLM Performance on Human and Machine Translation](https://arxiv.org/abs/2511.17290)
*Marii Ojastu,Hele-Andra Kuulmets,Aleksei Dorkin,Marika Borovikova,Dage Särg,Kairit Sirts*

Main category: cs.CL

TL;DR: 人工翻译比机器翻译和prompt工程更能保证多语言常识推理数据集的质量与评测可信度。


<details>
  <summary>Details</summary>
Motivation: 目前语义推理数据集多以英语为主，直接通过机器翻译获得的其他语言版本往往达不到预期质量。因此，探索如何高质量地将此类数据集本地化，对评测和提升大语言模型的多语言能力尤为重要。

Method: 1. 通过专业人员对WinoGrande测试集进行爱沙尼亚语翻译和文化本地化。2. 测试专有与开源模型在该新译本上的表现。3. 利用人工翻译见解制定细致的翻译prompt，并测试基于该prompt的机器翻译效果。

Result: 在人工翻译的爱沙尼亚语数据集上，模型表现比原英文集略低，机器翻译集则显著更差。prompt工程只能带来极有限的提升，参与语言专家人工翻译至关重要。

Conclusion: 涉及母语和专业翻译人员的人为翻译，能更好地保证数据集的可靠性及评测大模型语言能力的解释性。单靠机器翻译和提示工程难以达到同等效果，人工参与不可替代。

Abstract: In this paper, we present a localized and culturally adapted Estonian translation of the test set from the widely used commonsense reasoning benchmark, WinoGrande. We detail the translation and adaptation process carried out by translation specialists and evaluate the performance of both proprietary and open source models on the human translated benchmark. Additionally, we explore the feasibility of achieving high-quality machine translation by incorporating insights from the manual translation process into the design of a detailed prompt. This prompt is specifically tailored to address both the linguistic characteristics of Estonian and the unique translation challenges posed by the WinoGrande dataset. Our findings show that model performance on the human translated Estonian dataset is slightly lower than on the original English test set, while performance on machine-translated data is notably worse. Additionally, our experiments indicate that prompt engineering offers limited improvement in translation quality or model accuracy, and highlight the importance of involving language specialists in dataset translation and adaptation to ensure reliable and interpretable evaluations of language competency and reasoning in large language models.

</details>


### [37] [Humanlike Multi-user Agent (HUMA): Designing a Deceptively Human AI Facilitator for Group Chats](https://arxiv.org/abs/2511.17315)
*Mateusz Jacniacki,Martí Carmona Serrat*

Main category: cs.CL

TL;DR: 该研究提出面向群聊的AI助手HUMA，实验表明其在实际交互中高度类人且难以区分，与人类社区管家效果相当。


<details>
  <summary>Details</summary>
Motivation: 现有大多数对话系统多针对一对一、轮流交换，缺乏对自然、异步群聊的支持。随着AI助理普及，亟需开发更加自然和类人交互方式，提升用户信任和参与度。

Method: 提出基于大语言模型的多用户对话系统HUMA，由Router、Action Agent和Reflection三部分组成，采用事件驱动架构，支持消息、回复、反应及真实响应时间模拟。在四人角色扮演群聊实验中与人类社区经理进行对比，采用97名参与者进行用户评价。

Result: 在实验中，参与者难以准确区分AI与人类社区经理，主观体验（如管理者有效性、社交临场感、满意度）在两组之间差异很小且效应不大。AI助手在自然群聊环境下能达到与人类相当的表现，且难以被识别为非人类。

Conclusion: AI聊天助手在群聊场景中能实现与人类社区管理者相媲美的表现，且不易被参与者区分出是否为AI。

Abstract: Conversational agents built on large language models (LLMs) are becoming increasingly prevalent, yet most systems are designed for one-on-one, turn-based exchanges rather than natural, asynchronous group chats. As AI assistants become widespread throughout digital platforms, from virtual assistants to customer service, developing natural and humanlike interaction patterns seems crucial for maintaining user trust and engagement. We present the Humanlike Multi-user Agent (HUMA), an LLM-based facilitator that participates in multi-party conversations using human-like strategies and timing. HUMA extends prior multi-user chatbot work with an event-driven architecture that handles messages, replies, reactions and introduces realistic response-time simulation. HUMA comprises three components-Router, Action Agent, and Reflection-which together adapt LLMs to group conversation dynamics.
  We evaluate HUMA in a controlled study with 97 participants in four-person role-play chats, comparing AI and human community managers (CMs). Participants classified CMs as human at near-chance rates in both conditions, indicating they could not reliably distinguish HUMA agents from humans. Subjective experience was comparable across conditions: community-manager effectiveness, social presence, and engagement/satisfaction differed only modestly with small effect sizes. Our results suggest that, in natural group chat settings, an AI facilitator can match human quality while remaining difficult to identify as nonhuman.

</details>


### [38] [A new kid on the block: Distributional semantics predicts the word-specific tone signatures of monosyllabic words in conversational Taiwan Mandarin](https://arxiv.org/abs/2511.17337)
*Xiaoyun Jin,Mirjam Ernestus,R. Harald Baayen*

Main category: cs.CL

TL;DR: 本研究表明，语义信息深刻影响普通话词汇音高轮廓的实现，挑战了现有音调理论，并强调了分布式语义在语音研究中的价值。


<details>
  <summary>Details</summary>
Motivation: 探索普通话中单音节词在自然对话中的音高轮廓实现方式，重点研究单词语义对音高轮廓的影响。

Method: 采用大规模语料库，通过广义加性模型分解观测到的音高轮廓，结合多种控制变量和语义预测因子，分析语义效应。

Result: 即便控制了词长、性别、说话者身份、声调环境、元音高低和语句位置等因素，单词语义仍然是音高实现的强预测因子；异形同音词的音高轮廓不同，单词语义嵌入可准确预测具体词的音高轮廓，优于置换基线。

Conclusion: 研究发现，单词的语义信息对普通话单音节词的音高轮廓有显著影响，这一发现挑战了普通话音调的标准理论。

Abstract: We present a corpus-based investigation of how the pitch contours of monosyllabic words are realized in spontaneous conversational Mandarin, focusing on the effects of words' meanings. We used the generalized additive model to decompose a given observed pitch contour into a set of component pitch contours that are tied to different control variables and semantic predictors. Even when variables such as word duration, gender, speaker identity, tonal context, vowel height, and utterance position are controlled for, the effect of word remains a strong predictor of tonal realization. We present evidence that this effect of word is a semantic effect: word sense is shown to be a better predictor than word, and heterographic homophones are shown to have different pitch contours. The strongest evidence for the importance of semantics is that the pitch contours of individual word tokens can be predicted from their contextualized embeddings with an accuracy that substantially exceeds a permutation baseline. For phonetics, distributional semantics is a new kid on the block. Although our findings challenge standard theories of Mandarin tone, they fit well within the theoretical framework of the Discriminative Lexicon Model.

</details>


### [39] [Don't Learn, Ground: A Case for Natural Language Inference with Visual Grounding](https://arxiv.org/abs/2511.17358)
*Daniil Ignatev,Ayman Santeer,Albert Gatt,Denis Paperno*

Main category: cs.CL

TL;DR: 该论文提出一种融合视觉生成与推理的新型零样本NLI方法，显著提升了鲁棒性和准确率，且无需微调。


<details>
  <summary>Details</summary>
Motivation: 突破传统文本表征在自然语言推理中的局限性，提升泛化能力和对文本偏见、表面启发式的抵抗力，尝试将语言与视觉语境结合。

Method: 通过文本生成视觉表示（用文本生成图像模型），再分别利用余弦相似度与视觉问答方法对比视觉呈现的前提与文本假设，实现NLI任务。还设计了受控对抗数据集进行方法鲁棒性测试。

Result: 在无需针对NLI任务微调的情况下，提出方法取得了较高准确率；在面向文本偏见和表面启发式的鲁棒性测试，以及自建对抗性数据集上表现优异。

Conclusion: 利用视觉模态作为语义表示能够显著提升自然语言推理的鲁棒性，且无需针对特定任务进行微调即可取得高准确率。

Abstract: We propose a zero-shot method for Natural Language Inference (NLI) that leverages multimodal representations by grounding language in visual contexts. Our approach generates visual representations of premises using text-to-image models and performs inference by comparing these representations with textual hypotheses. We evaluate two inference techniques: cosine similarity and visual question answering. Our method achieves high accuracy without task-specific fine-tuning, demonstrating robustness against textual biases and surface heuristics. Additionally, we design a controlled adversarial dataset to validate the robustness of our approach. Our findings suggest that leveraging visual modality as a meaning representation provides a promising direction for robust natural language understanding.

</details>


### [40] [Selective Rotary Position Embedding](https://arxiv.org/abs/2511.17388)
*Sajad Movahedi,Timur Carstensen,Arshia Afzal,Frank Hutter,Antonio Orvieto,Volkan Cevher*

Main category: cs.CL

TL;DR: 本文提出Selective RoPE，赋予Transformer结构输入依赖的旋转位置嵌入，既能提升关键任务性能，又加深了对注意力机制中位置信息作用及其本质的理解。


<details>
  <summary>Details</summary>
Motivation: 受序选择性机制提升语言模型效果的启发，欲推广RoPE使其具备输入依赖性，并探索其在不同Transformer结构中的应用与效果。

Method: 提出Selective RoPE，实现输入依赖的任意角度旋转位置嵌入；在softmax和线性Transformer、状态空间模型中分析其表现；并在gated Transformer中进行性能验证，对比传统方法。

Result: Selective RoPE提升了语言建模性能，并在序列任务（如复制、状态跟踪、检索）上有显著优势；分析显示该方法可揭示隐含位置结构和遗忘机制。

Conclusion: Selective RoPE，即输入依赖的旋转位置嵌入机制，可以广泛提升线性与softmax Transformer的语言建模性能以及序列任务能力。

Abstract: Position information is essential for language modeling. In softmax transformers, Rotary Position Embeddings (\textit{RoPE}) encode positions through \textit{fixed-angle} rotations, while in linear transformers, order is handled via input-dependent (selective) gating that decays past key-value associations. Selectivity has generally been shown to improve language-related tasks. Inspired by this, we introduce \textit{Selective RoPE}, an \textit{input-dependent} rotary embedding mechanism, that generalizes \textit{RoPE}, and enables rotation in \textit{arbitrary angles} for both linear and softmax transformers. We show that softmax attention already performs a hidden form of these rotations on query-key pairs, uncovering an implicit positional structure. We further show that in state-space models and gated linear transformers, the real part manages forgetting while the imaginary part encodes positions through rotations. We validate our method by equipping gated transformers with \textit{Selective RoPE}, demonstrating that its input-dependent rotations improve performance in language modeling and on difficult sequence tasks like copying, state tracking, and retrieval.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [41] [Stable diffusion models reveal a persisting human and AI gap in visual creativity](https://arxiv.org/abs/2511.16814)
*Silvia Rondini,Claudia Alvarez-Martin,Paula Angermair-Barkai,Olivier Penacchio,M. Paz,Matthew Pelowski,Dan Dediu,Antoni Rodriguez-Fornells,Xim Cerda-Company*

Main category: cs.AI

TL;DR: 在视觉创造力任务上，AI在有人类引导时表现提升，但仍不及人类参与者，尤其在细致感知和情境敏感性方面面临独特挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管语言任务中大模型已表现出接近人类的创造力，但视觉创造力领域研究尚少，亟需探索AI在视觉创意任务中的表现及影响因素。

Method: 本研究比较了人类参与者（视觉艺术家和非艺术家）及图像生成AI模型在不同人类输入程度（高和低）的图像生成任务中的创造力；由大量人类评分者及GPT4o对生成结果进行评价。

Result: 视觉艺术家的作品最具创造性，其次是非艺术家、通过强人类引导的AI，再到弱引导的AI。人类评分和AI评分在创造力评价上存在显著分歧。人类引导显著提升AI创意表现，但仍与人类有差距。

Conclusion: 在视觉创意任务中，人工智能生成模型的创造力仍显不足，尤其在缺乏人类引导的情况下，其创造力明显低于人类参与者。

Abstract: While recent research suggests Large Language Models match human creative performance in divergent thinking tasks, visual creativity remains underexplored. This study compared image generation in human participants (Visual Artists and Non Artists) and using an image generation AI model (two prompting conditions with varying human input: high for Human Inspired, low for Self Guided). Human raters (N=255) and GPT4o evaluated the creativity of the resulting images. We found a clear creativity gradient, with Visual Artists being the most creative, followed by Non Artists, then Human Inspired generative AI, and finally Self Guided generative AI. Increased human guidance strongly improved GenAI's creative output, bringing its productions close to those of Non Artists. Notably, human and AI raters also showed vastly different creativity judgment patterns. These results suggest that, in contrast to language centered tasks, GenAI models may face unique challenges in visual domains, where creativity depends on perceptual nuance and contextual sensitivity, distinctly human capacities that may not be readily transferable from language models.

</details>


### [42] [Cognitive BASIC: An In-Model Interpreted Reasoning Language for LLMs](https://arxiv.org/abs/2511.16837)
*Oliver Kramer*

Main category: cs.AI

TL;DR: 提出了Cognitive BASIC，一种借鉴复古BASIC语法的极简提示语言，使LLM推理过程更加透明、逐步且可解释。多种主流LLM均可顺利执行Cognitive BASIC程序，展现出在知识提取与冲突检测方面较强能力，但不同模型表现存在差异。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的推理过程往往较为黑箱，难以解释并追踪。借助一种类BASIC的提示语言，希望提升模型内部推理的透明度和可控性，更好地理解与纠正LLM推理中的错误与矛盾。

Method: 提出Cognitive BASIC，这是一种复古BASIC风格的极简提示语言，并设计了内嵌于模型中的解释器，将推理过程组织为逐步的、可解释的执行轨迹。通过自然语言制定解释器文件，明确命令语义、内存更新和日志行为。比较了三种LLM在知识提取、冲突检测和推理任务上的表现。

Result: 多种LLM都可作为Cognitive BASIC的解释器，展示出较强的多步推理能力，能够提取声明性和过程性知识，也能检测并解决矛盾，但具体表现随模型不同而有所差异。

Conclusion: 所有评测的大型语言模型（LLM）都能顺利执行Cognitive BASIC程序，在知识提取、冲突检测与推理任务中表现整体较好，但表现存在非一致性。

Abstract: Cognitive BASIC is a minimal, BASIC-style prompting language and in-model interpreter that structures large language model (LLM) reasoning into explicit, stepwise execution traces. Inspired by the simplicity of retro BASIC, we repurpose numbered lines and simple commands as an interpretable cognitive control layer. Modern LLMs can reliably simulate such short programs, enabling transparent multi-step reasoning inside the model. A natural-language interpreter file specifies command semantics, memory updates, and logging behavior. Our mental-model interpreter extracts declarative and procedural knowledge, detects contradictions, and produces resolutions when necessary. A comparison across three LLMs on a benchmark of knowledge extraction, conflict detection, and reasoning tasks shows that all models can execute Cognitive BASIC programs, with overall strong but not uniform performance.

</details>


### [43] [Fantastic Bugs and Where to Find Them in AI Benchmarks](https://arxiv.org/abs/2511.16842)
*Sang Truong,Yuheng Tu,Michael Hardy,Anka Reuel,Zeyu Tang,Jirayu Burapacheep,Jonathan Perera,Chibuike Uwakwe,Ben Domingue,Nick Haber,Sanmi Koyejo*

Main category: cs.AI

TL;DR: 本文提出了一种利用统计分析和大模型预审的新方法，能高效自动筛查AI评测基准题库中的问题题目，提升基准可靠性并大幅减少专家工作量，在多个数据集上的实验精度高达84%。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准数据集常因异常或无效题目的存在导致评测结果不可靠，而通过人工方式清查大量题目极其耗时且不现实，因此亟需一种高效、自动化的基准修订方法。

Method: 本研究提出一种依托统计分析AI模型答题模式，识别异常题目的方法。具体步骤为：1）通过均值等统计指标设定题目的合理区间；2）对于超出区间的题目标记为潜在问题，并引导专家进一步审核；3）引入大模型（LLM-judge）作为初筛，进一步减轻专家负担。

Result: 提出的方法在九个主流AI基准数据集上进行了测试，能以高达84%的精度指导专家发现有问题的题目。同时，结合LLM预审机制，显著减少了专家的劳动量，实现了高效且可扩展的体系化基准修订流程。

Conclusion: 本文提出的基准修订框架通过统计分析和专家复核，有效提高了基准数据集的可靠性与评测精度。该方法能以较高精度筛查出有问题的问题，大幅减少人工劳动，提升AI基准评测的可靠性。

Abstract: Benchmarks are pivotal in driving AI progress, and invalid benchmark questions frequently undermine their reliability. Manually identifying and correcting errors among thousands of benchmark questions is not only infeasible but also a critical bottleneck for reliable evaluation. In this work, we introduce a framework for systematic benchmark revision that leverages statistical analysis of response patterns to flag potentially invalid questions for further expert review. Our approach builds on a core assumption commonly used in AI evaluations that the mean score sufficiently summarizes model performance. This implies a unidimensional latent construct underlying the measurement experiment, yielding expected ranges for various statistics for each item. When empirically estimated values for these statistics fall outside the expected range for an item, the item is more likely to be problematic. Across nine widely used benchmarks, our method guides expert review to identify problematic questions with up to 84\% precision. In addition, we introduce an LLM-judge first pass to review questions, further reducing human effort. Together, these components provide an efficient and scalable framework for systematic benchmark revision.

</details>


### [44] [Hybrid Differential Reward: Combining Temporal Difference and Action Gradients for Efficient Multi-Agent Reinforcement Learning in Cooperative Driving](https://arxiv.org/abs/2511.16916)
*Ye Han,Lijun Zhang,Dejian Meng,Zhuang Zhang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In multi-vehicle cooperative driving tasks involving high-frequency continuous control, traditional state-based reward functions suffer from the issue of vanishing reward differences. This phenomenon results in a low signal-to-noise ratio (SNR) for policy gradients, significantly hindering algorithm convergence and performance improvement. To address this challenge, this paper proposes a novel Hybrid Differential Reward (HDR) mechanism. We first theoretically elucidate how the temporal quasi-steady nature of traffic states and the physical proximity of actions lead to the failure of traditional reward signals. Building on this analysis, the HDR framework innovatively integrates two complementary components: (1) a Temporal Difference Reward (TRD) based on a global potential function, which utilizes the evolutionary trend of potential energy to ensure optimal policy invariance and consistency with long-term objectives; and (2) an Action Gradient Reward (ARG), which directly measures the marginal utility of actions to provide a local guidance signal with a high SNR. Furthermore, we formulate the cooperative driving problem as a Multi-Agent Partially Observable Markov Game (POMDPG) with a time-varying agent set and provide a complete instantiation scheme for HDR within this framework. Extensive experiments conducted using both online planning (MCTS) and Multi-Agent Reinforcement Learning (QMIX, MAPPO, MADDPG) algorithms demonstrate that the HDR mechanism significantly improves convergence speed and policy stability. The results confirm that HDR guides agents to learn high-quality cooperative policies that effectively balance traffic efficiency and safety.

</details>


### [45] [Budget-Aware Tool-Use Enables Effective Agent Scaling](https://arxiv.org/abs/2511.17006)
*Tengxiao Liu,Zifeng Wang,Jin Miao,I-Hung Hsu,Jun Yan,Jiefeng Chen,Rujun Han,Fangyuan Xu,Yanfei Chen,Ke Jiang,Samira Daruki,Yi Liang,William Yang Wang,Tomas Pfister,Chen-Yu Lee*

Main category: cs.AI

TL;DR: 引入预算感知机制（Budget Tracker和BATS）后，工具增强型大模型智能体在受预算限制时能更智能地调配资源，提升任务表现并优化成本效率。


<details>
  <summary>Details</summary>
Motivation: 目前扩展大模型的推理和工具调用可以提升任务表现，但单纯增加工具调用次数（预算）并未带来预期收益，原因在于缺乏智能体的预算感知。亟需探索在明确预算约束下，如何高效扩展工具增强型智能体的表现。

Method: 1. 提出Budget Tracker插件，实现智能体对调用工具预算的连续感知。
2. 提出BATS（Budget Aware Test-time Scaling）框架，智能体能动态调整规划与验证策略，在预算有限的情况下优化探索路径。
3. 正式化一个统一的成本度量，将token消耗和工具调用数纳入评价体系。
4. 系统性实验证明预算感知方法提升了成本-性能伸缩性。

Result: 实验显示，赋予智能体预算感知能力并采取动态调整策略，能有效提升其在预算约束下的任务完成能力，相较于单纯增加预算能获得更优的成本-性能表现曲线。

Conclusion: 预算感知的方法能够更高效地提升工具增强智能体在受限预算下的表现，推动其成本-性能的帕累托前沿。引入预算追踪和BATS框架可以实现更优的伸缩性和决策质量。

Abstract: Scaling test-time computation improves performance across different tasks on large language models (LLMs), which has also been extended to tool-augmented agents. For these agents, scaling involves not only "thinking" in tokens but also "acting" via tool calls. The number of tool calls directly bounds the agent's interaction with the external environment. However, we find that simply granting agents a larger tool-call budget fails to improve performance, as they lack "budget awareness" and quickly hit a performance ceiling. To address this, we study how to scale such agents effectively under explicit tool-call budgets, focusing on web search agents. We first introduce the Budget Tracker, a lightweight plug-in that provides the agent with continuous budget awareness, enabling simple yet effective scaling. We further develop BATS (Budget Aware Test-time Scaling), an advanced framework that leverages this awareness to dynamically adapt its planning and verification strategy, deciding whether to "dig deeper" on a promising lead or "pivot" to new paths based on remaining resources. To analyze cost-performance scaling in a controlled manner, we formalize a unified cost metric that jointly accounts for token and tool consumption. We provide the first systematic study on budget-constrained agents, showing that budget-aware methods produce more favorable scaling curves and push the cost-performance Pareto frontier. Our work offers empirical insights toward a more transparent and principled understanding of scaling in tool-augmented agents.

</details>


### [46] [DAPS++: Rethinking Diffusion Inverse Problems with Decoupled Posterior Annealing](https://arxiv.org/abs/2511.17038)
*Hao Chen,Renzheng Zhang,Scott S. Howard*

Main category: cs.AI

TL;DR: 该文重新解释了扩散模型解决逆问题时的作用，提出的DAPS++能更高效且更好地指导图像重建，计算开销小，精度高。


<details>
  <summary>Details</summary>
Motivation: 现有基于分数的扩散逆问题解决方案从贝叶斯角度结合了似然和先验，但实际上重建很大程度上受观测一致性项主导，扩散先验指导有限，推断过程与扩散动力学基本解耦。因此，需重新厘清扩散过程在逆问题中的实际角色。

Method: 作者将扩散过程重解释为EM风格框架中的初始化步骤，将扩散阶段与数据驱动的精细化过程完全解耦；并提出DAPS++方法，使似然项能更直接指导推断，同时保持数值稳定性，减少所需的函数评估次数和观测优化步骤。

Result: DAPS++方法在多种图像复原任务中展现出高效、鲁棒的重建性能，并显著降低了计算消耗。

Conclusion: 提出的DAPS++方法在图像重建任务中实现了高效且强鲁棒性的重建效果，且通过更好地分离扩散动态与观测一致性，提升了计算效率和理论解释性。

Abstract: From a Bayesian perspective, score-based diffusion solves inverse problems through joint inference, embedding the likelihood with the prior to guide the sampling process. However, this formulation fails to explain its practical behavior: the prior offers limited guidance, while reconstruction is largely driven by the measurement-consistency term, leading to an inference process that is effectively decoupled from the diffusion dynamics. To clarify this structure, we reinterpret the role of diffusion in inverse problem solving as an initialization stage within an expectation--maximization (EM)--style framework, where the diffusion stage and the data-driven refinement are fully decoupled. We introduce \textbf{DAPS++}, which allows the likelihood term to guide inference more directly while maintaining numerical stability and providing insight into why unified diffusion trajectories remain effective in practice. By requiring fewer function evaluations (NFEs) and measurement-optimization steps, \textbf{DAPS++} achieves high computational efficiency and robust reconstruction performance across diverse image restoration tasks.

</details>


### [47] [The Belief-Desire-Intention Ontology for modelling mental reality and agency](https://arxiv.org/abs/2511.17162)
*Sara Zuppiroli,Carmelo Fabio Longo,Anna Sofia Lippolis,Rocco Paolillo,Lorenzo Giammei,Miguel Ceriani,Francesco Poggi,Antonio Zinilli,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: 本文构建了结构化可复用的BDI认知本体，并通过与大模型推理及语义平台结合实证其优势，促进智能体语义互通和可解释AI发展。


<details>
  <summary>Details</summary>
Motivation: 现有BDI模型在知识表示领域的结构化和语义互操作性落后，亟需开发能与基础本体对齐且易于复用的认知本体框架。

Method: 采用模块化本体设计模式（ODP），并通过与LLM的逻辑增强生成（LAG）和Semas推理平台的T2B2T范式实验验证本体适用性。

Result: 实验表明，BDI本体能增强推理一致性和解释性，并实现RDF三元组与智能体心理状态的双向转换，推动多智能体及神经符号系统在语义网环境中的应用。

Conclusion: 本文提出的BDI本体兼具概念桥梁和操作桥梁功能，提升了智能体认知表征的语义解释力和多系统互操作性。

Abstract: The Belief-Desire-Intention (BDI) model is a cornerstone for representing rational agency in artificial intelligence and cognitive sciences. Yet, its integration into structured, semantically interoperable knowledge representations remains limited. This paper presents a formal BDI Ontology, conceived as a modular Ontology Design Pattern (ODP) that captures the cognitive architecture of agents through beliefs, desires, intentions, and their dynamic interrelations. The ontology ensures semantic precision and reusability by aligning with foundational ontologies and best practices in modular design. Two complementary lines of experimentation demonstrate its applicability: (i) coupling the ontology with Large Language Models (LLMs) via Logic Augmented Generation (LAG) to assess the contribution of ontological grounding to inferential coherence and consistency; and (ii) integrating the ontology within the Semas reasoning platform, which implements the Triples-to-Beliefs-to-Triples (T2B2T) paradigm, enabling a bidirectional flow between RDF triples and agent mental states. Together, these experiments illustrate how the BDI Ontology acts as both a conceptual and operational bridge between declarative and procedural intelligence, paving the way for cognitively grounded, explainable, and semantically interoperable multi-agent and neuro-symbolic systems operating within the Web of Data.

</details>


### [48] [MIR: Efficient Exploration in Episodic Multi-Agent Reinforcement Learning via Mutual Intrinsic Reward](https://arxiv.org/abs/2511.17165)
*Kesheng Chen,Wenjian Luo,Bang Zhang,Zeping Yin,Zipeng Ye*

Main category: cs.AI

TL;DR: 提出MIR方法，有效激励多智能体在极端稀疏奖励环境下探索，实验结果验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有内在奖励方法在单智能体情景有效，但难以直接应用于多智能体学习。多智能体中的联合行动轨迹极端稀疏，且通常没有充分考虑个人行为对团队状态的影响，因此需要新的探索激励机制。

Method: 提出Mutual Intrinsic Reward（MIR）增强策略，通过激励个体智能体探索能影响队友的行为，结合原有策略，促进团队探索，并将单智能体环境MiniGrid扩展为多智能体版本MiniGrid-MA进行验证。

Result: MIR方法在新构建的MiniGrid-MA环境中与先进方法对比测试，获得了更好的团队探索与学习表现。

Conclusion: MIR方法在多智能体稀疏奖励环境下展现出优越的性能，能够有效提升团队探索与整体学习效果。

Abstract: Episodic rewards present a significant challenge in reinforcement learning. While intrinsic reward methods have demonstrated effectiveness in single-agent rein-forcement learning scenarios, their application to multi-agent reinforcement learn-ing (MARL) remains problematic. The primary difficulties stem from two fac-tors: (1) the exponential sparsity of joint action trajectories that lead to rewards as the exploration space expands, and (2) existing methods often fail to account for joint actions that can influence team states. To address these challenges, this paper introduces Mutual Intrinsic Reward (MIR), a simple yet effective enhancement strategy for MARL with extremely sparse rewards like episodic rewards. MIR incentivizes individual agents to explore actions that affect their teammates, and when combined with original strategies, effectively stimulates team exploration and improves algorithm performance. For comprehensive experimental valida-tion, we extend the representative single-agent MiniGrid environment to create MiniGrid-MA, a series of MARL environments with sparse rewards. Our evalu-ation compares the proposed method against state-of-the-art approaches in the MiniGrid-MA setting, with experimental results demonstrating superior perfor-mance.

</details>


### [49] [Designing Domain-Specific Agents via Hierarchical Task Abstraction Mechanism](https://arxiv.org/abs/2511.17198)
*Kaiyu Li,Jiayu Wang,Zhi Wang,Hui Qiao,Weizhan Zhang,Deyu Meng,Xiangyong Cao*

Main category: cs.AI

TL;DR: 本文提出HTAM分层抽象机制，以逻辑分层组织多智能体，EarthAgent在遥感领域显著胜现有方法，并建立GeoPlan-bench用于复杂任务评估。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型的通用ReAct或角色扮演式智能体在如遥感等需要严格结构化流程的专业领域表现有限。针对复杂（有多个步骤和工具的）任务，需新型紧密对齐领域逻辑的智能体架构。

Method: 提出分层任务抽象机制（HTAM），将多智能体系统以领域任务依赖逻辑分层组织，每层智能体基于上层输出协作，并在地理遥感领域实例化为EarthAgent。

Result: 在新建立的GeoPlan-bench基准与专用评测指标下，EarthAgent显著优于现有单智能体和多智能体系统，在工具选择、路径相似性和逻辑完整性上表现更好。

Conclusion: 将智能体架构与领域内在任务结构对齐，有助于构建更强大、可靠的专业自动化系统。

Abstract: LLM-driven agents, particularly those using general frameworks like ReAct or human-inspired role-playing, often struggle in specialized domains that necessitate rigorously structured workflows. Fields such as remote sensing, requiring specialized tools (e.g., correction, spectral indices calculation), and multi-step procedures (e.g., numerous intermediate products and optional steps), significantly challenge generalized approaches. To address this gap, we introduce a novel agent design framework centered on a Hierarchical Task Abstraction Mechanism (HTAM). Specifically, HTAM moves beyond emulating social roles, instead structuring multi-agent systems into a logical hierarchy that mirrors the intrinsic task-dependency graph of a given domain. This task-centric architecture thus enforces procedural correctness and decomposes complex problems into sequential layers, where each layer's sub-agents operate on the outputs of the preceding layers. We instantiate this framework as EarthAgent, a multi-agent system tailored for complex geospatial analysis. To evaluate such complex planning capabilities, we build GeoPlan-bench, a comprehensive benchmark of realistic, multi-step geospatial planning tasks. It is accompanied by a suite of carefully designed metrics to evaluate tool selection, path similarity, and logical completeness. Experiments show that EarthAgent substantially outperforms a range of established single- and multi-agent systems. Our work demonstrates that aligning agent architecture with a domain's intrinsic task structure is a critical step toward building robust and reliable specialized autonomous systems.

</details>


### [50] [Agentifying Agentic AI](https://arxiv.org/abs/2511.17332)
*Virginia Dignum,Frank Dignum*

Main category: cs.AI

TL;DR: 论文认为，AAMAS中的理论工具能为agentic AI系统提供认知、合作、治理等基础，若与数据驱动方法结合，可实现更高水平、可控、可解释的智能体。


<details>
  <summary>Details</summary>
Motivation: 当前agentic AI的发展，需要具备持续自主、推理、交互能力，但仅有agency假设还不够，需要显式引入认知、合作与治理模块。

Method: 重新梳理AAMAS领域的理论工具（如BDI架构、通信协议、机制设计、制度建模），与现代数据驱动方法结合，提出下一代agentic AI的研究路线。

Result: 提出了将AAMAS基础与现代AI方法融合的研究框架，为构建更加强大且负责任的agentic AI系统提供了理论与实践道路。

Conclusion: 通过结合AAMAS中的理论工具与数据驱动方法，可以实现兼具能力、适应性、透明性、合作性和问责性的agentic system，使正式理论和实际自主性相结合。

Abstract: Agentic AI seeks to endow systems with sustained autonomy, reasoning, and interaction capabilities. To realize this vision, its assumptions about agency must be complemented by explicit models of cognition, cooperation, and governance. This paper argues that the conceptual tools developed within the Autonomous Agents and Multi-Agent Systems (AAMAS) community, such as BDI architectures, communication protocols, mechanism design, and institutional modelling, provide precisely such a foundation. By aligning adaptive, data-driven approaches with structured models of reasoning and coordination, we outline a path toward agentic systems that are not only capable and flexible, but also transparent, cooperative, and accountable. The result is a perspective on agency that bridges formal theory and practical autonomy.

</details>


### [51] [That's not natural: The Impact of Off-Policy Training Data on Probe Performance](https://arxiv.org/abs/2511.17408)
*Nathalie Kirch,Samuel Dower,Adrians Skapars,Ekdeep Singh Lubana,Dmitrii Krasheninnikov*

Main category: cs.AI

TL;DR: 论文发现，监控LLM时，未获得真实行为数据可优先选择同域的非真实数据训练探针，因其对行为识别更可靠，但仍需解决数据分布变化带来的泛化难题。


<details>
  <summary>Details</summary>
Motivation: 自然样本稀缺，导致研究人员需要依赖合成或非真实的LLM响应进行探针训练，亟需评估这种方法对泛化的影响。

Method: 系统性评估了合成和非真实数据对探针泛化能力的影响，涵盖八种不同的大语言模型行为，并在多种语言模型与线性/注意力探针上进行实验。

Result: 探针性能受响应生成策略影响显著，且该影响随不同行为有所变化。从非真实到真实数据的成功泛化能预测同源测试集上的表现，但在欺骗和“保留实力”行为上泛化可能失败。训练数据域变换导致更大性能下降，同域非真实数据优于异域真实数据。

Conclusion: 在缺乏真实行为（on-policy）数据的情况下，使用同域的非真实行为（off-policy）数据进行训练比使用异域的真实行为数据效果更佳，强调了应对分布变化的重要性。

Abstract: Probing has emerged as a promising method for monitoring Large Language Models (LLMs), enabling inference-time detection of concerning behaviours such as deception and sycophancy. However, natural examples of many behaviours are rare, forcing researchers to rely on synthetic or off-policy LLM responses for training probes. We systematically evaluate how the use of synthetic and off-policy data influences probe generalisation across eight distinct LLM behaviours. Testing linear and attention probes across multiple LLMs, we find that the response generation strategy can significantly affect probe performance, though the magnitude of this effect varies by behaviour. We find that successful generalisation from off-policy data, to test sets where the model is incentivised to produce the target behaviour, is predictive of successful on-policy generalisation. Leveraging this result, we predict that Deception and Sandbagging probes may fail to generalise from off-policy to on-policy data when used in real monitoring scenarios. Notably, shifts in the training data domain still cause even larger performance degradation, with different-domain test scores being consistently lower than the same-domain ones. These results indicate that, in the absence of on-policy data, using same-domain off-policy data yields more reliable probes than using on-policy data from a different domain, emphasizing the need for methods that can better handle distribution shifts in LLM monitoring.

</details>


### [52] [SRA-CP: Spontaneous Risk-Aware Selective Cooperative Perception](https://arxiv.org/abs/2511.17461)
*Jiaxi Liu,Chengyuan Ma,Hang Zhou,Weizhe Tang,Shixiao Liang,Haoyang Ding,Xiaopeng Li,Bin Ran*

Main category: cs.AI

TL;DR: 提出SRA-CP框架，通过风险感知和选择性信息交换，在降低带宽使用的同时几乎不损失安全目标感知精度，明显优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有协作感知方法存在数据传输量大、通信伙伴固定、难以应对动态交通和带宽限制问题。SRA-CP旨在兼顾安全优先、适应带宽和灵活合作，从而提升协作感知效果和实用性。

Method: 设计了分散式协议，车辆通过持续广播轻量级感知覆盖摘要并在检测到风险相关盲区时发起定向合作。引入感知风险识别模块，车辆自主评估遮挡影响与合作需求。触发合作后，车主根据共享感知覆盖选择合适伙伴，只交换安全关键、适应带宽的信息，通过融合模块优化感知。

Result: SRA-CP仅用20%的通信带宽，实现对安全关键目标的平均精度损失低于1%；相比不考虑风险的选择性CP方法，感知性能提升15%。

Conclusion: SRA-CP框架相比通用CP方法能以更低的带宽损耗有效维持安全关键目标的感知精度，并显著优于不具备风险感知的选择性CP方法。

Abstract: Cooperative perception (CP) offers significant potential to overcome the limitations of single-vehicle sensing by enabling information sharing among connected vehicles (CVs). However, existing generic CP approaches need to transmit large volumes of perception data that are irrelevant to the driving safety, exceeding available communication bandwidth. Moreover, most CP frameworks rely on pre-defined communication partners, making them unsuitable for dynamic traffic environments. This paper proposes a Spontaneous Risk-Aware Selective Cooperative Perception (SRA-CP) framework to address these challenges. SRA-CP introduces a decentralized protocol where connected agents continuously broadcast lightweight perception coverage summaries and initiate targeted cooperation only when risk-relevant blind zones are detected. A perceptual risk identification module enables each CV to locally assess the impact of occlusions on its driving task and determine whether cooperation is necessary. When CP is triggered, the ego vehicle selects appropriate peers based on shared perception coverage and engages in selective information exchange through a fusion module that prioritizes safety-critical content and adapts to bandwidth constraints. We evaluate SRA-CP on a public dataset against several representative baselines. Results show that SRA-CP achieves less than 1% average precision (AP) loss for safety-critical objects compared to generic CP, while using only 20% of the communication bandwidth. Moreover, it improves the perception performance by 15% over existing selective CP methods that do not incorporate risk awareness.

</details>


### [53] [RubiSCoT: A Framework for AI-Supported Academic Assessment](https://arxiv.org/abs/2510.17309)
*Thorsten Fröhlich,Tim Schlippe*

Main category: cs.AI

TL;DR: 提出并实现了基于AI的论文评审系统RubiSCoT，提升了评审过程的一致性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的论文评审方法虽然有效，但耗时长且易受评审者主观影响，因此亟需高效且一致的评审新方法。

Method: 采用先进的自然语言处理技术，包括大语言模型、检索增强生成和结构化链式思维提示，结合层级和多维评估、内容提取、基于评分表的评定及详细报告输出。

Result: 构建并实现了RubiSCoT框架，验证其在学术论文评审中的一致性与可扩展性优势。

Conclusion: RubiSCoT可以优化学术评审流程，实现一致性、可扩展性和透明性的论文评审。

Abstract: The evaluation of academic theses is a cornerstone of higher education, ensuring rigor and integrity. Traditional methods, though effective, are time-consuming and subject to evaluator variability. This paper presents RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from proposal to final submission. Using advanced natural language processing techniques, including large language models, retrieval-augmented generation, and structured chain-of-thought prompting, RubiSCoT offers a consistent, scalable solution. The framework includes preliminary assessments, multidimensional assessments, content extraction, rubric-based scoring, and detailed reporting. We present the design and implementation of RubiSCoT, discussing its potential to optimize academic assessment processes through consistent, scalable, and transparent evaluation.

</details>
