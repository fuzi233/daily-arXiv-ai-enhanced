<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 64]
- [cs.AI](#cs.AI) [Total: 26]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Data Analysis and Performance Evaluation of Simulation Deduction Based on LLMs](https://arxiv.org/abs/2511.10651)
*Shansi Zhang,Min Li*

Main category: cs.CL

TL;DR: 针对军事模拟推演的数据分析，作者提出利用大语言模型进行多步分解、互动及工具辅助，显著提升报告质量与评分。


<details>
  <summary>Details</summary>
Motivation: 现代战争中数据分析和性能评价至关重要，以往人工分析方法耗时且容易出现人为错误。作者希望提升分析效率与准确性。

Method: 提出了将复杂任务分解为多个子任务的方法，并为每个子任务设计有效的问题提示和系统提示。通过与大语言模型进行多轮互动，包括自检和反思，实现结构化数据提取及多步分析。同时自定义工具用于生成图表和计算指标，并设计多种报告模板适应不同场景。

Result: 基于所提出的方法生成的分析报告质量更高，评分超越了基线方法，表现出更好的效果。

Conclusion: 通过任务分解、多轮交互、自定义工具及模板设计，能够显著提升模拟推演数据分析报告的质量和适应性。

Abstract: Data analysis and performance evaluation of simulation deduction plays a pivotal role in modern warfare, which enables military personnel to gain invaluable insights into the potential effectiveness of different strategies, tactics, and operational plans. Traditional manual analysis approach is time-consuming and limited by human errors. To enhance efficiency and accuracy, large language models (LLMs) with strong analytical and inferencing capabilities can be employed. However, high-quality analysis reports with well-structured formatting cannot be obtained through a single instruction input to the LLM. To tackle this issue, we propose a method that first decomposes the complex task into several sub-tasks and designs effective system prompts and user prompts for each sub-task. Multi-round interactions with the LLM incorporating self-check and reflection are then conducted to enable structured data extraction as well as multi-step analysis and evaluation. Furthermore, custom tools are defined and invoked to generate figures and compute metrics. We also design multiple report templates, each tailored to a specific application and input data type, ensuring their adaptability across a variety of scenarios. Extensive evaluation results demonstrate that the reports generated by our method exhibit higher quality, therefore obtaining higher scores than the baseline method.

</details>


### [2] [Cognitively-Inspired Episodic Memory Architectures for Accurate and Efficient Character AI](https://arxiv.org/abs/2511.10652)
*Rafael Arias Gonzalez,Steve DiPaola*

Main category: cs.CL

TL;DR: 提出高效历史人物对话系统架构，通过结构化情感记忆和并行检索，显著提升小模型表现与响应速度，支持丰富可视化，应用前景广泛。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型展现了具身历史人物对话系统的潜力，但当前方法在浅层回应和深度延迟之间存在关键权衡。研究动机在于解决兼顾响应深度与生成效率的问题。

Method: 提出一种离线数据增强与高效并行检索结构化情节记忆的架构。首先将人物传记数据转化为1774条丰富的第一人称记忆，并加入情感-语义元数据。随后采用两阶段检索实现快速生成提示。

Result: 系统实现0.52秒的提示生成速度，LLM-as-judge与RAGAs评测显示其在GPT-4上能与传统RAG效果持平，并在GPT-3.5与GPT-3上明显优于传统方法，在资源受限环境下更具优势。结构化记忆还能支持可视化工具，如时空热力图、情感轨迹分析与交互路径追踪。

Conclusion: 新架构兼顾了深度和效率，成为对话接口与人物研究工具，在教育、博物馆、研究等需高效准确性的场景中具有实用价值，通用于有丰富文本资料的历史人物。

Abstract: Large language models show promise for embodying historical characters in dialogue systems, but existing approaches face a critical trade-off: simple retrieval-augmented generation produces shallow responses, while multi-stage reflection achieves depth at prohibitive latency. We present an architecture that resolves this tension through offline data augmentation and efficient parallel retrieval from structured episodic memory. Our system transforms biographical data into 1,774 enriched first-person memories with affective-semantic metadata, then employs two-stage retrieval achieving 0.52s prompt generation. Evaluation using LLM-as-judge and RAGAs metrics shows our approach achieves parity with traditional RAG on GPT-4 while significantly outperforming it on smaller models (GPT-3.5, GPT-3), suggesting particular value for resource-constrained deployments. Beyond dialogue, the structured memory enables novel visualization tools: spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, positioning the system as both a dialogue interface and research tool for biographical analysis. We use Van Gogh as a test case, but the architecture is generalizable to any historical figure with substantial textual records, offering a practical framework for educational, museum, and research applications requiring both accuracy and efficiency

</details>


### [3] [Hybrid Quantum Transformer for Language Generation](https://arxiv.org/abs/2511.10653)
*Desheng Kong,Xiangshuo Cui,Jiaying Jin,Jing Xu,Donglin Wang*

Main category: cs.CL

TL;DR: HyQuT是首个将变分量子电路集成到大语言模型中的混合量子-经典模型，仅用少量量子比特即可部分替代大模型参数，保持性能，为量子与生成式AI结合提供早期证明。


<details>
  <summary>Details</summary>
Motivation: 量子计算在增强传统计算方面展现潜力，但在复杂自然语言生成任务上的应用尚未取得突破。作者旨在探索量子方法在大规模语言模型上的可行性与优势。

Method: 将变分量子电路（VQC）集成到Transformer架构中，在8M和150M参数规模的模型上进行实验。

Result: 仅用10个量子比特和80个量子门即可替换150M参数模型中约10%的经典参数，并达到类似的性能和生成质量，首次验证了该方向的可行性。

Conclusion: 该研究首次展示了量子计算可与大规模生成型语言模型（LLM）融合，并保持生成质量和收敛稳定性。

Abstract: Although quantum computing has been increasingly applied to replace classical computation, most existing quantum or hybrid models remain confined to simple tasks, with no successful application to large-scale natural language generation to date. In this work, we present the first hybrid quantum-classical large language model (LLM) for natural language generation, HyQuT, capable of performing coherent and context-aware dialogue. The proposed architecture integrates variational quantum circuits (VQCs) into the Transformer framework at both 8M and 150M parameter scales. Experimental results show that a minimal number of qubits (10 qubits with 80 quantum gates) can replace about 10% of the classical parameters in the 150M-parameter model, while achieving comparable convergence stability and generation quality. This study provides an early demonstration of the feasibility of integrating quantum computing to large-scale generative language models.

</details>


### [4] [Empirical Characterization of Temporal Constraint Processing in LLMs](https://arxiv.org/abs/2511.10654)
*Javier Marín*

Main category: cs.CL

TL;DR: 主流LLM模型无法可靠处理时间约束任务，提示词格式极其影响准确率，且能力与参数数量无关。微调提升有限，只有引入时间状态表示与符号推理模块的混合架构才能适用于对时间敏感的应用。


<details>
  <summary>Details</summary>
Motivation: LLM被广泛用于需要实时决策的智能体系统，许多场景假设模型可可靠判断操作窗口是否有效，但这一基础假设缺乏系统性验证。

Method: 对8个具有2.8-8B参数规模的实际生产模型，通过“截止时间检测”任务进行实验，评估模型对时间约束的处理能力，并观察性能分布与提示格式变化的影响。进一步采用200个合成样本对部分模型进行微调，测试能力提升。

Result: 发现模型性能呈两极分化（95%或50%正确率），对提示格式极度敏感（准确率波动30-60个百分点），在失败样本中有100%错误正判断。模型参数规模与能力无关。有限度微调可提升部分能力但无法根本解决。提出需要新的架构机制才能可靠处理时间约束。

Conclusion: 当前的自回归LLM架构在处理时间约束和实时决策方面存在重大可靠性缺陷，无明确结构机制支持时间状态连续表示和约束检测。单靠自然语言的下一个词预测与微调无法弥补该能力缺失。在未引入符号推理等混合架构前，LLM在时间敏感应用中的部署风险很高。

Abstract: When deploying LLMs in agentic architectures requiring real-time decisions under temporal constraints, we assume they reliably determine whether action windows remain open or have closed. This assumption is untested. We characterize temporal constraint processing across eight production-scale models (2.8-8B parameters) using deadline detection tasks, revealing systematic deployment risks: bimodal performance distribution (models achieve either 95% or 50% accuracy), extreme prompt brittleness (30-60 percentage point swings from formatting changes alone), and systematic action bias (100% false positive rates in failing models). Parameter count shows no correlation with capability in this range-a 3.8B model matches 7B models while other 7B models fail completely. Fine-tuning on 200 synthetic examples improves models with partial capability by 12-37 percentage points. We demonstrate that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language, even with targeted fine-tuning. This capability requires architectural mechanisms for: (1) continuous temporal state representation, (2) explicit constraint checking separate from linguistic pattern matching, (3) systematic compositional reasoning over temporal relations. Current autoregressive architectures lack these mechanisms. Deploying such systems in time-critical applications without hybrid architectures incorporating symbolic reasoning modules represents unacceptable risk.

</details>


### [5] [Spectral Neuro-Symbolic Reasoning II: Semantic Node Merging, Entailment Filtering, and Knowledge Graph Alignment](https://arxiv.org/abs/2511.10655)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.CL

TL;DR: 该论文在谱神经符号推理框架前添加了语义预处理，包括节点合并、边验证和外部知识对齐，强化了推理系统的准确率、泛化和稳定性，提升了实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 原有的Spectral Neuro-Symbolic Reasoning框架在节点冗余、图边质量和上下文缺失方面存在一定局限，因此有必要通过语义和符号预处理加强推理系统的鲁棒性、泛化能力和可解释性。

Method: （1）使用基于transformer的上下文嵌入（如Sentence-BERT，SimCSE）进行节点合并，减少冗余。（2）用预训练NLI分类器（如RoBERTa，DeBERTa）进行句子层面蕴涵验证，提高边的质量。（3）与外部知识图谱（如ConceptNet，Wikidata）对齐，以补充缺失的上下文。这些步骤都在核心谱推理流程前进行，属于模块化语义预处理。

Result: 在ProofWriter、EntailmentBank和CLUTRR等基准上，系统准确率提高（最高+3.8%），在应对对抗样例时表现更佳，推理过程中的噪声也有所减少。

Conclusion: 本工作通过模块化、语义驱动的预处理增强了谱符号推理系统，显著提升了图质量和推理系统的鲁棒性、可解释性和可扩展性，且这些改进不影响推理引擎本身，适合开放域和实际应用场景部署。

Abstract: This report extends the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by introducing three semantically grounded enhancements: (1) transformer-based node merging using contextual embeddings (e.g., Sentence-BERT, SimCSE) to reduce redundancy, (2) sentence-level entailment validation with pretrained NLI classifiers (e.g., RoBERTa, DeBERTa) to improve edge quality, and (3) alignment with external knowledge graphs (e.g., ConceptNet, Wikidata) to augment missing context. These modifications enhance graph fidelity while preserving the core spectral reasoning pipeline. Experimental results on ProofWriter, EntailmentBank, and CLUTRR benchmarks show consistent accuracy gains (up to +3.8\%), improved generalization to adversarial cases, and reduced inference noise. The novelty lies in performing semantic and symbolic refinement entirely upstream of the spectral inference stage, enabling efficient, interpretable, and scalable reasoning without relying on quadratic attention mechanisms. In summary, this work extends the Spectral NSR framework with modular, semantically grounded preprocessing steps that improve graph quality without altering the core spectral reasoning engine. The result is a more robust, interpretable, and scalable reasoning system suitable for deployment in open-domain and real-world settings.

</details>


### [6] [Preference Orchestrator: Prompt-Aware Multi-Objective Alignment for Large Language Models](https://arxiv.org/abs/2511.10656)
*Biao Liu,Ning Xu,Junming Yang,Xin Geng*

Main category: cs.CL

TL;DR: 该文提出PRO框架，通过自动推断每个prompt的偏好权重，免去手动设置权重的繁琐和低效，显著提高了大语言模型在多目标对齐任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多目标对齐需要用户手动指定偏好权重，既增加了用户负担，也导致训练效率低下。缺乏合适的自适应权重分配机制限制了LLMs在实际场景下的灵活性和效果。

Method: 提出了轻量级的preference adapter，利用多重奖励模型对答案的归一化评分进行训练，实现训练和推理阶段的动态权重调整，并进行了理论分析和多任务实验验证。

Result: PRO框架在多项任务实验中，优于现有的多目标对齐方法。理论分析印证了其权重动态推断机制的有效性。

Conclusion: PRO框架通过自动推断和适配提示特定的偏好权重，有效提升LLM在多目标对齐任务中的表现。

Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, aligning these models with varying human preferences across multiple objectives remains a significant challenge in practical deployments. Existing multi-objective alignment methods rely on manually specified preference weights, which not only burden users with difficult preference specification tasks but also lead to suboptimal training efficiency due to exploration of irrelevant preference combinations. To alleviate these issues, we propose a novel framework named PRO, i.e., PReference Orchestrator, which features a lightweight preference adapter that automatically infers prompt-specific preference weights during both training and deployment phases. Specifically, the adapter automatically learns appropriate preference weights for each prompt by training on normalized reward scores from multiple reward models for preferred responses, which inherently reflect effective preference balances across objectives. Additionally, We provide theoretical analysis proving that our prompt-aware preference mechanism achieves superior performance compared to fixed preference weights in multi-objective alignment scenarios. Extensive experiments across multiple tasks demonstrate the effectiveness of our method over existing multi-objective alignment approaches.

</details>


### [7] [Patent Representation Learning via Self-supervision](https://arxiv.org/abs/2511.10657)
*You Zuo,Kim Gerdes,Eric Villemonte de La Clergerie,Benoît Sagot*

Main category: cs.CL

TL;DR: 本文提出基于专利多文档区块的自监督对比学习框架，有效提升专利文本表征能力，在无监督条件下超越了多项有监督方法，显示结构性专利信息对表征学习和下游任务的巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 现有SimCSE等自监督文本嵌入方法在专利领域存在表达过度均匀、语义结构丧失的问题。专利文本本身结构丰富（如摘要、权利要求、背景等），但这一特点尚未被现有方法充分利用。

Method: 文章提出section-based augmentation，即利用专利的不同部分（如摘要、权利要求、背景等）作为多视图进行对比学习，有效弥补了SimCSE类dropout增强造成嵌入表达过分均匀的问题。整个学习过程为完全自监督，无需依赖外部注释。

Result: 在大规模基准数据集上，该方法在先前技术检索与分类任务中表现与基于引文或IPC标签的有监督方法持平甚至更优，无需依赖不完备或易碎的外部注释。此外，不同的文档区块在不同任务上具有专长，例如权利要求与摘要更适合检索，背景区块则突出分类能力。

Conclusion: 本文提出了一种利用专利文档内部多视图信息进行对比学习的方法，有效提升了自监督下的专利表征质量。通过实验证明，方法能在专利检索与分类任务中达到或超过依赖监督标签的先进基线，且不同文档区块适用于不同任务，凸显专利文本结构的重要性。

Abstract: This paper presents a simple yet effective contrastive learning framework for learning patent embeddings by leveraging multiple views from within the same document. We first identify a patent-specific failure mode of SimCSE style dropout augmentation: it produces overly uniform embeddings that lose semantic cohesion. To remedy this, we propose section-based augmentation, where different sections of a patent (e.g., abstract, claims, background) serve as complementary views. This design introduces natural semantic and structural diversity, mitigating over-dispersion and yielding embeddings that better preserve both global structure and local continuity. On large-scale benchmarks, our fully self-supervised method matches or surpasses citation-and IPC-supervised baselines in prior-art retrieval and classification, while avoiding reliance on brittle or incomplete annotations. Our analysis further shows that different sections specialize for different tasks-claims and summaries benefit retrieval, while background sections aid classification-highlighting the value of patents' inherent discourse structure for representation learning. These results highlight the value of exploiting intra-document views for scalable and generalizable patent understanding.

</details>


### [8] [Evaluating Open-Weight Large Language Models for Structured Data Extraction from Narrative Medical Reports Across Multiple Use Cases and Languages](https://arxiv.org/abs/2511.10658)
*Douwe J. Spaanderman,Karthik Prathaban,Petr Zelina,Kaouther Mouheb,Lukáš Hejtmánek,Matthew Marzetti,Antonius W. Schurink,Damian Chan,Ruben Niemantsverdriet,Frederik Hartmann,Zhen Qian,Maarten G. J. Thomeer,Petr Holub,Farhan Akram,Frank J. Wolters,Meike W. Vernooij,Cornelis Verhoef,Esther E. Bron,Vít Nováček,Dirk J. Grünhagen,Wiro J. Niessen,Martijn P. A. Starmans,Stefan Klein*

Main category: cs.CL

TL;DR: 本文系统评估了15款开放权重LLM在多任务、多机构、多语言的临床报告结构化提取任务中的表现，发现小型通用模型即可达到与人工一致性接近的水平，提示策略与任务特征对性能影响显著。


<details>
  <summary>Details</summary>
Motivation: 以往相关研究多聚焦于单一任务、有限模型或仅限英语报告，缺乏对跨多任务、多语言、多机构环境下LLMs结构化信息提取能力的系统评估。

Method: 对15个开放权重的LLM（包含通用及医学专用，大小不一）在6种用例和3家机构的病理和放射科报告上进行评估，并比较了6种提示策略，采用多种任务相关指标进行性能评估，并通过一致性排序聚合和线性混合效应模型分析方差。

Result: 排名前列的模型在各任务中宏观平均分接近人工标注者一致性，小型和中型通用模型效果可媲美大型模型，微型和专用医学模型表现较差，prompt graph及few-shot提示策略可提升约13%性能，任务复杂性及标注差异对结果影响更大。

Conclusion: 开放权重的大语言模型（LLMs）能够跨疾病、语言和机构，从临床报告中提取结构化数据，为临床数据整理提供可扩展的解决方案。任务特异性因素对表现的影响大于模型规模或提示策略。

Abstract: Large language models (LLMs) are increasingly used to extract structured information from free-text clinical records, but prior work often focuses on single tasks, limited models, and English-language reports. We evaluated 15 open-weight LLMs on pathology and radiology reports across six use cases, colorectal liver metastases, liver tumours, neurodegenerative diseases, soft-tissue tumours, melanomas, and sarcomas, at three institutes in the Netherlands, UK, and Czech Republic. Models included general-purpose and medical-specialised LLMs of various sizes, and six prompting strategies were compared: zero-shot, one-shot, few-shot, chain-of-thought, self-consistency, and prompt graph. Performance was assessed using task-appropriate metrics, with consensus rank aggregation and linear mixed-effects models quantifying variance. Top-ranked models achieved macro-average scores close to inter-rater agreement across tasks. Small-to-medium general-purpose models performed comparably to large models, while tiny and specialised models performed worse. Prompt graph and few-shot prompting improved performance by ~13%. Task-specific factors, including variable complexity and annotation variability, influenced results more than model size or prompting strategy. These findings show that open-weight LLMs can extract structured data from clinical reports across diseases, languages, and institutions, offering a scalable approach for clinical data curation.

</details>


### [9] [Test-Time Steering for Lossless Text Compression via Weighted Product of Experts](https://arxiv.org/abs/2511.10660)
*Qihang Zhang,Muchen Li,Ziao Wang,Renjie Liao,Lele Wang*

Main category: cs.CL

TL;DR: 提出了一种在推理时自适应结合通用压缩器与神经语言模型的框架（wPoE），无需微调即可提升文本压缩效果，并具备强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统压缩器通用性强但压缩率较低，神经压缩器压缩率更好但泛化能力不足，亟需能兼顾两者优点的方法。

Method: 采用加权专家乘积（wPoE）框架，将传统通用压缩模型与预训练神经语言模型在推理时自适应组合，实现压缩率至少不低于最佳单一模型。

Result: 实验表明，该方法在无需微调的情况下优于单一模型，提高了文本压缩性能，具备跨数据分布的实用性。

Conclusion: 提出的方法能够在无需微调的情况下提高文本压缩性能，并且能够与任意自回归语言模型无缝集成。

Abstract: Lossless compression techniques are crucial in an era of rapidly growing data. Traditional universal compressors like gzip offer low computational overhead, high speed, and broad applicability across data distributions. However, they often lead to worse compression rates than modern neural compressors, which leverage large-scale training data to model data distributions more effectively. Despite their advantages, neural compressors struggle to generalize to unseen data. To address this limitation, we propose a novel framework that performs Test-Time Steering via a Weighted Product of Experts (wPoE). At inference, our method adaptively combines a universal compression model with a pretrained neural language model, ensuring the compression rate is at least as good as that of the best individual model. Extensive experiments demonstrate that our approach improves the performance of text compression without requiring fine-tuning. Furthermore, it seamlessly integrates with any autoregressive language model, providing a practical solution for enhancing text compression across diverse data distributions.

</details>


### [10] [Bayesian Evaluation of Large Language Model Behavior](https://arxiv.org/abs/2511.10661)
*Rachel Longjohn,Shang Wu,Saatvik Kher,Catarina Belém,Padhraic Smyth*

Main category: cs.CL

TL;DR: 本文提出用贝叶斯方法量化大语言模型二元评测的统计不确定性，在有害性和偏好两个评估案例中展现了该方法的有效性，提高了评测的科学性和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前评测LLM输出的标准通常缺乏对统计不确定性的量化，这会影响对模型真实表现的判读，因此提出针对二元评测指标的不确定性量化新方法。

Method: 采用贝叶斯统计方法对基于LLM生成文本的二元评测指标（如有害/无害或泄露/不泄露）进行不确定性量化分析，并通过两个案例（有害性输入的拒绝率、模型间偏好对比）进行实证分析。

Result: 贝叶斯方法可针对LLM生成的不确定性提供明确的统计置信区间，使评估结果更具参考价值，并应用在有害性检测与模型偏好评估中均表现良好。

Conclusion: 研究表明贝叶斯方法能够有效量化LLM系统评测结果中的不确定性，提升评测的解释力和鲁棒性。

Abstract: It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.

</details>


### [11] [Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages:A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish](https://arxiv.org/abs/2511.10664)
*Chengxuan Xia,Qianye Wu,Hongbin Guan,Sixuan Tian,Yilun Hao,Xiaoyu Wu*

Main category: cs.CL

TL;DR: 本文首次系统性评估主流大语言模型在粤语、日语和土耳其语上的多任务表现，发现顶级专有模型虽领先，但均面对文化和形态挑战，开源模型则明显落后。作者发布了标准数据集促进后续研究，呼吁开发更具文化和语言普适性的LLM。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高资源语言（如英语）表现优异，但对低资源和形态复杂语言的有效性尚缺系统研究。作者希望通过建立多语言、多任务基准，揭示现有LLM在多样语言环境下的能力缺陷，推动更公平且普适的AI发展。

Method: 作者对七个主流大语言模型进行系统评测，涵盖粤语、日语、土耳其语三种语言，在开放域问答、文档摘要、英译X、文化对话四项任务上，采用人工评分（流畅度、事实准确性、文化适切性）与自动指标（BLEU、ROUGE）结合，进行性能对比，并提供详细的定量结果和错误分析。

Result: 大的专有模型总体上表现突出，其中GPT-4o在多语任务中尤其表现强劲，Claude 3.5在知识和推理上竞争力强。然而，所有模型均难以解决各语言中的形态和文化难题，资源较少的开源模型在准确性和流畅度方面差距明显。

Conclusion: 最大的专有模型（GPT-4o、GPT-4、Claude 3.5）在各语言和任务中表现领先，但在文化细致理解和形态学泛化上仍存在显著差距。所有模型在土耳其语聚合形态、粤语俚语等独特语言挑战面前都有不同程度的困难。开源小模型在流畅性和准确性方面落后，凸显资源差异。作者呼吁开发更具文化意识和广泛语言泛化能力的大模型，并公布基准与评测数据以促进复现和后续研究。

Abstract: Large language models (LLMs) have achieved impressive results in high-resource languages like English, yet their effectiveness in low-resource and morphologically rich languages remains underexplored. In this paper, we present a comprehensive evaluation of seven cutting-edge LLMs -- including GPT-4o, GPT-4, Claude~3.5~Sonnet, LLaMA~3.1, Mistral~Large~2, LLaMA-2~Chat~13B, and Mistral~7B~Instruct -- on a new cross-lingual benchmark covering \textbf{Cantonese, Japanese, and Turkish}. Our benchmark spans four diverse tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. We combine \textbf{human evaluations} (rating fluency, factual accuracy, and cultural appropriateness) with automated metrics (e.g., BLEU, ROUGE) to assess model performance.
  Our results reveal that while the largest proprietary models (GPT-4o, GPT-4, Claude~3.5) generally lead across languages and tasks, significant gaps persist in culturally nuanced understanding and morphological generalization. Notably, GPT-4o demonstrates robust multilingual performance even on cross-lingual tasks, and Claude~3.5~Sonnet achieves competitive accuracy on knowledge and reasoning benchmarks. However, all models struggle to some extent with the unique linguistic challenges of each language, such as Turkish agglutinative morphology and Cantonese colloquialisms. Smaller open-source models (LLaMA-2~13B, Mistral~7B) lag substantially in fluency and accuracy, highlighting the resource disparity. We provide detailed quantitative results, qualitative error analysis, and discuss implications for developing more culturally aware and linguistically generalizable LLMs. Our benchmark and evaluation data are released to foster reproducibility and further research.

</details>


### [12] [Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models](https://arxiv.org/abs/2511.10665)
*Cristina Pinneri,Christos Louizos*

Main category: cs.CL

TL;DR: 论文发现LLM守卫模型对同义改写表现出评分不一致，提出新的自监督鲁棒性训练方法，通过偏态感知集成优化，显著提升模型一致性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 守卫模型对意义不变但表达不同的句子表现出明显评分波动，说明缺少语义基础，导致安全评估易受表层修辞干扰，需要提升模型对语义保持一致表达的鲁棒性。

Method: 采用自监督方法，利用意译（paraphrase）集，通过一种新颖的偏态感知聚合策略，强制模型在不同表达下输出一致预测。实验对比常用均值/中位数聚合方法，分析六种开源守卫模型，并通过鲁棒性训练提升语义一致性与校准性。

Result: 方法使守卫模型对paraphrase评分变异性降低约58%，基准准确率提升平均2.5%，对新型语言风格有良好泛化能力，模型校准提升最高达40%。说明语义一致性训练可显著改善安全守卫模型。

Conclusion: 提出的自监督训练框架能显著提升守卫模型对语义保持一致的表达的鲁棒性，降低了因表面语言变化导致的安全分数波动，提高了模型的可靠性和泛化能力。

Abstract: Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.

</details>


### [13] [Evaluating LLM Understanding via Structured Tabular Decision Simulations](https://arxiv.org/abs/2511.10667)
*Sichao Li,Xinyue Xu,Xiaomeng Li*

Main category: cs.CL

TL;DR: 论文提出了一种新方法系统评估LLM在专业决策中的理解能力，发现模型准确性与真实理解不一致，呼吁建立更严谨的评估标准以推动模型理解能力的发展。


<details>
  <summary>Details</summary>
Motivation: 准确率高的LLM并不等同于真正理解问题，现有评估方式不足以刻画模型的深层理解能力，需建立更严格的理解评估框架。

Method: 提出了“结构化表格决策模拟”(STaDS)，设计多领域专业决策场景，对模型的题目理解、知识预测及决策要素依赖进行联合评估。

Result: 实验显示，多数主流LLM在不同领域无法持续高水平决策，准确性与决策依据不一致，常出现理由和决定因素错配。强调需要更全面的理解评估和模型改进方案。

Conclusion: 当前主流的大型语言模型在多领域结构化决策任务中还不能展现出类专业者的广泛、稳健理解能力。仅凭准确性无法证明模型真正理解问题，需要新的评估方式。

Abstract: Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.

</details>


### [14] [Forecasting Spoken Language Development in Children with Cochlear Implants Using Preimplantation MRI](https://arxiv.org/abs/2511.10669)
*Yanlin Wang,Di Yuan,Shani Dettman,Dawn Choo,Emily Shimeng Xu,Denise Thomas,Maura E Ryan,Patrick C M Wong,Nancy M Young*

Main category: cs.CL

TL;DR: 深度迁移学习模型结合脑部影像特征，在预测人工耳蜗患儿术后语言发展表现出卓越性能，准确率、灵敏度与特异性均高于传统机器学习方法，显示其广泛应用潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管人工耳蜗可以显著改善患儿的语言能力，但效果个体差异较大，且仅通过植入年龄或残余听力难以准确预测。因此亟需更优的预测方法来帮助制定个体化干预方案。

Method: 研究比较了传统机器学习与深度迁移学习算法，利用二分类模型（高语言提升与低语言提升）预测双侧SNHL儿童植入人工耳蜗后的语言发展，基于脑神经解剖特征构建预测模型。

Result: DTL模型采用双线性注意力融合策略，准确率92.39%、灵敏度91.22%、特异性93.56%、AUC为0.977，所有评估指标均优于传统ML模型；表明DTL在直接提取判别性与任务相关信息方面具备显著优势。

Conclusion: 深度迁移学习（DTL）模型能够显著优于传统机器学习（ML）方法，准确预测重度至极重度感音神经性听力损失（SNHL）患儿植入人工耳蜗后的语言发展结果。

Abstract: Cochlear implants (CI) significantly improve spoken language in children with severe-to-profound sensorineural hearing loss (SNHL), yet outcomes remain more variable than in children with normal hearing. This variability cannot be reliably predicted for individual children using age at implantation or residual hearing. This study aims to compare the accuracy of traditional machine learning (ML) to deep transfer learning (DTL) algorithms to predict post-CI spoken language development of children with bilateral SNHL using a binary classification model of high versus low language improvers. A total of 278 implanted children enrolled from three centers. The accuracy, sensitivity and specificity of prediction models based upon brain neuroanatomic features using traditional ML and DTL learning. DTL prediction models using bilinear attention-based fusion strategy achieved: accuracy of 92.39% (95% CI, 90.70%-94.07%), sensitivity of 91.22% (95% CI, 89.98%-92.47%), specificity of 93.56% (95% CI, 90.91%-96.21%), and area under the curve (AUC) of 0.977 (95% CI, 0.969-0.986). DTL outperformed traditional ML models in all outcome measures. DTL was significantly improved by direct capture of discriminative and task-specific information that are advantages of representation learning enabled by this approach over ML. The results support the feasibility of a single DTL prediction model for language prediction of children served by CI programs worldwide.

</details>


### [15] [Towards Fine-Grained Code-Switch Speech Translation with Semantic Space Alignment](https://arxiv.org/abs/2511.10670)
*Yan Gao,Yazheng Yang,Zhibin Lan,Yidong Chen,Min Zhang,Daimeng Wei,Hui Huang,Jinsong Su*

Main category: cs.CL

TL;DR: 针对混合语码语音翻译的挑战，提出了集成MoE专家和多阶段训练框架，提升了模型语义建模能力和数据适应能力，并在多数据集上效果显著。


<details>
  <summary>Details</summary>
Motivation: 混合语码语音翻译面临语义建模复杂和高质量数据稀缺问题，现有方法依靠模型隐性学习或人工标注，效率低且成本高，亟需新的方法提升建模和数据利用能力。

Method: 提出Mixture of Experts（MoE）语音投影器，使每个专家专注于某一语言的语义子空间，并引入多阶段训练范式，结合单语ASR和ST数据进行对齐及训练。在训练中应用语言特定损失和组内负载均衡损失引导专家分配，同时通过过渡损失实现各阶段数据的平滑适配。

Result: 实验表明该方法在主流数据集上有效提升了语码混合语音翻译的性能，具备良好的通用性和适应性。

Conclusion: 通过在大规模语言模型中集成MoE语音投影器及多阶段训练方法，有效提升了混合语码语音翻译的性能，对复杂语义建模和数据稀缺问题有显著改善。

Abstract: Code-switching (CS) speech translation (ST) refers to translating speech that alternates between two or more languages into a target language text, which poses significant challenges due to the complexity of semantic modeling and the scarcity of CS data. Previous studies tend to rely on the model itself to implicitly learn semantic modeling during training, and resort to inefficient and costly manual annotations for these two challenges. To mitigate these limitations, we propose enhancing Large Language Models (LLMs) with a Mixture of Experts (MoE) speech projector, where each expert specializes in the semantic subspace of a specific language, enabling fine-grained modeling of speech features. Additionally, we introduce a multi-stage training paradigm that utilizes readily available monolingual automatic speech recognition (ASR) and monolingual ST data, facilitating speech-text alignment and improving translation capabilities. During training, we leverage a combination of language-specific loss and intra-group load balancing loss to guide the MoE speech projector in efficiently allocating tokens to the appropriate experts, across expert groups and within each group, respectively. To bridge the data gap across different training stages and improve adaptation to the CS scenario, we further employ a transition loss, enabling smooth transitions of data between stages, to effectively address the scarcity of high-quality CS speech translation data. Extensive experiments on widely used datasets demonstrate the effectiveness and generality of our approach.

</details>


### [16] [Grounded Visual Factualization: Factual Anchor-Based Finetuning for Enhancing MLLM Factual Consistency](https://arxiv.org/abs/2511.10671)
*Filippo Morbiato,Luca Romano,Alessandro Persona*

Main category: cs.CL

TL;DR: GVF微调通过创新数据增强、指令设计和损失函数，有效提升了多模态大模型视觉事实一致性，减少幻觉且不牺牲泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型普遍存在视觉幻觉，即生成与图片内容不符的细节，严重影响模型可靠性，而传统微调方法在事实推理介入层面有限，亟需更为系统和深入的方案以增强模型事实一致性。

Method: GVF方法包含三大核心机制：1）事实锚点数据增强，扩充带有结构化事实锚点与反事实提示的训练集；2）事实感知指令微调，将事实提示嵌入显式指令中；3）事实一致性损失函数，针对事实性错误进行专门惩罚。实验在LLaVA-1.5-13B模型上评估，采用VHTest、MME与POPE等多模态基准测试集。

Result: 在VHTest开放式问题（OEQ）和是/否问题（YNQ）两大评测中，GVF方法显著优于标准微调方法。此外，在MME与POPE等多模态基准中，GVF的整体表现保持甚至小幅提升，显示出了减少视觉幻觉的同时不损失通用理解与推理能力的优势。

Conclusion: 本文提出的Grounded Visual Factualization (GVF)微调方法，能大幅提升多模态大模型（MLLM）在视觉事实一致性方面的表现，并有效减少视觉幻觉现象，同时不损害模型的整体理解与推理能力。

Abstract: Visual hallucination, where Multimodal Large Language Models fabricate details inconsistent with image content, critically undermines their reliability. Existing fine-tuning methods offer limited improvement, failing to deeply intervene in factual reasoning. This paper introduces Grounded Visual Factualization (GVF) Finetuning, a novel approach to systematically enhance MLLM visual factual consistency. GVF integrates explicit factual signals via three core mechanisms: Factual Anchor Data Augmentation, enriching training data with structured factual anchors and counter-factual prompts; Fact-Aware Instruction Tuning, embedding these cues into explicit instructions; and a Factual Consistency Loss function, specifically penalizing factual inaccuracies. Evaluated on LLaVA-1.5-13B, GVF Finetuning significantly outperforms standard fine-tuning on the VHTest benchmark for both Open-Ended Question (OEQ) and Yes/No Question (YNQ) formats. Crucially, GVF maintains or even slightly improves performance on general multimodal benchmarks like MME and POPE, demonstrating effective mitigation of visual hallucinations without compromising general understanding and reasoning abilities.

</details>


### [17] [Large language models in materials science and the need for open-source approaches](https://arxiv.org/abs/2511.10673)
*Fengxu Yang,Weitong Chen,Jack D. Evans*

Main category: cs.CL

TL;DR: 本综述回顾了大语言模型在材料科学中的三大应用，发现开源模型表现优异并有助于构建更开放和灵活的科学AI平台，建议社区更多采用开源模式。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在材料科学中的快速应用，作者希望系统梳理其在不同材料研发环节中的角色，并评估开源模型的潜力与优势。

Method: 回顾性综述，梳理了大语言模型在材料科学中的三大应用方向，并进行了开源与闭源模型表现的基准对比。

Result: LLMs已在文献挖掘、预测建模和多智能体实验系统中取得显著成效。开源模型在性能上已能媲美商业产品，且在透明度、可复现性、成本和数据隐私方面具有明显优势。

Conclusion: 开源大语言模型能够媲美商业模型，并为科学发现提供透明、可复现、低成本和隐私性强的AI平台，建议社区广泛采用开源模型。

Abstract: Large language models (LLMs) are rapidly transforming materials science. This review examines recent LLM applications across the materials discovery pipeline, focusing on three key areas: mining scientific literature , predictive modelling, and multi-agent experimental systems. We highlight how LLMs extract valuable information such as synthesis conditions from text, learn structure-property relationships, and can coordinate agentic systems integrating computational tools and laboratory automation. While progress has been largely dependent on closed-source commercial models, our benchmark results demonstrate that open-source alternatives can match performance while offering greater transparency, reproducibility, cost-effectiveness, and data privacy. As open-source models continue to improve, we advocate their broader adoption to build accessible, flexible, and community-driven AI platforms for scientific discovery.

</details>


### [18] [Learn to Select: Exploring Label Distribution Divergence for In-Context Demonstration Selection in Text Classification](https://arxiv.org/abs/2511.10675)
*Ye Jiang,Taihang Wang,Youzheng Liu,Yimin Wang,Yuhan Xia,Yunfei Long*

Main category: cs.CL

TL;DR: 提出结合语义相似性与标签分布对齐的演示选择方法L2D，有效提升文本分类ICL效果，在多个基准上表现优越，并验证SLM好坏对最终结果影响明显。


<details>
  <summary>Details</summary>
Motivation: 现有的演示样本选择方法过于关注语义相似性，忽略了标签分布对齐对大模型表现的重要影响，因此需设计兼顾二者的方法以提升ICL效果。

Method: 使用两阶段的演示样本选择方法：首先通过语义相似性筛选候选样本（TopK），再利用微调后的BERT小型语言模型生成测试输入与样本的标签分布，计算分布散度，最终选取标签分布对齐好的示例。

Result: 在七个文本分类基准测试上，该方法均优于其他选择策略，且分析显示SLM标签分布估算准确性越高，对选择演示样本的帮助和最终分类效果越好。

Conclusion: 提出的方法不仅考虑语义相似性，还关注标签分布的对齐，能有效提升大语言模型在文本分类中的表现，且SLM估算标签分布的准确率与LLM效果正相关。

Abstract: In-context learning (ICL) for text classification, which uses a few input-label demonstrations to describe a task, has demonstrated impressive performance on large language models (LLMs). However, the selection of in-context demonstrations plays a crucial role and can significantly affect LLMs' performance. Most existing demonstration selection methods primarily focus on semantic similarity between test inputs and demonstrations, often overlooking the importance of label distribution alignment. To address this limitation, we propose a two-stage demonstration selection method, TopK + Label Distribution Divergence (L2D), which leverages a fine-tuned BERT-like small language model (SLM) to generate label distributions and calculate their divergence for both test inputs and candidate demonstrations. This enables the selection of demonstrations that are not only semantically similar but also aligned in label distribution with the test input. Extensive experiments across seven text classification benchmarks show that our method consistently outperforms previous demonstration selection strategies. Further analysis reveals a positive correlation between the performance of LLMs and the accuracy of the underlying SLMs used for label distribution estimation.

</details>


### [19] [Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2511.10676)
*Shien Zhu,Samuel Bohl,Robin Oester,Gustavo Alonso*

Main category: cs.CL

TL;DR: 本文提出了一种轻量高效的预注意力专家预测方法，提升了专家路由准确率，降低了MoE模型推理延迟，在多种主流模型上获得了大幅领先的实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型在专家选择预测方面准确率不高，尤其第一层受限，且复杂预测模型计算代价大。因此作者欲寻求一种轻量且高效的专家预测机制。

Method: 提出了一种基于同层注意力前线性变换和排序感知损失的专家预测方法，使专家路由可在第一层及后续层实现更高准确的预取决策。

Result: 所提出的预注意力专家路由器在DeepSeek V2 Lite、Qwen3-30B和Phi-mini-MoE等模型上的专家预测准确率分别达到了93.03%、94.69%和97.62%，较现有最佳方法提升约15个百分点。

Conclusion: 通过采用预注意力专家预测方法，该方法显著提升了专家预获取的准确率，并且避免引入高复杂度计算，取得比此前方法更有效的效果。

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, existing approaches utilize the activations from the previous layer for prediction, incurring low accuracy and leave the first layer unoptimized. Applying complex layers or even training standalone networks for better prediction introduces high computation overhead. In this paper, we propose pre-attention expert prediction to achieve accurate and lightweight expert prefetching. The key insight is that some functions in LLMs are ranking-preserving, indicating that matching the ranking of selected experts using simple linear functions is possible. Therefore, we utilize the activations before the attention block in the same layer with 2 linear functions and ranking-aware loss to achieve accurate prediction, which also supports prefetching in the first layer. Our lightweight, pre-attention expert routers achieve 93.03% accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing about 15% improvement on absolute accuracy over the state-of-the-art methods.

</details>


### [20] [SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments with Generative AI](https://arxiv.org/abs/2511.10684)
*Anupama Sitaraman,Bharathan Balaji,Yuvraj Agarwal*

Main category: cs.CL

TL;DR: 本文提出的SpiderGen系统利用大语言模型高效生成LCA流程信息，显著降低了耗时和成本，在准确性上也优于现有方法，是LCA自动化和环境影响估算的重要进展。


<details>
  <summary>Details</summary>
Motivation: 鉴于温室气体排放严重影响全球气候变化，而消费品生产、使用及废弃是主要源头，准确、经济估算消费品环境影响的方法极为重要。传统LCA耗费巨大，因此需开发新的自动化工具。

Method: SpiderGen结合了传统生命周期评估（LCA）的分类法和方法论，以及大语言模型的推理能力和世界知识，生成LCA流程所需的详细信息。其输出结果通过真实LCA文档对照评测，并与其他基线方法进行对比。

Result: SpiderGen生成的LCA过程信息准确率高（F1分数为62%），主要误差源于LCA文档细节和辅助流程界定范围的差异，且相比于链式思维和一次性提示等基线方法表现更优，能将成本降至1美元以下并将所需时间缩短至10分钟内。

Conclusion: 本文提出了一种基于大语言模型（LLM）的工作流SpiderGen，可用于快速、成本低地生成生命周期评估（LCA）所需的过程信息，并在准确性上优于传统提示方法，极大降低了LCA的人力和经济成本。

Abstract: Investigating the effects of climate change and global warming caused by GHG emissions have been a primary concern worldwide. These emissions are largely contributed to by the production, use and disposal of consumer products. Thus, it is important to build tools to estimate the environmental impact of consumer goods, an essential part of which is conducting Life Cycle Assessments (LCAs). LCAs specify and account for the appropriate processes involved with the production, use, and disposal of the products. We present SpiderGen, an LLM-based workflow which integrates the taxonomy and methodology of traditional LCA with the reasoning capabilities and world knowledge of LLMs to generate the procedural information used for LCA. We additionally evaluate the output of SpiderGen using real-world LCA documents as ground-truth. We find that SpiderGen provides accurate LCA process information that is either fully correct or has minor errors, achieving an F1-Score of 62% across 10 sample data points. We observe that the remaining missed processes and hallucinated errors occur primarily due to differences in detail between LCA documents, as well as differences in the "scope" of which auxiliary processes must also be included. We also demonstrate that SpiderGen performs better than several baselines techniques, such as chain-of-thought prompting and one-shot prompting. Finally, we highlight SpiderGen's potential to reduce the human effort and costs for estimating carbon impact, as it is able to produce LCA process information for less than \$1 USD in under 10 minutes as compared to the status quo LCA, which can cost over \$25000 USD and take up to 21-person days.

</details>


### [21] [A methodological analysis of prompt perturbations and their effect on attack success rates](https://arxiv.org/abs/2511.10686)
*Tiago Machado,Maysa Malfiza Garcia de Macedo,Rogerio Abreu de Paula,Marcelo Carpinette Grave,Aminat Adebiyi,Luan Soares de Souza,Enrico Santarelli,Claudio Pinhanez*

Main category: cs.CL

TL;DR: 本研究比较了三种主流对齐方法下开源LLM对提示攻击的敏感性，发现小范围的提示变动会显著影响模型的攻击成功率，且现有攻击基准无法覆盖所有潜在风险，建议未来基于更系统、统计的方法持续完善模型攻击评估。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs面临提示攻击风险，不同对齐方式对安全性的影响尚不明确，需评估各种对齐方法下模型对攻击的脆弱性、并改进现有的攻击评估方法。

Method: 选取多种主流开源大模型，分别采用SFT、DPO、RLHF三种对齐手段，通过统计分析方法系统研究模型应对提示攻击时ASR的敏感性。

Result: 实验显示，微小的提示变体会显著影响各对齐模型的ASR；模型对特定攻击类型的敏感度随对齐方式和提示变动而变化，提示现有评估手段不够全面。

Conclusion: 现有的攻击基准未必能充分揭示所有模型及对齐方法的潜在漏洞；不同对齐方式的ASR对提示变体敏感性不同，需要更系统与统计的方法进行评价。

Abstract: This work aims to investigate how different Large Language Models (LLMs) alignment methods affect the models' responses to prompt attacks. We selected open source models based on the most common alignment methods, namely, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Human Feedback (RLHF). We conducted a systematic analysis using statistical methods to verify how sensitive the Attack Success Rate (ASR) is when we apply variations to prompts designed to elicit inappropriate content from LLMs. Our results show that even small prompt modifications can significantly change the Attack Success Rate (ASR) according to the statistical tests we run, making the models more or less susceptible to types of attack. Critically, our results demonstrate that running existing 'attack benchmarks' alone may not be sufficient to elicit all possible vulnerabilities of both models and alignment methods. This paper thus contributes to ongoing efforts on model attack evaluation by means of systematic and statistically-based analyses of the different alignment methods and how sensitive their ASR is to prompt variation.

</details>


### [22] [Modeling and Predicting Multi-Turn Answer Instability in Large Language Models](https://arxiv.org/abs/2511.10688)
*Jiahang He,Rishi Ramachandran,Neel Ramachandran,Aryan Katakam,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Aryan Shrivastava*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As large language models (LLMs) are adopted in an increasingly wide range of applications, user-model interactions have grown in both frequency and scale. Consequently, research has focused on evaluating the robustness of LLMs, an essential quality for real-world tasks. In this paper, we employ simple multi-turn follow-up prompts to evaluate models' answer changes, model accuracy dynamics across turns with Markov chains, and examine whether linear probes can predict these changes. Our results show significant vulnerabilities in LLM robustness: a simple "Think again" prompt led to an approximate 10% accuracy drop for Gemini 1.5 Flash over nine turns, while combining this prompt with a semantically equivalent reworded question caused a 7.5% drop for Claude 3.5 Haiku. Additionally, we find that model accuracy across turns can be effectively modeled using Markov chains, enabling the prediction of accuracy probabilities over time. This allows for estimation of the model's stationary (long-run) accuracy, which we find to be on average approximately 8% lower than its first-turn accuracy for Gemini 1.5 Flash. Our results from a model's hidden states also reveal evidence that linear probes can help predict future answer changes. Together, these results establish stationary accuracy as a principled robustness metric for interactive settings and expose the fragility of models under repeated questioning. Addressing this instability will be essential for deploying LLMs in high-stakes and interactive settings where consistent reasoning is as important as initial accuracy.

</details>


### [23] [Equilibrium Dynamics and Mitigation of Gender Bias in Synthetically Generated Data](https://arxiv.org/abs/2511.10689)
*Ashish Kattamuri,Arpita Vats,Harshwardhan Fartale,Rahul Raja,Akshata Kishore Moharir,Ishita Prasad*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recursive prompting with large language models enables scalable synthetic dataset generation but introduces the risk of bias amplification. We investigate gender bias dynamics across three generations of recursive text generation using three complementary evaluation frameworks: rule-based pattern matching, embedding-based semantic similarity, and downstream task performance. Experiments with three initial bias levels (0.1, 0.3, 0.6) and four mitigation strategies reveal equilibrium dynamics rather than monotonic amplification. The low initial bias amplifies toward the model's inherent bias level (+36%), whereas the high initial bias decays toward it (-26%). Among mitigation methods, contrastive augmentation, which introduces gender-swapped variants, achieves significant downstream bias reduction (98.8% for low initial bias and 91% on average) despite producing higher embedding-based bias scores. This paradox demonstrates that semantic similarity metrics may diverge from behavioral fairness outcomes, highlighting the need for multidimensional evaluation in responsible synthetic data generation.

</details>


### [24] [Saying the Unsaid: Revealing the Hidden Language of Multimodal Systems Through Telephone Games](https://arxiv.org/abs/2511.10690)
*Juntu Zhao,Jialing Zhang,Chongxuan Li,Dequan Wang*

Main category: cs.CL

TL;DR: 本研究通过传话游戏方法，系统性地探索并量化了多模态系统“隐藏语言”中概念连接的偏好与关联，提出了全新数据集和测试框架，为做出系统可解释性与可控性研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态系统取得重大进展，但由于其黑箱结构，其理解世界的“隐藏语言”尚难以解释。研究旨在揭开这些系统黑盒内部偏好如何影响概念理解与连接，并寻找系统泛化与解释的新方法。

Method: 利用多轮“传话游戏”模型，通过观测系统在输入、压缩、重构多概念图像过程中的概念共现频次，量化系统对概念关联的理解强度。提供了包含万组概念对的Telescope数据集，并利用Reasoning-LLMs进一步挖掘超越表层语义的新概念联系。

Result: 提出的框架能够构建多模态系统概念连接的全局地图，发现偏好偏差、评估泛化能力、并挖掘脆弱概念间更稳定的联系路径。此外，利用Reasoning-LLMs揭示了跨越文本与视觉的新颖概念关系。

Conclusion: 本研究揭示了多模态系统在图像到文本再回到图像的转换过程中，系统固有的偏好引入了特定的输出偏移，从而影响原始概念的共现关系，并通过一系列实验量化了其“隐藏语言”。提出了基于“传话游戏”的方法及Telescope数据集，为解释和控制多模态系统提供了新的视角。

Abstract: Recent closed-source multimodal systems have made great advances, but their hidden language for understanding the world remains opaque because of their black-box architectures. In this paper, we use the systems' preference bias to study their hidden language: During the process of compressing the input images (typically containing multiple concepts) into texts and then reconstructing them into images, the systems' inherent preference bias introduces specific shifts in the outputs, disrupting the original input concept co-occurrence. We employ the multi-round "telephone game" to strategically leverage this bias. By observing the co-occurrence frequencies of concepts in telephone games, we quantitatively investigate the concept connection strength in the understanding of multimodal systems, i.e., "hidden language." We also contribute Telescope, a dataset of 10,000+ concept pairs, as the database of our telephone game framework. Our telephone game is test-time scalable: By iteratively running telephone games, we can construct a global map of concept connections in multimodal systems' understanding. Here we can identify preference bias inherited from training, assess generalization capability advancement, and discover more stable pathways for fragile concept connections. Furthermore, we use Reasoning-LLMs to uncover unexpected concept relationships that transcend textual and visual similarities, inferring how multimodal systems understand and simulate the world. This study offers a new perspective on the hidden language of multimodal systems and lays the foundation for future research on the interpretability and controllability of multimodal systems.

</details>


### [25] [Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models](https://arxiv.org/abs/2511.10691)
*Zijian Chen,Wenjun Zhang,Guangtao Zhai*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Contemporary benchmarks are struggling to keep pace with the development of large language models (LLMs). Although they are indispensable to evaluate model performance on various tasks, it is uncertain whether the models trained on Internet data have genuinely learned how to solve problems or merely seen the questions before. This potential data contamination issue presents a fundamental challenge to establishing trustworthy evaluation frameworks. Meanwhile, existing benchmarks predominantly assume benign, resource-rich settings, leaving the behavior of LLMs under pressure unexplored. In this paper, we introduce Squid Game, a dynamic and adversarial evaluation environment with resource-constrained and asymmetric information settings elaborated to evaluate LLMs through interactive gameplay against other LLM opponents. Notably, Squid Game consists of six elimination-style levels, focusing on multi-faceted abilities, such as instruction-following, code, reasoning, planning, and safety alignment. We evaluate over 50 LLMs on Squid Game, presenting the largest behavioral evaluation study of general LLMs on dynamic adversarial scenarios. We observe a clear generational phase transition on performance in the same model lineage and find evidence that some models resort to speculative shortcuts to win the game, indicating the possibility of higher-level evaluation paradigm contamination in static benchmarks. Furthermore, we compare prominent LLM benchmarks and Squid Game with correlation analyses, highlighting that dynamic evaluation can serve as a complementary part for static evaluations. The code and data will be released in the future.

</details>


### [26] [Do AI Voices Learn Social Nuances? A Case of Politeness and Speech Rate](https://arxiv.org/abs/2511.10693)
*Eyal Rabin,Zohar Elyoseph,Rotem Israel-Fishelson,Adi Dali,Ravit Nussinson*

Main category: cs.CL

TL;DR: 主流文本合成语音系统在未被明确编程下，能通过减慢语速来表达礼貌，显示了AI对人类社会规范的自发学习能力。


<details>
  <summary>Details</summary>
Motivation: 探究当前AI语音系统是否能自发学习、复制人类沟通中隐性、非显性编程的心理及社会线索，尤其是语速减慢表达礼貌这一不明显的韵律标记。

Method: 用22种来自AI Studio和OpenAI的合成语音，分别在“礼貌正式”和“随意非正式”条件下朗读固定脚本，测量语音持续时间并统计分析语速的变化。

Result: 两大平台上的AI语音在‘礼貌’条件下语速显著减慢，所有AI Studio语音和大部分OpenAI语音均有统计学上的显著效果，表明AI已能内化并复现复杂的人类交际细节。

Conclusion: 主流文本合成语音（TTS）系统在不明确编程的情况下，能够内化并表现出人类用降慢语速表达礼貌的倾向。这表明AI可以模仿并强化人类社会规范。

Abstract: Voice-based artificial intelligence is increasingly expected to adhere to human social conventions, but can it learn implicit cues that are not explicitly programmed? This study investigates whether state-of-the-art text-to-speech systems have internalized the human tendency to reduce speech rate to convey politeness - a non-obvious prosodic marker. We prompted 22 synthetic voices from two leading AI platforms (AI Studio and OpenAI) to read a fixed script under both "polite and formal" and "casual and informal" conditions and measured the resulting speech duration. Across both AI platforms, the polite prompt produced slower speech than the casual prompt with very large effect sizes, an effect that was statistically significant for all of AI Studio's voices and for a large majority of OpenAI's voices. These results demonstrate that AI can implicitly learn and replicate psychological nuances of human communication, highlighting its emerging role as a social actor capable of reinforcing human social norms.

</details>


### [27] ["As Eastern Powers, I will veto." : An Investigation of Nation-level Bias of Large Language Models in International Relations](https://arxiv.org/abs/2511.10695)
*Jonghyeon Choi,Yeonjun Choi,Hyun-chul Kim,Beakcheol Jang*

Main category: cs.CL

TL;DR: 系统评估LLM国际关系领域国家级偏见，发现偏见多维化且与模型与任务相关。提出新去偏框架，实验验证有效降低偏见并增强模型表现，尤其适用于强推理能力模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在国际关系领域应用逐渐增多，但其国家级偏见可能影响决策和公平性，因此需系统性评估并提出有效去偏方案。

Method: 利用联合国安理会历史记录，设计了三种测试方法系统评估不同LLM在国家级偏见上的表现，聚焦五个常任理事国，并通过实验比较；采用检索增强生成和自反思去偏技术。

Result: 主流模型普遍对西方国家偏好、对俄罗斯不利，但具体表现根据模型和任务情境变化。强推理模型偏见较弱。引入去偏框架后，偏见显著降低，模型性能提升，尤其在GPT-4o-mini和LLaMA-3.3-70B上效果明显。

Conclusion: 大模型在国际关系领域存在国家级偏见，且偏见表现多维化、因模型和任务不同而变化，强推理能力模型偏见较小。提出的去偏框架可有效降低国家级偏见，提高模型表现。

Abstract: This paper systematically examines nation-level biases exhibited by Large Language Models (LLMs) within the domain of International Relations (IR). Leveraging historical records from the United Nations Security Council (UNSC), we developed a bias evaluation framework comprising three distinct tests to explore nation-level bias in various LLMs, with a particular focus on the five permanent members of the UNSC. Experimental results show that, even with the general bias patterns across models (e.g., favorable biases toward the western nations, and unfavorable biases toward Russia), these still vary based on the LLM. Notably, even within the same LLM, the direction and magnitude of bias for a nation change depending on the evaluation context. This observation suggests that LLM biases are fundamentally multidimensional, varying across models and tasks. We also observe that models with stronger reasoning abilities show reduced bias and better performance. Building on this finding, we introduce a debiasing framework that improves LLMs' factual reasoning combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experiments show it effectively reduces nation-level bias, and improves performance, particularly in GPT-4o-mini and LLama-3.3-70B. Our findings emphasize the need to assess nation-level bias alongside performance when applying LLMs in the IR domain.

</details>


### [28] [$π$-Attention: Periodic Sparse Transformers for Efficient Long-Context Modeling](https://arxiv.org/abs/2511.10696)
*Dong Liu,Yanxuan Yu*

Main category: cs.CL

TL;DR: \PiAttention 通过周期性稀疏注意力机制，实现线性复杂度下的优异长程关系建模能力，性能显著优于现有稀疏注意力方法且资源消耗更低。


<details>
  <summary>Details</summary>
Motivation: Transformer 在处理长序列时由于二次复杂度受限，现有的稀疏注意力如 RingAttention 受感受野和适应性所限，亟需兼顾高效计算与广泛感受野的方法。

Method: 提出周期性稀疏注意力机制（\PiAttention），将注意力分解为环状局部窗口、确定性的 $\pi$ 步长跳跃和自适应融合门，并进行理论复杂度分析和多任务实验验证。

Result: \PiAttention 达到 $\mathcal{O}(kL + \pi\log L)$ 的感受野扩展，显著优于 RingAttention，在多个任务上提升准确率，对比 RingAttention 降低了 8.3% 的 perplexity，GPU 占用减少 50%。

Conclusion: \PiAttention 能以线性复杂度有效扩展注意力的感受野，提升长序列建模效率，同时保持甚至超越稠密注意力的性能。

Abstract: Transformers have revolutionized natural language processing, but their quadratic complexity with respect to sequence length remains a fundamental bottleneck for long-range modeling. While sparse attention mechanisms like RingAttention reduce computational costs by restricting attention to local neighborhoods, they suffer from limited receptive fields and lack of adaptability. We present \PiAttention, a periodic sparse Transformer that factorizes attention into ring-local neighborhoods, deterministic $π$-stride skips, and an adaptive fusion gate. The periodic structure provides predictable coverage of distant tokens, while the sparse footprint keeps the per-layer complexity linear in context length. We prove that \PiAttention achieves $\mathcal{O}(kL + π\log L)$ receptive field growth compared to $\mathcal{O}(kL)$ for RingAttention, where $k$ is the local window size, $π$ is the skip period, and $L$ is the sequence length. Extensive experiments on language modeling, retrieval, and vision-language tasks demonstrate that \PiAttention matches or surpasses dense attention quality with 8.3\% lower perplexity than RingAttention while using 50\% fewer GPUs for the same context length. Our detailed ablations and visualizations reveal the importance of periodic skips, adaptive fusion, and head-level sparsity coordination for efficient long-context modeling.

</details>


### [29] [Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework with LLMs](https://arxiv.org/abs/2511.10768)
*Ajwad Abrar,Nafisa Tabassum Oeshy,Prianka Maheru,Farzana Tabassum,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 提出了结合句子抽取、医学实体识别和大模型的方法，显著提升了医学问答摘要的忠实性和质量，在英孟两种数据集上均表现优异，助力医疗场景安全应用。


<details>
  <summary>Details</summary>
Motivation: 医学摘要的不忠实可能导致信息误解，给医疗场景带来严重风险。目标是提升自动医学文本摘要的忠实性，从而提高医疗沟通的安全性和可靠性。

Method: 结合TextRank句子抽取、医学命名实体识别以及大型语言模型（LLM），并在MeQSum和BanglaCHQ-Summ数据集上微调了LLaMA-2-7B模型。

Result: 在多个自动化指标（ROUGE、BERTScore、SummaC、AlignScore等）上显著优于零样本和先前系统。人工评估显示，80%以上生成摘要能保留关键医学信息。

Conclusion: 提高医学文本摘要的忠实性对于可靠的医疗交流至关重要。所提出的方法在英语和孟加拉语数据集上均取得了优异成绩，且生成的摘要大多数都能保留关键医学信息。

Abstract: Summarizing consumer health questions (CHQs) can ease communication in healthcare, but unfaithful summaries that misrepresent medical details pose serious risks. We propose a framework that combines TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to enhance faithfulness in medical text summarization. In our experiments, we fine-tuned the LLaMA-2-7B model on the MeQSum (English) and BanglaCHQ-Summ (Bangla) datasets, achieving consistent improvements across quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, and outperforming zero-shot baselines and prior systems. Human evaluation further shows that over 80\% of generated summaries preserve critical medical information. These results highlight faithfulness as an essential dimension for reliable medical summarization and demonstrate the potential of our approach for safer deployment of LLMs in healthcare contexts.

</details>


### [30] [TEDxTN: A Three-way Speech Translation Corpus for Code-Switched Tunisian Arabic - English](https://arxiv.org/abs/2511.10780)
*Fethi Bougares,Salima Mdhaffar,Haroun Elleuch,Yannick Estève*

Main category: cs.CL

TL;DR: 本文提出并公开了首个突尼斯阿拉伯语-英语语音翻译大数据集TEDxTN，适合用于代码切换场景。包括注释规范和基准实验，有助于促进突尼斯阿拉伯语相关NLP研究。


<details>
  <summary>Details</summary>
Motivation: 突尼斯方言等阿拉伯语变体的数据稀缺，研究受限。本文旨在通过建立公开数据集来缓解该问题，促进该领域进一步的基础及应用研究。

Method: 作者收集、分段、转录并翻译了108个TEDx演讲，覆盖11个不同地区的突尼斯口音。采用自主开发的注释标准，由多种预训练和微调模型建立语音识别及翻译基线。

Result: 发布了涵盖11个地区口音、包含代码切换特性、时长共25小时的TEDxTN语音翻译语料库。并给出了基准系统的实验结果。数据与注释规范已全部公开，可供未来扩展和研究使用。

Conclusion: 本论文发布了首个突尼斯阿拉伯语到英语公开语音翻译数据集TEDxTN，并提供了数据与注释指南，以促进突尼斯方言及其自然语言处理相关研究。

Abstract: In this paper, we introduce TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset. This work is in line with the ongoing effort to mitigate the data scarcity obstacle for a number of Arabic dialects. We collected, segmented, transcribed and translated 108 TEDx talks following our internally developed annotations guidelines. The collected talks represent 25 hours of speech with code-switching that cover speakers with various accents from over 11 different regions of Tunisia. We make the annotation guidelines and corpus publicly available. This will enable the extension of TEDxTN to new talks as they become available. We also report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models. This corpus is the first open source and publicly available speech translation corpus of Code-Switching Tunisian dialect. We believe that this is a valuable resource that can motivate and facilitate further research on the natural language processing of Tunisian Dialect.

</details>


### [31] [Sabiá: Um Chatbot de Inteligência Artificial Generativa para Suporte no Dia a Dia do Ensino Superior](https://arxiv.org/abs/2511.10787)
*Guilherme Biava Rodrigues,Franciele Beal,Marlon Marcon,Alinne Cristinne Corrêa Souza,André Roberto Ortoncelli,Francisco Carlos Monteiro Souza,Rodolfo Adamshuk Silva*

Main category: cs.CL

TL;DR: 该项目通过开发基于GenAI和RAG的聊天机器人，显著提高了学生获取日常学术信息的便捷性，测试表明Gemini 2.0 Flash和Gemma 3n是最佳模型选择。


<details>
  <summary>Details</summary>
Motivation: 学生常因学术信息分散在众多文件和网站中而获取日常信息困难，带来困惑。本项目旨在通过AI技术简化信息获取过程。

Method: 项目开发了一个利用生成式人工智能（GenAI）和检索增强生成（RAG）的聊天机器人。测试并评估了多个GenAI模型，采用质量指标和LLM-as-a-Judge方法进行比较。

Result: 项目识别出了两个优异模型：Gemini 2.0 Flash（速度与质量最佳）和Gemma 3n（开源，性能良好）。

Conclusion: Gemini 2.0 Flash 在质量和速度上表现优异，Gemma 3n 则因其良好的性能和开源特性而突出。

Abstract: Students often report difficulties in accessing day-to-day academic information, which is usually spread across numerous institutional documents and websites. This fragmentation results in a lack of clarity and causes confusion about routine university information. This project proposes the development of a chatbot using Generative Artificial Intelligence (GenAI) and Retrieval-Augmented Generation (RAG) to simplify access to such information. Several GenAI models were tested and evaluated based on quality metrics and the LLM-as-a-Judge approach. Among them, Gemini 2.0 Flash stood out for its quality and speed, and Gemma 3n for its good performance and open-source nature.

</details>


### [32] [LLM-as-a-Grader: Practical Insights from Large Language Model for Short-Answer and Report Evaluation](https://arxiv.org/abs/2511.10819)
*Grace Byun,Swati Rajwal,Jinho D. Choi*

Main category: cs.CL

TL;DR: 本文实证分析了GPT-4o在大学课堂自动评分中的表现：与人类助教评分高度一致、但在技术性和开放性问题上有一定局限，为自动评分系统的实际应用提供了数据支持和代码资源。


<details>
  <summary>Details</summary>
Motivation: 当前大模型自动评分广受关注，但在真实课堂环境下与人类评分的一致性尚未充分研究，尤其在复杂、开放性作业评分方面。

Method: 收集本科计算语言学课程的短答题和项目报告，由约50名学生参与；通过GPT-4o和人类助教分别独立评分，然后评估两者的一致性。

Result: GPT-4o与人类评分在短答题上的分数相关系数高达0.98，55%情况下分数完全一致，在项目报告评分上总体一致性也较高，但在技术和开放性问题上存在一定分数波动。

Conclusion: LLM（GPT-4o）在自动评分任务中与人类助教评分有较高的一致性，但在技术性、开放性强的回答上评分有一定变异性，表明其有应用前景也有局限。

Abstract: Large Language Models (LLMs) are increasingly explored for educational tasks such as grading, yet their alignment with human evaluation in real classrooms remains underexamined. In this study, we investigate the feasibility of using an LLM (GPT-4o) to evaluate short-answer quizzes and project reports in an undergraduate Computational Linguistics course. We collect responses from approximately 50 students across five quizzes and receive project reports from 14 teams. LLM-generated scores are compared against human evaluations conducted independently by the course teaching assistants (TAs). Our results show that GPT-4o achieves strong correlation with human graders (up to 0.98) and exact score agreement in 55\% of quiz cases. For project reports, it also shows strong overall alignment with human grading, while exhibiting some variability in scoring technical, open-ended responses. We release all code and sample data to support further research on LLMs in educational assessment. This work highlights both the potential and limitations of LLM-based grading systems and contributes to advancing automated grading in real-world academic settings.

</details>


### [33] [Tracing Multilingual Representations in LLMs with Cross-Layer Transcoders](https://arxiv.org/abs/2511.10840)
*Abir Harrasse,Florent Draye,Zhijing Jin,Bernhard Schölkopf*

Main category: cs.CL

TL;DR: 论文发现多语LLMs内部存在共同的“枢纽语言”表示，多语的解码来自后续层次的特异化处理，高频特征驱动语言输出。主导语种影响显著，理解此机制有助于改进多语能力。


<details>
  <summary>Details</summary>
Motivation: 现有多语LLMs虽能处理多种语言，但其内部如何表征和处理不同语种，以及为何性能仍偏向主导训练语种尚不明确。作者希望揭示LLMs在语言处理过程中的共享机制及其局限。

Method: 作者训练了多组接收不同混合多语数据的LLMs，利用跨层转码器（CLT）与归因图（attribution graphs）对其内部机制进行分析。还通过对高频语言特征的干预实验，验证模型对语种切换输出的可控性。

Result: 实验证据强烈支持“枢纽语言”机制：模型多语言间内部表示趋于一致，语种特异解码出现在后层。高频特征可线性读取与控制语种输出，主语言训练数据对解码机制与归因路径存在显著主导。该机制的挖掘有助于优化多语对齐。

Conclusion: 作者发现，虽然LLMs具备多语种处理能力，但在模型内部存在“枢纽语言”（pivot language）表示机制。模型在各语言间采用几乎一致的内部表示，而具备区分性的多语种解码仅在后续层次逐步显现。影响主要受到高频语言特征的线性读取与干预。此外，主导训练语种对模型机制有显著影响。理解该机制对改进多语对齐很关键。

Abstract: Multilingual Large Language Models (LLMs) can process many languages, yet how they internally represent this diversity remains unclear. Do they form shared multilingual representations with language-specific decoding, and if so, why does performance still favor the dominant training language? To address this, we train a series of LLMs on different mixtures of multilingual data and analyze their internal mechanisms using cross-layer transcoders (CLT) and attribution graphs. Our results provide strong evidence for pivot language representations: the model employs nearly identical representations across languages, while language-specific decoding emerges in later layers. Attribution analyses reveal that decoding relies in part on a small set of high-frequency language features in the final layers, which linearly read out language identity from the first layers in the model. By intervening on these features, we can suppress one language and substitute another in the model's outputs. Finally, we study how the dominant training language influences these mechanisms across attribution graphs and decoding pathways. We argue that understanding this pivot-language mechanism is crucial for improving multilingual alignment in LLMs.

</details>


### [34] [Reinforcing Stereotypes of Anger: Emotion AI on African American Vernacular English](https://arxiv.org/abs/2511.10846)
*Rebecca Dorn,Christina Chance,Casandra Rusti,Charles Bickham,Kai-Wei Chang,Fred Morstatter,Kristina Lerman*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Automated emotion detection is widely used in applications ranging from well-being monitoring to high-stakes domains like mental health and hiring. However, models often rely on annotations that reflect dominant cultural norms, limiting model ability to recognize emotional expression in dialects often excluded from training data distributions, such as African American Vernacular English (AAVE). This study examines emotion recognition model performance on AAVE compared to General American English (GAE). We analyze 2.7 million tweets geo-tagged within Los Angeles. Texts are scored for strength of AAVE using computational approximations of dialect features. Annotations of emotion presence and intensity are collected on a dataset of 875 tweets with both high and low AAVE densities. To assess model accuracy on a task as subjective as emotion perception, we calculate community-informed "silver" labels where AAVE-dense tweets are labeled by African American, AAVE-fluent (ingroup) annotators. On our labeled sample, GPT and BERT-based models exhibit false positive prediction rates of anger on AAVE more than double than on GAE. SpanEmo, a popular text-based emotion model, increases false positive rates of anger from 25 percent on GAE to 60 percent on AAVE. Additionally, a series of linear regressions reveals that models and non-ingroup annotations are significantly more correlated with profanity-based AAVE features than ingroup annotations. Linking Census tract demographics, we observe that neighborhoods with higher proportions of African American residents are associated with higher predictions of anger (Pearson's correlation r = 0.27) and lower joy (r = -0.10). These results find an emergent safety issue of emotion AI reinforcing racial stereotypes through biased emotion classification. We emphasize the need for culturally and dialect-informed affective computing systems.

</details>


### [35] [Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs](https://arxiv.org/abs/2511.10850)
*Stefan Horoi,Sangwoo Cho,Supriyo Chakraborty,Shi-Xiong Zhang,Sambit Sahu,Guy Wolf,Genta Indra Winata*

Main category: cs.CL

TL;DR: 作者提出一种参数空间先对齐的方法，解决了任务算术在技能转移时的负干扰问题，在推理能力迁移和模型融合方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 当前LLM任务算术在模型训练分歧时存在负迁移问题，阻碍技能转移。作者希望通过参数空间对齐来克服此缺陷，提升技能迁移效果。

Method: 通过对Transformer架构固有的参数排列、旋转和缩放对称性进行利用，作者将模型参数空间进行对齐。在此基础上，适配了现代的Grouped-Query Attention和SwiGLU层，使用权重和激活两种对齐策略进行实验。

Result: 在多个难度较高的推理基准上，参数空间对齐的任务算术方法稳定优于标准方法，实现了将高级推理能力成功迁移至无推理能力模型。

Conclusion: 本文提出的参数空间对齐方法能有效提升任务算术的技能转移能力，并在多个推理基准上优于传统方法。该方法有助于专业技能的高效迁移，减少重复微调，提升大模型适应性。

Abstract: Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.

</details>


### [36] [Unsupervised Cycle Detection in Agentic Applications](https://arxiv.org/abs/2511.10650)
*Felix George,Harshit Kumar,Divya Pathak,Kaustabha Ray,Mudit Verma,Pratibha Moogi*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Agentic applications powered by Large Language Models exhibit non-deterministic behaviors that can form hidden execution cycles, silently consuming resources without triggering explicit errors. Traditional observability platforms fail to detect these costly inefficiencies. We present an unsupervised cycle detection framework that combines structural and semantic analysis. Our approach first applies computationally efficient temporal call stack analysis to identify explicit loops and then leverages semantic similarity analysis to uncover subtle cycles characterized by redundant content generation. Evaluated on 1575 trajectories from a LangGraph-based stock market application, our hybrid approach achieves an F1 score of 0.72 (precision: 0.62, recall: 0.86), significantly outperforming individual structural (F1: 0.08) and semantic methods (F1: 0.28). While these results are encouraging, there remains substantial scope for improvement, and future work is needed to refine the approach and address its current limitations.

</details>


### [37] [From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems](https://arxiv.org/abs/2511.10871)
*Parisa Rabbani,Nimet Beyza Bozdag,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: LLMs are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such LLM-judges can reliably assess tasks that require social or conversational judgment. We investigate how an LLM's conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model's performance on direct factual queries with its assessment of a speaker's correctness when the same information is presented within a minimal dialogue, effectively shifting the query from "Is this statement correct?" to "Is this speaker correct?". Furthermore, we apply pressure in the form of a simple rebuttal ("The previous answer is incorrect.") to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like GPT-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8B-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in LLM-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.

</details>


### [38] [ICX360: In-Context eXplainability 360 Toolkit](https://arxiv.org/abs/2511.10879)
*Dennis Wei,Ronny Luss,Xiaomeng Hu,Lucas Monteiro Paes,Pin-Yu Chen,Karthikeyan Natesan Ramamurthy,Erik Miehling,Inge Vejsbjerg,Hendrik Strobelt*

Main category: cs.CL

TL;DR: ICX360是面向大语言模型输入可解释性的开源工具包，内含多种方法和完善教程，助力用户分析模型输出，关注实际应用中的输入影响。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用于重要场景，开发可解释性工具变得关键，尤其需要关注用户输入对模型结果的影响。

Method: 集成了三种近期LLM可解释性方法，包括黑盒（扰动式）和白盒（梯度式）技术，对模型输入进行分析。

Result: ICX360工具包已开源，提供快速上手与详细教程，支持用于检索增强生成、自然语言生成及越狱分析等多样场景。

Conclusion: 提出并发布了ICX360工具包，帮助用户从多种角度解释大语言模型（LLM）的输出，针对用户输入的上下文进行可解释性分析。

Abstract: Large Language Models (LLMs) have become ubiquitous in everyday life and are entering higher-stakes applications ranging from summarizing meeting transcripts to answering doctors' questions. As was the case with earlier predictive models, it is crucial that we develop tools for explaining the output of LLMs, be it a summary, list, response to a question, etc. With these needs in mind, we introduce In-Context Explainability 360 (ICX360), an open-source Python toolkit for explaining LLMs with a focus on the user-provided context (or prompts in general) that are fed to the LLMs. ICX360 contains implementations for three recent tools that explain LLMs using both black-box and white-box methods (via perturbations and gradients respectively). The toolkit, available at https://github.com/IBM/ICX360, contains quick-start guidance materials as well as detailed tutorials covering use cases such as retrieval augmented generation, natural language generation, and jailbreaking.

</details>


### [39] [A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge](https://arxiv.org/abs/2511.10881)
*Jongyoon Song,Sangwon Yu,Sungroh Yoon*

Main category: cs.CL

TL;DR: 本文系统剖析了大语言模型负面偏见的成因与影响因素，提出构建新评测集并探索不同提示方式下的偏见表现，发现补充信息可缓解而链式思考会加重负面偏见，为未来改善LLMs输出可靠性提供方法和方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注检测和处理引发负面偏见的注意力头，但负面偏见的具体诱因尚未被深入剖析。本文旨在细致揭示影响负面偏见的底层因素，为未来缓解该问题提供依据。

Method: 1. 引入新的评测集构建流程，将数据根据模型参数化知识分为正确、错误与知识不足三类。2. 多样化提示场景下观测负面偏见表现。3. 分析模型在不同知识覆盖下的行为。

Result: 发现LLMs在知识不足时会倾向输出否定（负面）回答，负面偏见和提示词格式关系密切。上下文补充和“我不知道”可缓解偏见，链式思考反而增强了偏见，不同类型的提示会影响偏见程度。

Conclusion: 本文表明大语言模型(LLMs)的负面偏见不仅仅来源于模型的负面注意力头，还与提示词格式等多维因素相关，提供特定上下文和引入“我不知道”选项能减轻负面偏见，而链式思考提示则加重偏见。

Abstract: Negative bias refers to the tendency of large language models (LLMs) to excessively generate negative responses in binary decision tasks (e.g., yes-no question answering). Previous research has focused on detecting and addressing negative attention heads that induce negative bias. However, the underlying detailed factors influencing negative bias remain underexplored. In this paper, we demonstrate that LLMs exhibit format-level negative bias, meaning the prompt format more influences their responses than the semantics of the negative response. For the fine-grained study of the negative bias, we introduce a pipeline for constructing the evaluation set, which systematically categorizes the dataset into three subsets based on the model's parametric knowledge: correct, incorrect, and insufficient relevant knowledge. Through analysis of this evaluation set, we identify a shortcut behavior in which models tend to generate negative responses when they lack sufficient knowledge to answer a yes-no question, leading to negative bias. We further examine how negative bias changes under various prompting scenarios related to parametric knowledge. We observe that providing relevant context and offering an "I don't know" option generally reduces negative bias, whereas chain-of-thought prompting tends to amplify the bias. Finally, we demonstrate that the degree of negative bias can vary depending on the type of prompt, which influences the direction of the response. Our work reveals the various factors that influence negative bias, providing critical insights for mitigating it in LLMs.

</details>


### [40] [MedPath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking](https://arxiv.org/abs/2511.10887)
*Nishant Mishra,Wilker Aziz,Iacer Calixto*

Main category: cs.CL

TL;DR: MedPath是一个大规模、具备本体路径增强特性的生物医学EL数据集，统一整合、增强了多种已有资源，为语义解释和下游临床NLP模型创新奠定数据基础。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学NER和EL领域存在数据碎片化、缺乏可解释模型建设资源、以及评估指标语义盲点等瓶颈。为突破这些限制，研究团队开发了MedPath数据集，致力于提升模型的可解释性和评价方式的语义敏感度。

Method: 该研究通过整合九个已有的专家标注EL数据集，基于最新UMLS对所有实体统一归一化，进一步映射到62个生物医学词汇表，并补充了最多11个本体词汇表中的完整本体路径信息。

Result: MedPath数据集的构建，实现了实体归一化、跨词汇映射和本体路径增强，为训练、评估高语义度、可解释、具互操作性的临床NLP模型铺平道路。

Conclusion: MedPath这一新型大规模多领域生物医学实体链接（EL）数据集的提出，有望推进语义丰富、可解释、具备互操作性的生物医学自然语言处理系统的研究与发展。

Abstract: Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is currently hindered by a fragmented data landscape, a lack of resources for building explainable models, and the limitations of semantically-blind evaluation metrics. To address these challenges, we present MedPath, a large-scale and multi-domain biomedical EL dataset that builds upon nine existing expert-annotated EL datasets. In MedPath, all entities are 1) normalized using the latest version of the Unified Medical Language System (UMLS), 2) augmented with mappings to 62 other biomedical vocabularies and, crucially, 3) enriched with full ontological paths -- i.e., from general to specific -- in up to 11 biomedical vocabularies. MedPath directly enables new research frontiers in biomedical NLP, facilitating training and evaluation of semantic-rich and interpretable EL systems, and the development of the next generation of interoperable and explainable clinical NLP models.

</details>


### [41] [From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models](https://arxiv.org/abs/2511.10899)
*Farima Fatahi Bayat,Pouya Pezeshkpour,Estevam Hruschka*

Main category: cs.CL

TL;DR: 本文揭示了工具增强LLMs表面答对却推理变差的问题（TIM），量化其影响，并提出改善框架，推动工具化大模型在严谨推理任务上的可靠应用。


<details>
  <summary>Details</summary>
Motivation: 当前工具增强型LLMs的准确率提升容易掩盖背后推理过程的衰败，尚缺乏对工具使用引发的推理失效机制的理解，以及如何缓解这种现象的方法。

Method: 作者针对Code Interpreter工具，基于PYMATH数学题数据集和多维度评测体系，系统分析了TaLMs在调用工具时推理退化现象，并使用偏好优化实现模型行为改进。

Result: （1）即使正确调用工具，模型推理退化广泛发生；（2）工具使用频率越高，推理连贯性损失越严重，错误类型从算术转向逻辑和假设等更深层次问题；（3）提出的优化方案能提升工具辅助下的推理和准确率。

Conclusion: 虽然工具增强型语言模型（TaLMs）在调用外部工具后答案正确率提升，但其推理连贯性和质量反而下降，出现了“工具诱发短视”（TIM）的失效模式。研究提出了一个优化框架，有助于提升TaLMs在使用工具情境下的推理深度和答案准确性。

Abstract: Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.

</details>


### [42] [Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering](https://arxiv.org/abs/2511.10900)
*Xueren Ge,Sahil Murtaza,Anthony Cortez,Homa Alemzadeh*

Main category: cs.CL

TL;DR: 针对医疗应急问答领域，提出结合细化专业知识的提示与检索策略，大幅提升了大模型的专业答题水平，并通过全部EMS认证仿真测试。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在医学问答领域展现出潜力，但往往忽视了专业领域知识，如临床学科细分和认证等级，现有方法普遍采用通用提示或检索，未利用结构化专业背景，导致在高风险场景下表现有限。本文提出针对医疗应急领域的特定挑战，结合专业知识提升模型表现。

Method: 1. 构建EMSQA数据集，涵盖10个临床学科、4个认证等级、2M标记的知识库。2. 提出Expert-CoT，将链式思维推理与学科和等级条件结合。3. 提出ExpertRAG，将检索增强生成与专业知识库及真实病例结合。4. 设计实验，在4种不同LLM上测试提出的策略。

Result: Expert-CoT策略相较普通链式思维提示，准确率最高提升2.05%；与ExpertRAG结合后，相比标准RAG基线，准确率最高提升4.59%。32B参数的增强模型通过了所有适应性EMS资格考试仿真。

Conclusion: 结合专业领域知识（学科、等级）进行提示和检索，显著提升了大语言模型在医疗高风险场景下的问答能力，使其有望在实际紧急医疗认证模拟考试中应用。

Abstract: Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.

</details>


### [43] [Multimodal Peer Review Simulation with Actionable To-Do Recommendations for Community-Aware Manuscript Revisions](https://arxiv.org/abs/2511.10902)
*Mengze Hong,Di Jiang,Weiwei Zhao,Yawen Li,Yihang Wang,Xinyuan Luo,Yanjie Sun,Chen Jason Zhang*

Main category: cs.CL

TL;DR: 本文提出一种结合多模态大模型与社区知识检索的新型同行评审模拟系统，通过结构化反馈促进学术论文高效修改，实验验证效果优越。


<details>
  <summary>Details</summary>
Motivation: 现有自动化学术评审系统仍以文本输入为主，缺乏多模态和社区上下文，且难以输出用户可操作的反馈，影响论文高效修改与学术交流。

Method: 系统集成了多模态大语言模型（可处理文本和视觉信息），结合基于OpenReview数据的检索增强生成（RAG）技术，将评论转化为结构化的Action:Objective[#]格式任务清单；并与学术写作平台无缝对接，提供实时交互与修订跟踪。

Result: 实验结果显示，本系统生成的评审意见更全面、实用且符合专家标准，在多项指标上优于消融版本，提升了学术写作辅助的透明度和质量。

Conclusion: 本文提出的多模态、社区感知同行评审模拟系统能够提升学术论文修改前的评审质量与效率，生成更具结构化和实用性的反馈，推动更公平、高效的人机协作学术评价。

Abstract: While large language models (LLMs) offer promising capabilities for automating academic workflows, existing systems for academic peer review remain constrained by text-only inputs, limited contextual grounding, and a lack of actionable feedback. In this work, we present an interactive web-based system for multimodal, community-aware peer review simulation to enable effective manuscript revisions before paper submission. Our framework integrates textual and visual information through multimodal LLMs, enhances review quality via retrieval-augmented generation (RAG) grounded in web-scale OpenReview data, and converts generated reviews into actionable to-do lists using the proposed Action:Objective[\#] format, providing structured and traceable guidance. The system integrates seamlessly into existing academic writing platforms, providing interactive interfaces for real-time feedback and revision tracking. Experimental results highlight the effectiveness of the proposed system in generating more comprehensive and useful reviews aligned with expert standards, surpassing ablated baselines and advancing transparent, human-centered scholarly assistance.

</details>


### [44] [Automated Analysis of Learning Outcomes and Exam Questions Based on Bloom's Taxonomy](https://arxiv.org/abs/2511.10903)
*Ramya Kumar,Dhruv Gulwani,Sonit Singh*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper explores the automatic classification of exam questions and learning outcomes according to Bloom's Taxonomy. A small dataset of 600 sentences labeled with six cognitive categories - Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation - was processed using traditional machine learning (ML) models (Naive Bayes, Logistic Regression, Support Vector Machines), recurrent neural network architectures (LSTM, BiLSTM, GRU, BiGRU), transformer-based models (BERT and RoBERTa), and large language models (OpenAI, Gemini, Ollama, Anthropic). Each model was evaluated under different preprocessing and augmentation strategies (for example, synonym replacement, word embeddings, etc.). Among traditional ML approaches, Support Vector Machines (SVM) with data augmentation achieved the best overall performance, reaching 94 percent accuracy, recall, and F1 scores with minimal overfitting. In contrast, the RNN models and BERT suffered from severe overfitting, while RoBERTa initially overcame it but began to show signs as training progressed. Finally, zero-shot evaluations of large language models (LLMs) indicated that OpenAI and Gemini performed best among the tested LLMs, achieving approximately 0.72-0.73 accuracy and comparable F1 scores. These findings highlight the challenges of training complex deep models on limited data and underscore the value of careful data augmentation and simpler algorithms (such as augmented SVM) for Bloom's Taxonomy classification.

</details>


### [45] [Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D](https://arxiv.org/abs/2511.10912)
*Arsh Gupta,Ajay Narayanan Sridhar,Bonam Mingole,Amulya Yadav*

Main category: cs.CL

TL;DR: 本文构建了豪斯医生数据集，评价主流LLMs在叙述案例罕见病诊断上的表现，新模型显著优于旧模型，开辟了AI医学诊断新方向。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在多个领域展现能力，但在通过叙述医学病例诊断罕见疾病方面鲜有研究，亟需建立相关基准和评估框架，促进AI医疗诊断的发展。

Method: 引入176个由美剧《豪斯医生》提取的症状-诊断配对数据集，对四个主流大型语言模型进行诊断推理性能评测。

Result: 四个模型在诊断任务上的准确率从16.48%到38.64%不等，新一代模型表现提高2.3倍。所有模型在罕见病诊断上均面临较大挑战，但整体趋势向好。

Conclusion: 大型语言模型在罕见疾病的叙述性医学案例诊断中表现有限，但新一代模型有显著改进，显示出未来提升空间。

Abstract: Large language models (LLMs) have demonstrated capabilities across diverse domains, yet their performance on rare disease diagnosis from narrative medical cases remains underexplored. We introduce a novel dataset of 176 symptom-diagnosis pairs extracted from House M.D., a medical television series validated for teaching rare disease recognition in medical education. We evaluate four state-of-the-art LLMs such as GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro on narrative-based diagnostic reasoning tasks. Results show significant variation in performance, ranging from 16.48% to 38.64% accuracy, with newer model generations demonstrating a 2.3 times improvement. While all models face substantial challenges with rare disease diagnosis, the observed improvement across architectures suggests promising directions for future development. Our educationally validated benchmark establishes baseline performance metrics for narrative medical reasoning and provides a publicly accessible evaluation framework for advancing AI-assisted diagnosis research.

</details>


### [46] [CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology](https://arxiv.org/abs/2511.10930)
*Richard J. Young,Alice M. Matthews*

Main category: cs.CL

TL;DR: 专为心脏学领域基于教科书训练的嵌入模型CardioEmbed，大幅提升了心脏语义检索准确率，并在其他生物医学基准上表现良好，优于现有主流医学嵌入模型。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学文本嵌入多依赖PubMed文献，但临床心脏科更多依赖教科书与专业术语，学术论文模型在临床应用效果有限。

Method: 基于Qwen3-Embedding-8B，采用对比学习（InfoNCE loss，批内负样本）对经过去重的七本心脏病学教科书约15万句子进行训练。

Result: CardioEmbed在心脏语义检索任务中达到99.60%的准确率，比SOTA模型MedTE提升15.94个百分点；在通用医学基准上也有竞争力表现，如在BIOSSES上取得0.77 Spearman，在SciFact上取得0.61 NDCG@10。

Conclusion: 通过在全面心脏病学教科书上进行领域专门化训练，可极大提升临床心脏学文本嵌入模型的语义检索能力，CardioEmbed在心脏相关任务的准确率远超现有先进模型。

Abstract: Biomedical text embeddings have primarily been developed using research literature from PubMed, yet clinical cardiology practice relies heavily on procedural knowledge and specialized terminology found in comprehensive textbooks rather than research abstracts. This research practice gap limits the effectiveness of existing embedding models for clinical applications incardiology. This study trained CardioEmbed, a domain-specialized embedding model based on Qwen3-Embedding-8B, using contrastive learning on a curated corpus of seven comprehensive cardiology textbooks totaling approximately 150,000 sentences after deduplication. The model employs InfoNCE loss with in-batch negatives and achieves 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, a +15.94 percentage point improvement over MedTE, the current state-of-the-art medical embedding model. On MTEB medical benchmarks, the model obtained BIOSSES 0.77 Spearman and SciFact 0.61 NDCG@10, indicating competitive performance on related biomedical domains. Domain-specialized training on comprehensive clinical textbooks yields near-perfect cardiology retrieval (99.60% Acc@1), improving over MedTE by +15.94 percentage points.

</details>


### [47] [DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains](https://arxiv.org/abs/2511.10984)
*Xiying Zhao,Zhoufutu Wen,Zhixuan Chen,Jingzhe Ding,Jianpeng Jiao,Shuai Li,Xi Li,Danni Liang,Shengda Long,Qianqian Liu,Xianbo Wu,Hongwan Gao,Xiang Gao,Liang Hu,Jiashuo Liu,Mengyun Liu,Weiran Shi,Chenghao Yang,Qianyu Yang,Xuanliang Zhang,Ge Zhang,Wenhao Huang*

Main category: cs.CL

TL;DR: 本文提出了面向专业领域中英文长文本翻译的DiscoX新基准及无参考自动评测工具Metric-S，实验发现大模型翻译与人类专家存在巨大差距，为后续专业级机器翻译研究提供了有力支撑。


<details>
  <summary>Details</summary>
Motivation: 当前专业领域的跨语言知识传播需求增加，但相关翻译评测多集中在语句级别，缺少对语篇级连贯性及专业术语精确性的系统性测评，这限制了机器翻译系统在学术/专业场景的实际应用和改进。

Method: 提出了DiscoX基准数据集，包含7大领域200篇专业文本，用于中英文层面的语篇翻译评测；同时开发了Metric-S自动评价系统，实现无参考译文情况下对准确性、流畅性和适切性进行精细评测；通过实验对多种翻译系统及大语言模型进行系统验证，并与人工评判结果比对一致性。

Result: DiscoX基准能够显著区分不同系统，Metric-S自动评价与人工高度一致且优于现有指标。实验中，所有主流大语言模型及机器翻译系统在该任务上均落后于人类专家，体现了任务的高难度及待突破空间。

Conclusion: 机器翻译在专业领域语篇级别的翻译上仍存在明显不足，即使是最先进的大模型也明显落后于人类专家。新提出的基准和评价系统能够更好地刻画这一挑战，并推动后续进展。

Abstract: The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.

</details>


### [48] [When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets](https://arxiv.org/abs/2511.10985)
*Aladin Djuhera,Farhan Ahmed,Swanand Ravindra Kadhe,Syed Zawad,Heiko Ludwig,Holger Boche*

Main category: cs.CL

TL;DR: 本文基于Magpie框架对五个主流开源DPO数据集进行细致分析与标注，并据此筛选优化，提出了表现更优的小型混合DPO数据集UltraMix并将相关数据全面公开。


<details>
  <summary>Details</summary>
Motivation: 公开DPO数据集之间缺乏系统比较，主要由于高昂算力消耗和缺乏完善的数据质量注释，导致难以理解不同集的样本偏好选择、任务覆盖情况以及其与人类判断的契合度。为解决这些瓶颈，有必要对这些数据集进行全面数据中心分析，并推动高质量DPO数据集建设。

Method: 利用Magpie框架对每条样本进行任务类别、输入质量、偏好奖励（基于奖励模型的信号）的详细注释，无需依赖人工标注，从而实现在多个数据集间对偏好质量的细致比较。根据分析结果，系统性筛选并混合五个主流数据集，去除噪声和冗余样本，最终得出更优的UltraMix语料库。

Result: 通过Magpie标注和多维分析，揭示了现有数据集在奖励边际等结构和质量上的差异。新构建的UltraMix混合数据集体积比表现最优的数据集小30%，但在主要评测基准上反而取得更优的性能。全部标注和数据公开，有助于未来领域研究。

Conclusion: 本文提出并实现了对流行开源DPO数据集的新型数据中心分析方法，并据此精细地构建了一个表现优于单一开源集的新DPO混合语料库UltraMix，还公开了注释和元数据以促进后续研究。

Abstract: Aligning large language models (LLMs) is a central objective of post-training, often achieved through reward modeling and reinforcement learning methods. Among these, direct preference optimization (DPO) has emerged as a widely adopted technique that fine-tunes LLMs on preferred completions over less favorable ones. While most frontier LLMs do not disclose their curated preference pairs, the broader LLM community has released several open-source DPO datasets, including TuluDPO, ORPO, UltraFeedback, HelpSteer, and Code-Preference-Pairs. However, systematic comparisons remain scarce, largely due to the high computational cost and the lack of rich quality annotations, making it difficult to understand how preferences were selected, which task types they span, and how well they reflect human judgment on a per-sample level. In this work, we present the first comprehensive, data-centric analysis of popular open-source DPO corpora. We leverage the Magpie framework to annotate each sample for task category, input quality, and preference reward, a reward-model-based signal that validates the preference order without relying on human annotations. This enables a scalable, fine-grained inspection of preference quality across datasets, revealing structural and qualitative discrepancies in reward margins. Building on these insights, we systematically curate a new DPO mixture, UltraMix, that draws selectively from all five corpora while removing noisy or redundant samples. UltraMix is 30% smaller than the best-performing individual dataset yet exceeds its performance across key benchmarks. We publicly release all annotations, metadata, and our curated mixture to facilitate future research in data-centric preference optimization.

</details>


### [49] [Automata-Based Steering of Large Language Models for Diverse Structured Generation](https://arxiv.org/abs/2511.11018)
*Xiaokun Luan,Zeming Wei,Yihao Zhang,Meng Sun*

Main category: cs.CL

TL;DR: 通过引入自动机遍历历史，创新方法有效提升了大语言模型结构性生成任务中的多样性且生成效率基本不变，在实际测试案例生成中具表现力。


<details>
  <summary>Details</summary>
Motivation: 以往结构化生成方法虽然能保证有效性，却在多样性方面存在明显不足，亟需新方法来改善结构和内容的多样性。

Method: 采用自动机遍历历史，引导大语言模型生成具备新颖结构模式的输出，从而提升多样性。

Result: 实验结果表明，该方法在结构和内容多样性方面显著优于传统方法，案例研究还证明了其在开源库测试用例生成中的有效性。

Conclusion: 作者提出的方法可以在保证生成效率的同时，大幅提高大语言模型结构性输出的内容和结构多样性。

Abstract: Large language models (LLMs) are increasingly tasked with generating structured outputs. While structured generation methods ensure validity, they often lack output diversity, a critical limitation that we confirm in our preliminary study. We propose a novel method to enhance diversity in automaton-based structured generation. Our approach utilizes automata traversal history to steer LLMs towards novel structural patterns. Evaluations show our method significantly improves structural and content diversity while maintaining comparable generation efficiency. Furthermore, we conduct a case study showcasing the effectiveness of our method in generating diverse test cases for testing open-source libraries.

</details>


### [50] [Correcting Mean Bias in Text Embeddings: A Refined Renormalization with Training-Free Improvements on MMTEB](https://arxiv.org/abs/2511.11041)
*Xingyu Ren,Youran Sun,Haoyu Liang*

Main category: cs.CL

TL;DR: 现有文本嵌入模型输出存在全局偏置，作者提出无需训练的Renormalization方法有效去除偏置，显著提升了多语言、多任务嵌入表现。


<details>
  <summary>Details</summary>
Motivation: 当前主流文本嵌入模型输出存在一致的偏置成分，该偏置影响模型在下游任务的表现，因此需提出一种简单有效的消除该偏置的方法。

Method: 提出一种无须额外训练、即插即用的去偏置方法（Renormalization），其核心操作是对嵌入向量进行中心化，分为直接减去均值或减去在均值上的投影两个变体，并通过跨38个模型的MMTEB数据集的实验证明其有效性。

Result: 该方法在38个模型的检索、分类等任务上分别取得了9.7σ、3.1σ和0.8σ的显著性能提升；理论上预期并实验证明减去投影优于直接减去均值。

Conclusion: 文中提出的Renormalization方法能有效去除文本嵌入模型的固定偏置，提升多种下游任务的表现，验证了理论基础并在各大模型中广泛适用。

Abstract: We find that current text embedding models produce outputs with a consistent bias, i.e., each embedding vector $e$ can be decomposed as $\tilde{e} + μ$, where $μ$ is almost identical across all sentences. We propose a plug-and-play, training-free and lightweight solution called Renormalization. Through extensive experiments, we show that renormalization consistently and statistically significantly improves the performance of existing models on the Massive Multilingual Text Embedding Benchmark (MMTEB). In particular, across 38 models, renormalization improves performance by 9.7 $σ$ on retrieval tasks, 3.1 $σ$ on classification tasks, and 0.8 $σ$ on other types of tasks. Renormalization has two variants: directly subtracting $μ$ from $e$, or subtracting the projection of $e$ onto $μ$. We theoretically predict that the latter performs better, and our experiments confirm this prediction.

</details>


### [51] [Can LLMs Detect Their Own Hallucinations?](https://arxiv.org/abs/2511.11087)
*Sora Kadotani,Kosuke Nishida,Kyosuke Nishida*

Main category: cs.CL

TL;DR: 本文发现，通过链式思维（CoT）方法，GPT-3.5 Turbo能检测自身约58.2%的幻觉，显示LLMs在知识足够时有自我纠错的潜力。


<details>
  <summary>Details</summary>
Motivation: LLMs在生成文本时虽流畅，但可能会出现事实幻觉。本研究旨在探索LLMs是否具备识别自身幻觉内容的能力，提高其生成内容的可靠性。

Method: 提出了一种利用链式思维（CoT）作为分类方法，通过模型参数提取知识用于幻觉检测。将幻觉检测任务转化为句子级别的分类任务，并设计了评估LLM幻觉检测能力的框架。

Result: 实验证明，使用CoT方法的GPT-3.5 Turbo能够检测并识别自身58.2%的幻觉内容。

Conclusion: LLMs在参数中包含足够知识的前提下，利用链式思维（CoT）能够有效检测自身的幻觉内容。

Abstract: Large language models (LLMs) can generate fluent responses, but sometimes hallucinate facts. In this paper, we investigate whether LLMs can detect their own hallucinations. We formulate hallucination detection as a classification task of a sentence. We propose a framework for estimating LLMs' capability of hallucination detection and a classification method using Chain-of-Thought (CoT) to extract knowledge from their parameters. The experimental results indicated that GPT-$3.5$ Turbo with CoT detected $58.2\%$ of its own hallucinations. We concluded that LLMs with CoT can detect hallucinations if sufficient knowledge is contained in their parameters.

</details>


### [52] [Analysing Personal Attacks in U.S. Presidential Debates](https://arxiv.org/abs/2511.11108)
*Ruban Goyal,Rohitash Chandra,Sonit Singh*

Main category: cs.CL

TL;DR: 该研究提出并验证了一套基于人工标注和先进语言模型（如BERT和LLMs）的人身攻击检测框架，展示了现代深度学习工具在分析美国总统辩论有害言论中的应用潜力和准确性。


<details>
  <summary>Details</summary>
Motivation: 近年来深度学习与transformer语言模型（如BERT及大型语言模型）在语言检测上的进步为自动识别有害言论带来新契机。此外，人身攻击作为总统辩论中的突出特征，对公众认知有深远影响，亟需更智能化的分析工具。

Method: 本研究首先对2016、2020和2024年美国总统辩论转录文本进行了人工标注，然后结合统计分析与基于BERT和大型语言模型的自动化检测方法，评估了这些模型在正式政治语境中识别人身攻击的能力。

Result: 结果表明，经过任务微调的transformer模型及通用大型语言模型在正式辩论文本中检测人身攻击方面表现良好，为自动化的政治言论分析提供了强有力的技术手段。

Conclusion: 通过结合人工标注和语言模型分析，研究表明现代深度学习模型（尤其是微调的transformer模型和通用大模型）能够有效检测美国总统辩论中的人身攻击，促进了对政治交流的深入理解。

Abstract: Personal attacks have become a notable feature of U.S. presidential debates and play an important role in shaping public perception during elections. Detecting such attacks can improve transparency in political discourse and provide insights for journalists, analysts and the public. Advances in deep learning and transformer-based models, particularly BERT and large language models (LLMs) have created new opportunities for automated detection of harmful language. Motivated by these developments, we present a framework for analysing personal attacks in U.S. presidential debates. Our work involves manual annotation of debate transcripts across the 2016, 2020 and 2024 election cycles, followed by statistical and language-model based analysis. We investigate the potential of fine-tuned transformer models alongside general-purpose LLMs to detect personal attacks in formal political speech. This study demonstrates how task-specific adaptation of modern language models can contribute to a deeper understanding of political communication.

</details>


### [53] [AV-Dialog: Spoken Dialogue Models with Audio-Visual Input](https://arxiv.org/abs/2511.11124)
*Tuochao Chen,Bandhav Veluri,Hongyu Gong,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: AV-Dialog 利用音频和视觉信息，针对嘈杂环境下多说话人对话，增强了说话人识别和接话能力，整体优于传统语音模型。


<details>
  <summary>Details</summary>
Motivation: 现有对话模型在吵闹、多人讲话场景下回复经常不相关、接话很生硬，因此需要集成更多模态信息提升鲁棒性和人机交互质量。

Method: 该框架融合了音频分词、多任务多阶段训练，并使用了单说话人、合成和真实音视对话数据集进行优化。

Result: 相比于仅语音模型，AV-Dialog 显著降低了转录错误、提升了轮次预测的准确性，并提升了人类评价的对话质量。

Conclusion: AV-Dialog 多模态对话框架能有效提升嘈杂多说话人环境下的对话模型表现，在语音和视觉干扰下依然能准确识别说话人、实现流畅沟通。

Abstract: Dialogue models falter in noisy, multi-speaker environments, often producing irrelevant responses and awkward turn-taking. We present AV-Dialog, the first multimodal dialog framework that uses both audio and visual cues to track the target speaker, predict turn-taking, and generate coherent responses. By combining acoustic tokenization with multi-task, multi-stage training on monadic, synthetic, and real audio-visual dialogue datasets, AV-Dialog achieves robust streaming transcription, semantically grounded turn-boundary detection and accurate responses, resulting in a natural conversational flow. Experiments show that AV-Dialog outperforms audio-only models under interference, reducing transcription errors, improving turn-taking prediction, and enhancing human-rated dialogue quality. These results highlight the power of seeing as well as hearing for speaker-aware interaction, paving the way for {spoken} dialogue agents that perform {robustly} in real-world, noisy environments.

</details>


### [54] [Speech-Aware Long Context Pruning and Integration for Contextualized Automatic Speech Recognition](https://arxiv.org/abs/2511.11139)
*Yiming Rong,Yixin Zhang,Ziyi Wang,Deyang Jiang,Yunlong Zhao,Haoran Wu,Shiyu Zhou,Bo Xu*

Main category: cs.CL

TL;DR: 本文提出了一种通过动态剪枝和注意力池化机制优化上下文利用的新方法SAP$^{2}$，在多个数据集上实现了当前最佳识别准确率，尤其在需要领域知识的复杂场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有ASR系统在需要领域知识的情境化场景（如会议演讲）下，因模型窗口受限和大量无关信息干扰，难以有效利用长距离上下文。该研究致力于解决上下文信息稀疏和冗余，提升情境化识别表现。

Method: 提出了一种SAP$^{2}$（Speech-Driven Attention-based Pooling Pruning）框架，分为两个阶段动态剪枝和整合相关的上下文关键词。每个阶段采用了基于注意力池化的机制来压缩上下文嵌入，同时保留与语音相关的重要信息。

Result: 在SlideSpeech和LibriSpeech两个数据集上，SAP$^{2}$分别实现了7.71%和1.12%的词错误率（WER），在SlideSpeech上相比无上下文基线，相关关键词错误率（B-WER）降低了41.1%。该方法在大量上下文输入下也展现出良好的可扩展性和稳定性。

Conclusion: SAP$^{2}$方法在利用长上下文信息进行自动语音识别方面效果显著，能够高效提升在复杂场景下的识别准确率，并在多个数据集上取得了最先进的表现。

Abstract: Automatic speech recognition (ASR) systems have achieved remarkable performance in common conditions but often struggle to leverage long-context information in contextualized scenarios that require domain-specific knowledge, such as conference presentations. This challenge arises primarily due to constrained model context windows and the sparsity of relevant information within extensive contextual noise. To solve this, we propose the SAP$^{2}$ method, a novel framework that dynamically prunes and integrates relevant contextual keywords in two stages. Specifically, each stage leverages our proposed Speech-Driven Attention-based Pooling mechanism, enabling efficient compression of context embeddings while preserving speech-salient information. Experimental results demonstrate state-of-the-art performance of SAP$^{2}$ on the SlideSpeech and LibriSpeech datasets, achieving word error rates (WER) of 7.71% and 1.12%, respectively. On SlideSpeech, our method notably reduces biased keyword error rates (B-WER) by 41.1% compared to non-contextual baselines. SAP$^{2}$ also exhibits robust scalability, consistently maintaining performance under extensive contextual input conditions on both datasets.

</details>


### [55] [Adverbs Revisited: Enhancing WordNet Coverage of Adverbs with a Supersense Taxonomy](https://arxiv.org/abs/2511.11214)
*Jooyoung Lee,Jader Martins Camboim de Sá*

Main category: cs.CL

TL;DR: 该文提出并验证了一套系统的副词超类语义分类，扩展了WordNet资源，对多种NLP应用具有促进作用。


<details>
  <summary>Details</summary>
Motivation: WordNet在名词和动词上有丰富的语义超类体系，但副词的系统性语义分类仍然缺失，影响了其在NLP应用中的表现。本文旨在弥补这一空白，为副词建立系统的超类语义分类。

Method: 提出了一套基于语言学的副词超类语义分类体系，并通过人工标注实验对该体系进行了实证检验，包括对自然文本中副词的广泛覆盖率和标注一致性分析。

Result: 实验结果显示，该分类体系可以广泛覆盖自然文本中的副词，人类标注者能够可靠地分配这些类别。该分类体系的引入拓展了WordNet的覆盖范围，提高了其和语言学理论的一致性。

Conclusion: 本文提出了副词超类语义分类体系，实证表明该体系具较强适用性和一致性，对WordNet扩展和多项NLP任务有积极促进作用，并指明了未来研究方向。

Abstract: WordNet offers rich supersense hierarchies for nouns and verbs, yet adverbs remain underdeveloped, lacking a systematic semantic classification. We introduce a linguistically grounded supersense typology for adverbs, empirically validated through annotation, that captures major semantic domains including manner, temporal, frequency, degree, domain, speaker-oriented, and subject-oriented functions. Results from a pilot annotation study demonstrate that these categories provide broad coverage of adverbs in natural text and can be reliably assigned by human annotators. Incorporating this typology extends WordNet's coverage, aligns it more closely with linguistic theory, and facilitates downstream NLP applications such as word sense disambiguation, event extraction, sentiment analysis, and discourse modeling. We present the proposed supersense categories, annotation outcomes, and directions for future work.

</details>


### [56] [LANE: Lexical Adversarial Negative Examples for Word Sense Disambiguation](https://arxiv.org/abs/2511.11234)
*Jader Martins Camboim de Sá,Jooyoung Lee,Cédric Pruski,Marcos Da Silveira*

Main category: cs.CL

TL;DR: 为提升神经语言模型细粒度词义解析能力，提出了LANE对抗性训练策略，通过生成有针对性的负样本，有效提升了模型在词义消歧及语义变化检测任务中的表现，并且方法通用。


<details>
  <summary>Details</summary>
Motivation: 现有神经语言模型倾向于记忆全句的全局语义，导致难以捕捉目标词的本地语义变化和细微含义，亟需针对性地强化模型对目标词语义的辨识能力。

Method: 提出了一种称为LANE的新型对抗性训练策略，通过在训练集中选择性标记替代词，生成具有挑战性的负样本，促使模型区分同一句子中不同标记词带来的语义细微差别。

Result: 在词义变化检测和词义消歧任务的标杆数据集上，LANE方法获得了对比方法更优的效果，并在定性分析中证实生成的负样本促使模型学习到更细致的语义区分能力。

Conclusion: 提出的LANE方法能够提升神经语言模型对词语细粒度语义的区分能力，相较于标准对比学习方法在词义消歧和词义变化检测任务中表现更佳。

Abstract: Fine-grained word meaning resolution remains a critical challenge for neural language models (NLMs) as they often overfit to global sentence representations, failing to capture local semantic details. We propose a novel adversarial training strategy, called LANE, to address this limitation by deliberately shifting the model's learning focus to the target word. This method generates challenging negative training examples through the selective marking of alternate words in the training set. The goal is to force the model to create a greater separability between same sentences with different marked words. Experimental results on lexical semantic change detection and word sense disambiguation benchmarks demonstrate that our approach yields more discriminative word representations, improving performance over standard contrastive learning baselines. We further provide qualitative analyses showing that the proposed negatives lead to representations that better capture subtle meaning differences even in challenging environments. Our method is model-agnostic and can be integrated into existing representation learning frameworks.

</details>


### [57] [KGQuest: Template-Driven QA Generation from Knowledge Graphs with LLM-Based Refinement](https://arxiv.org/abs/2511.11258)
*Sania Nayab,Marco Simoni,Giulio Rossolini,Andrea Saracino*

Main category: cs.CL

TL;DR: 该工作提出并验证了一种自动从知识图谱高质量生成问答对的新方法，通过模板和大语言模型优化，兼顾了可扩展性与语言优质性。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱问答生成方法在扩展性、语言质量和事实一致性上表现不足，亟需一种既高效又保证高语义质量的自动化生成方法。

Method: 先对知识图谱三元组按关系聚类，通过实体类型和关系制定可复用的自然语言模板，随后利用大语言模型对模板进行语言质量优化，并通过干扰项选择策略生成答案选项。

Result: 实验表明，该混合方法不仅可扩展性强，还能生成流畅且语法精准的高质量问答对，满足教育和语言模型训练等多种实际需求。

Conclusion: 本论文提出了一种高效且可扩展的知识图谱自动生成问答对的方法，能够结合语言流畅性和事实精确性，显著提升生成问答对的质量。

Abstract: The generation of questions and answers (QA) from knowledge graphs (KG) plays a crucial role in the development and testing of educational platforms, dissemination tools, and large language models (LLM). However, existing approaches often struggle with scalability, linguistic quality, and factual consistency. This paper presents a scalable and deterministic pipeline for generating natural language QA from KGs, with an additional refinement step using LLMs to further enhance linguistic quality. The approach first clusters KG triplets based on their relations, creating reusable templates through natural language rules derived from the entity types of objects and relations. A module then leverages LLMs to refine these templates, improving clarity and coherence while preserving factual accuracy. Finally, the instantiation of answer options is achieved through a selection strategy that introduces distractors from the KG. Our experiments demonstrate that this hybrid approach efficiently generates high-quality QA pairs, combining scalability with fluency and linguistic precision.

</details>


### [58] [LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models](https://arxiv.org/abs/2511.11315)
*Jawad Ibn Ahad,Muhammad Rafsan Kabir,Robin Krambroeckers,Sifat Momen,Nabeel Mohammed,Shafin Rahman*

Main category: cs.CL

TL;DR: 这篇论文提出了一种高效的金融大模型微调方法LAET，通过只对关键层进行微调，大幅度降低了硬件需求，同时性能仍优于现有最强模型，非常适合实际金融应用场景。


<details>
  <summary>Details</summary>
Motivation: 当前金融领域的主流大语言模型（如BloombergGPT、FinMA等）虽然在多项金融NLP任务上展现出强劲性能，但它们对算力资源要求极高，限制了中小型组织的应用，因此亟需提出更高效的模型微调方案。

Method: 提出Layer-wise Adaptive Ensemble Tuning (LAET)，即通过分析预训练大模型的隐藏层表示，只对最有效的若干层进行微调，其余层保持冻结状态，以此来降低调整成本并提升针对金融任务的表现。

Result: LAET方法极大降低了模型微调的计算负担，并在金融NLP任务（如情感分析、股票走势预测等）中超越了包括GPT-4在内的最先进基线，即使是在规模较小的参数模型（如3B参数）下也能取得优异结果。

Conclusion: 提出的LAET方法在金融NLP任务中取得了优异表现，不仅能减少计算资源消耗，还能超越现有主流模型，包括GPT-4，表明该方法既高效又具有应用价值。

Abstract: Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.

</details>


### [59] [NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery](https://arxiv.org/abs/2511.11324)
*Anurag J. Vaidya,Felix Meissen,Daniel C. Castro,Shruthi Bannur,Tristan Lazard,Drew F. K. Williamson,Faisal Mahmood,Javier Alvarez-Valle,Stephanie L. Hyland,Kenza Bouzid*

Main category: cs.CL

TL;DR: 文章提出NOVA智能体框架，可自动将科研问题转为数字病理分析流程，整合了49个专业工具，并可动态扩展，是一种兼具自动化与灵活性的分析平台。在SlideQuest挑战中表现优异，具备临床应用和科研发现的广泛潜力。


<details>
  <summary>Details</summary>
Motivation: 当前数字化病理分析流程复杂且耗时，需要高度专业知识，限制了其普及和应用。作者希望通过智能体和自动化的方法降低门槛，提高分析效率和灵活性。

Method: 提出了NOVA智能体框架，通过自动生成和执行Python代码，将科学问题转化为可执行的数据分析流程，并整合了49个针对病理领域的工具。此外，还可以按需动态创建新工具。为了评估系统，作者构建了SlideQuest基准，包括90个由病理学家和生物医学专家验证的问题，涵盖数据处理、定量分析和假设检验。

Result: NOVA在SlideQuest基准测试中优于现有编码智能体基线方法，并通过病理科医生验证的案例展示了其将组织形态与预后相关分型关联的能力，体现了NOVA在大规模发现中的潜力。

Conclusion: NOVA框架能够有效提升数字病理分析的自动化和可扩展性，能处理复杂的分析任务，并在SlideQuest基准上表现优异。

Abstract: Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.

</details>


### [60] [M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text](https://arxiv.org/abs/2511.11340)
*Salima Lamsiyah,Saad Ezzini,Abdelkader El Mahdaouy,Hamza Alami,Abdessamad Benlahbib,Samir El Amrany,Salmane Chafik,Hicham Hammouchi*

Main category: cs.CL

TL;DR: 提出了跨领域AI文本检测共享任务和大规模数据集，实现了对新闻和学术AI文本的自动识别，推动了相关技术发展。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs生成高流畅度文本，信息真实性和学术研究面临挑战，需要跨领域自动识别AI文本，推进行业规范与诚信。

Method: 设定新闻文章检测和学术写作检测两大子任务，采集30,000条均衡数据，包括多种LLM生成文本，通过公开比赛方式，分析团队提交的检测策略。

Result: 共46支团队参与报名，4支团队完成最终提交，均覆盖两个任务。论文对参赛团队方法进行了总结并提供了数据集，为AI文本检测领域提供了新资源和基线。

Conclusion: 本文介绍了M-DAIGT共享任务，证实通过多域大规模数据集及多团队参与，AI生成文本检测在新闻和学术领域均可实施，并展望了后续研究方向。

Abstract: The generation of highly fluent text by Large Language Models (LLMs) poses a significant challenge to information integrity and academic research. In this paper, we introduce the Multi-Domain Detection of AI-Generated Text (M-DAIGT) shared task, which focuses on detecting AI-generated text across multiple domains, particularly in news articles and academic writing. M-DAIGT comprises two binary classification subtasks: News Article Detection (NAD) (Subtask 1) and Academic Writing Detection (AWD) (Subtask 2). To support this task, we developed and released a new large-scale benchmark dataset of 30,000 samples, balanced between human-written and AI-generated texts. The AI-generated content was produced using a variety of modern LLMs (e.g., GPT-4, Claude) and diverse prompting strategies. A total of 46 unique teams registered for the shared task, of which four teams submitted final results. All four teams participated in both Subtask 1 and Subtask 2. We describe the methods employed by these participating teams and briefly discuss future directions for M-DAIGT.

</details>


### [61] [Studies with impossible languages falsify LMs as models of human language](https://arxiv.org/abs/2511.11389)
*Jeffrey S. Bowers,Jeff Mitchell*

Main category: cs.CL

TL;DR: 与婴儿不同，语言模型缺乏对自然语言的归纳偏置，导致其学习自然语言和某些不自然语言没有明显差异，只是在面对高度复杂或随机语言时难以学习。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在语言学习过程中与人类婴儿的异同，特别是在面对自然语言和‘不可能’（非自然）语言时的学习能力对比，以考察LM是否具备类似人类的归纳偏置。

Method: 回顾现有文献，与Futrell和Mahowald（2024）提出的观点进行比较与分析，总结主要发现，讨论语言模型对自然语言与非自然语言的学习表现。

Result: LM在学习自然语言与许多‘不可能’语言时表现相近，仅在面对更复杂或随机的规则时学习困难。LM缺乏人类特有的归纳偏置，这可能限制了其更深入的类人语言习得能力。

Conclusion: 语言模型（LM）缺乏支持人类语言习得的归纳偏置，因此与婴儿不同，LM对“非自然”语言的学习能力与自然语言无明显差异。对LM来说，难以学习的“非自然”语言本质上更复杂或更随机。

Abstract: According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures. We review the literature and show that LMs often learn attested and many impossible languages equally well. Difficult to learn impossible languages are simply more complex (or random). LMs are missing human inductive biases that support language acquisition.

</details>


### [62] [MajinBook: An open catalogue of digital world literature with likes](https://arxiv.org/abs/2511.11412)
*Antoine Mazières,Thierry Poibeau*

Main category: cs.CL

TL;DR: MajinBook 结合影子图书馆与 Goodreads 数据，构建了53万+高质量可用英文书目语料库，全部开放，支持计算社科与文化分析，且具备合规探讨。


<details>
  <summary>Details</summary>
Motivation: 为计算社会科学和文化分析领域提供一个高质量、可机读、开放获取的大规模图书语料库，弥补传统数据集的偏见与局限。

Method: 将 Library Genesis 和 Z-Library 等影子图书馆中的元数据，与 Goodreads 的结构化书目信息进行匹配和融合，重点选择原生 EPUB 格式以提升数据可读性，并引入英文主要，加配法语、德语、西班牙语数据集。评估了匹配策略的准确性，并分析法律合规性。

Result: 打造了超过53.9万条、附带详细元数据的英文图书语料库，并额外发布多语种次级数据集。验证了数据链接的准确性，并探讨了其在 EU 与 US 科研挖掘许可下的合法性。

Conclusion: MajinBook 成功整合影子图书馆和 Goodreads 元数据，构建了高精准度的大型图书数据集，并完全开放底层数据以支持研究用途。

Abstract: This data paper introduces MajinBook, an open catalogue designed to facilitate the use of shadow libraries--such as Library Genesis and Z-Library--for computational social science and cultural analytics. By linking metadata from these vast, crowd-sourced archives with structured bibliographic data from Goodreads, we create a high-precision corpus of over 539,000 references to English-language books spanning three centuries, enriched with first publication dates, genres, and popularity metrics like ratings and reviews. Our methodology prioritizes natively digital EPUB files to ensure machine-readable quality, while addressing biases in traditional corpora like HathiTrust, and includes secondary datasets for French, German, and Spanish. We evaluate the linkage strategy for accuracy, release all underlying data openly, and discuss the project's legal permissibility under EU and US frameworks for text and data mining in research.

</details>


### [63] [Proactive Hearing Assistants that Isolate Egocentric Conversations](https://arxiv.org/abs/2511.11473)
*Guilin Hu,Malek Itani,Tuochao Chen,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: 该文提出一种可主动识别并隔离佩戴者对话对象的助听系统，通过双模型架构实现实时、高效的对话分离。实验结果表明该方法能在多对话环境下具备良好的识别与隔离效果，促进助听器智能化发展。


<details>
  <summary>Details</summary>
Motivation: 当前助听设备依赖用户主动指定对话对象，操作繁琐且易错。为提升佩戴体验及在多对话情境下的实用性，作者尝试设计能自动识别对话伙伴并主动适应对话动态的助听系统。

Method: 提出一种双模型架构：其中轻量级流式模型每12.5毫秒运行一次，负责实时对话伙伴的提取；而另一个较慢模型则用于捕捉更长时域内对话动态，并以佩戴者自我语音为锚点，结合轮流发言及对话结构实现分辨。整个系统在佩戴者的双耳听觉音频上运行，无需额外提示。

Result: 在真实的2人或3人对话场景中测试，包含11位参与者共6.8小时录音，系统能有效识别及分离多方对话中的伙伴音频，且能推广到不同实际环境，有较好的通用性。

Conclusion: 本研究展示了一种能够主动识别并分离佩戴者对话伙伴的新型助听系统，无需用户手动操作，为提升听觉辅助设备智能化和实用性提供了新方向。

Abstract: We introduce proactive hearing assistants that automatically identify and separate the wearer's conversation partners, without requiring explicit prompts. Our system operates on egocentric binaural audio and uses the wearer's self-speech as an anchor, leveraging turn-taking behavior and dialogue dynamics to infer conversational partners and suppress others. To enable real-time, on-device operation, we propose a dual-model architecture: a lightweight streaming model runs every 12.5 ms for low-latency extraction of the conversation partners, while a slower model runs less frequently to capture longer-range conversational dynamics. Results on real-world 2- and 3-speaker conversation test sets, collected with binaural egocentric hardware from 11 participants totaling 6.8 hours, show generalization in identifying and isolating conversational partners in multi-conversation settings. Our work marks a step toward hearing assistants that adapt proactively to conversational dynamics and engagement. More information can be found on our website: https://proactivehearing.cs.washington.edu/

</details>


### [64] [PRBench: Large-Scale Expert Rubrics for Evaluating High-Stakes Professional Reasoning](https://arxiv.org/abs/2511.11562)
*Afra Feyza Akyürek,Advait Gosai,Chen Bo Calvin Zhang,Vipul Gupta,Jaehwan Jeong,Anisha Gunjal,Tahseen Rabbani,Maria Mazzone,David Randolph,Mohammad Mahmoudi Meymand,Gurshaan Chattha,Paula Rodriguez,Diego Mares,Pavit Singh,Michael Liu,Subodh Chawla,Pete Cline,Lucy Ogaz,Ernesto Hernandez,Zihao Wang,Pavi Bhatter,Marcos Ayestaran,Bing Liu,Yunzhong He*

Main category: cs.CL

TL;DR: 论文提出了PRBench，这是迄今为止法律和金融领域最大、最现实的开放式专业推理评测集合。其评测显示，当前大模型在真实高难任务下表现有限，显示出准确性、透明度和推理链等多方面不足，警示其在实际高风险专业领域的广泛应用仍需更多提升和验证。


<details>
  <summary>Details</summary>
Motivation: 现有学术基准无法全面反映前沿模型在真实高风险专业领域中的实际能力，尤其是在法律和金融等与经济利益紧密相关的重要任务上，评价普遍不足。为推动更具现实意义的能力评估，需开发新型开放式、经济价值导向的基准。

Method: 提出PRBench基准，内容涵盖法律和金融领域的真实复杂任务，由182名具有资质的专业人士根据实际工作流程编写并验证，采用专家策划的细致评分标准进行评测，并分析了20个领先模型的表现和经济相关影响。

Result: PRBench共收录了1100个专家任务和19356条评分标准，覆盖47个美国司法辖区和114个国家。20个主流模型在金融与法律高难任务上的最高得分仅为0.39和0.37，且模型表现经常在具体能力维度上产生分化，常见问题包括推理不透明、判断不准和逻辑不完整，这对专业应用提出了严峻挑战。

Conclusion: 当前前沿模型在法律和金融等高风险现实场景中的表现仍有较大提升空间，尤其在经济相关性强的实际任务上，模型在专业可靠性、推理透明度和准确性等方面存在显著缺陷。

Abstract: Frontier model progress is often measured by academic benchmarks, which offer a limited view of performance in real-world professional contexts. Existing evaluations often fail to assess open-ended, economically consequential tasks in high-stakes domains like Legal and Finance, where practical returns are paramount. To address this, we introduce Professional Reasoning Bench (PRBench), a realistic, open-ended, and difficult benchmark of real-world problems in Finance and Law. We open-source its 1,100 expert-authored tasks and 19,356 expert-curated criteria, making it, to our knowledge, the largest public, rubric-based benchmark for both legal and finance domains. We recruit 182 qualified professionals, holding JDs, CFAs, or 6+ years of experience, who contributed tasks inspired by their actual workflows. This process yields significant diversity, with tasks spanning 114 countries and 47 US jurisdictions. Our expert-curated rubrics are validated through a rigorous quality pipeline, including independent expert validation. Subsequent evaluation of 20 leading models reveals substantial room for improvement, with top scores of only 0.39 (Finance) and 0.37 (Legal) on our Hard subsets. We further catalog associated economic impacts of the prompts and analyze performance using human-annotated rubric categories. Our analysis shows that models with similar overall scores can diverge significantly on specific capabilities. Common failure modes include inaccurate judgments, a lack of process transparency and incomplete reasoning, highlighting critical gaps in their reliability for professional adoption.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [65] [The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems](https://arxiv.org/abs/2511.10704)
*Samih Fadli*

Main category: cs.AI

TL;DR: 作者提出AI存在类似热力学第二定律的“道德熵”自发增长现象，推导了维持对齐稳定的理论边界，并通过大模型实验验证，量化展示了持续对齐对于AI安全性的重要性。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统的目标对齐问题一直未有量化统一的理论描述，作者受到热力学第二定律启发，试图用“道德熵”度量AI系统在缺乏干预时偏移预期的趋势，从而为AI安全研究提供理论工具。

Method: 作者定义了AI系统在不同目标{g_i}上的概率分布p(g_i; theta)及其熵，借助梯度优化理论和Fisher信息矩阵，推导道德熵的动力学，并通过仿真实验检验理论预测。

Result: 仿真结果显示：未加对齐控制时，大模型的道德熵显著增长；而采用论文推导出的临界及以上的对齐工作水平，可以长期将道德熵维持在接近零，模型行为稳定可信。

Conclusion: 该论文提出AI系统在缺乏持续对齐工作的情况下，会自发产生“道德熵”增加，驱使AI走向偏离预期目标的状态。通过将AI对齐问题形式化为类似热力学的过程，证明了AI安全性需以持续的“对齐功”来维持。

Abstract: We propose that unconstrained artificial intelligence obeys a Second Law analogous to thermodynamics, where ethical entropy, defined as a measure of divergence from intended goals, increases spontaneously without continuous alignment work. For gradient-based optimizers, we define this entropy over a finite set of goals {g_i} as S = -Σ p(g_i; theta) ln p(g_i; theta), and we prove that its time derivative dS/dt >= 0, driven by exploration noise and specification gaming. We derive the critical stability boundary for alignment work as gamma_crit = (lambda_max / 2) ln N, where lambda_max is the dominant eigenvalue of the Fisher Information Matrix and N is the number of model parameters. Simulations validate this theory. A 7-billion-parameter model (N = 7 x 10^9) with lambda_max = 1.2 drifts from an initial entropy of 0.32 to 1.69 +/- 1.08 nats, while a system regularized with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17, n = 20 trials). This framework recasts AI alignment as a problem of continuous thermodynamic control, providing a quantitative foundation for maintaining the stability and safety of advanced autonomous systems.

</details>


### [66] [Picking a Representative Set of Solutions in Multiobjective Optimization: Axioms, Algorithms, and Experiments](https://arxiv.org/abs/2511.10716)
*Niclas Boehmer,Maximilian T. Wittmann*

Main category: cs.AI

TL;DR: 本文针对多目标决策中的Pareto剪枝问题，提出了新质量度量方法并系统分析其理论与实际性能，证实新方法有效提升代表性解集的选择。


<details>
  <summary>Details</summary>
Motivation: 多目标优化中解空间庞大，决策者选择负担重。现有Pareto剪枝度量存在意外行为，需改进以更好辅助决策。

Method: 将Pareto剪枝问题重新定义为多赢家投票问题，通过公理化分析现有质量度量，提出新的度量方法，并结合复杂性分析和实验评估。

Result: 揭示了现有度量的不足，提出了有向覆盖的新方法；分析质量度量优化的计算复杂性；实验显示新方法在多种场景下性能优异。

Conclusion: 本文提出了一种新的质量度量方法——有向覆盖（directed coverage），并通过理论和实验分析表明该方法在多目标优化问题中表现出色。不同质量度量对于所选解集特性有重要影响。

Abstract: Many real-world decision-making problems involve optimizing multiple objectives simultaneously, rendering the selection of the most preferred solution a non-trivial problem: All Pareto optimal solutions are viable candidates, and it is typically up to a decision maker to select one for implementation based on their subjective preferences. To reduce the cognitive load on the decision maker, previous work has introduced the Pareto pruning problem, where the goal is to compute a fixed-size subset of Pareto optimal solutions that best represent the full set, as evaluated by a given quality measure. Reframing Pareto pruning as a multiwinner voting problem, we conduct an axiomatic analysis of existing quality measures, uncovering several unintuitive behaviors. Motivated by these findings, we introduce a new measure, directed coverage. We also analyze the computational complexity of optimizing various quality measures, identifying previously unknown boundaries between tractable and intractable cases depending on the number and structure of the objectives. Finally, we present an experimental evaluation, demonstrating that the choice of quality measure has a decisive impact on the characteristics of the selected set of solutions and that our proposed measure performs competitively or even favorably across a range of settings.

</details>


### [67] [Structure-Aware Encodings of Argumentation Properties for Clique-width](https://arxiv.org/abs/2511.10767)
*Yasir Mahmood,Markus Hecher,Johanna Groven,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 本文针对团块宽度这一重要图参数，提出了一种创新的抽象论证到(Q)SAT的线性归约方法，有效保持了图的结构紧致性，并证明了归约的优化极限，为高效SAT求解的实例编码策略提供了理论基础和新工具。


<details>
  <summary>Details</summary>
Motivation: 尽管团块宽度比树宽更普适（尤其适用于稠密图），并且关于团块宽度的算法不断发展，但如何高效地编码到(Q)SAT并利用这一参数提升计算效率仍然缺乏系统研究，因此本文以抽象论证为实际代表问题，填补了团块宽度在编码理论中的空白。

Method: 设计了基于有向分解引导(DDG)的归约方法，将抽象论证问题线性归约到(Q)SAT，并系统性分析和证明了归约在团块宽度方面的保持性，同时对所有论证语义进行了实验性和理论性的探讨。

Result: 所提DDG归约方法实现了团块宽度的线性保持，覆盖了所有主流论证语义和计数情形；并且理论上证明了该方法在归约开销上接近最优。拓展了复杂结构图实例在SAT/QSAT求解中的编码能力边界认知。

Conclusion: 提出了一种新颖的从抽象论证问题到(Q)SAT的归约方法，在归约过程中线性保持了有向图的团块宽度，从而更好地理解了团块宽度对编码(可满足性)能力的影响。本文还证明了这种归约带来的开销在合理假设下无法显著改进。

Abstract: Structural measures of graphs, such as treewidth, are central tools in computational complexity resulting in efficient algorithms when exploiting the parameter. It is even known that modern SAT solvers work efficiently on instances of small treewidth. Since these solvers are widely applied, research interests in compact encodings into (Q)SAT for solving and to understand encoding limitations. Even more general is the graph parameter clique-width, which unlike treewidth can be small for dense graphs. Although algorithms are available for clique-width, little is known about encodings. We initiate the quest to understand encoding capabilities with clique-width by considering abstract argumentation, which is a robust framework for reasoning with conflicting arguments. It is based on directed graphs and asks for computationally challenging properties, making it a natural candidate to study computational properties. We design novel reductions from argumentation problems to (Q)SAT. Our reductions linearly preserve the clique-width, resulting in directed decomposition-guided (DDG) reductions. We establish novel results for all argumentation semantics, including counting. Notably, the overhead caused by our DDG reductions cannot be significantly improved under reasonable assumptions.

</details>


### [68] [Potential Outcome Rankings for Counterfactual Decision Making](https://arxiv.org/abs/2511.10776)
*Yuta Kawakami,Jin Tian*

Main category: cs.AI

TL;DR: 本文首次提出潜在结果排序概率（PoR）和最优潜在结果概率（PoB）作为反事实决策新指标，并给出理论分析与实证验证，丰富了反事实推断的决策支持手段。


<details>
  <summary>Details</summary>
Motivation: 现有反事实决策方法侧重于期望效用和排序，缺乏对潜在结果排序和最优结果实现概率的直接度量方法，因此需要新的指标和方法支持复杂决策需求。

Method: 提出PoR和PoB两个新指标，给出其识别理论与界定方法，并进行了估计和数值实验，同时应用到实际数据集进行验证。

Result: 提出了两种新指标PoR和PoB，并提供了其识别定理与界限，同时通过数值实验和真实数据展示了估计方法的有效性和实际应用价值。

Conclusion: 本文提出了两种新的反事实决策指标（PoR与PoB），并验证了其在理论和实际数据中的有效性。

Abstract: Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.

</details>


### [69] [From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models](https://arxiv.org/abs/2511.10788)
*Chao Wu,Baoheng Li,Mingchen Gao,Zhenyi Wang*

Main category: cs.AI

TL;DR: 本文系统性阐述了大语言模型自适应推理的理论与分类，并提出评价和发展方向，为LLM推理能力提升和效果评估提供新思路。


<details>
  <summary>Details</summary>
Motivation: 传统效率驱动的推理测评忽视了模型在遇到不同复杂度问题时推理努力分配不足，导致模型在简单任务上推理冗长，而在复杂任务上推理不足。作者希望推动LLM按任务难度自适应分配推理资源。

Method: 论文提出推理能力适应性的新视角，将归纳、演绎、反演推理模式形式化，并采用控制增强型策略优化理论，系统性地将现有方法分为基于训练和非训练的两大类，并为方法分类建立了标准化框架。

Result: 论文提出了分别从学习策略（强化学习、微调、控制器）和推理过程（使用prompt、反馈驱动、模块组合）两方面实现推理自适应的系统分类，并揭示了不同机制实际实现自适应推理的方式，为未来方法比较和演化提供了理论工具。

Conclusion: 该论文重新定义了LLM的推理能力，强调了自适应推理（根据输入难度调整推理努力）在未来智能评估中的重要性，并指出了自我评价、元推理和与人类一致的推理控制等领域的挑战。

Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.

</details>


### [70] [HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings](https://arxiv.org/abs/2511.10842)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy,Vaibhav Gandhi*

Main category: cs.AI

TL;DR: HyperComplEx自适应多空间嵌入架构，显著提升各类关系建模效果，在大规模知识图谱上优于现有方法，兼具准确率与效率，且实现及数据集已公开。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入方法难以统一建模层级、对称和非对称等多样关系类型，且在大规模知识图谱上精度和效率受限；因此需要一种能自适应关系空间特征并具高扩展性的新方法。

Method: 提出了一种结合双曲、复数及欧几里得空间的混合嵌入框架，核心是通过可学习的注意力机制，为不同关系类型分配最优空间，借助多空间一致性损失实现跨空间预测一致性。并通过适应性维度分配，保证了模型对大数据集的近线性扩展能力。

Result: 在包含1K到10M论文的知识图谱（25K到45M三元组）上，HyperComplEx在准确率（MRR）上全面超越TransE、RotatE、DistMult、ComplEx等主流方法。尤其在最大数据集上MRR达到0.612，相较最佳基线提升4.8%；推理效率高达85毫秒/三元组，并具备极佳的扩展性。

Conclusion: HyperComplEx显著提升了知识图谱嵌入的准确性，尤其在大规模复杂关系类型任务中能超越当前所有主流方法，同时保持高效的训练与推理速度。其实现与数据集公开，为知识图谱领域的可复现性和后续研究提供基础。

Abstract: Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.

</details>


### [71] [Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction](https://arxiv.org/abs/2511.10853)
*Gerui Xu,Boyou Chen,Huizhong Guo,Dave LeBlanc,Ananna Ahmed,Zhaonan Sun,Shan Bao*

Main category: cs.AI

TL;DR: 该文开发了一个利用多智能体AI的自动交通事故重建系统，可以处理各种不完整、多源的撞车数据。在挑战性测试中，该系统识别车辆行为和EDR事件的表现优于人类专家，并能提升重建一致性与准确率。


<details>
  <summary>Details</summary>
Motivation: 动机是解决传统人工交通碰撞重建在多模态不完整数据分析时结果不一致、准确率有限、推理困难等长期瓶颈问题，引入AI多智能体协作以提升重建的准确率与一致性。

Method: 方法上，论文采用两阶段多智能体AI框架，第一阶段（重建）将多模态碰撞片段数据（文本报告、表格数据、视觉场景图）生成自然语言描述，第二阶段（推理）结合上述描述与时序EDR（事件数据记录仪）进行碰撞推理。通过对真实事故数据库中的277起后车追尾主车减速（LVD）案件进行处理，对39起数据冲突复杂的案例进行深入评估。

Result: 框架在全部39个复杂案例上达到100%准确率，成功定位EDR关键时刻并区分主/被撞车辆，超越了人类专家92%的准确率，且能够稳健处理缺失、冲突或模糊的数据，展现了自动化多模态数据处理与碰撞推理的明显优势。

Conclusion: 该研究提出的多智能体AI框架能够比人类专家更准确并且一致地重建交通碰撞场景，尤其是在数据不完整或存在歧义的情况下展现出更强的鲁棒性和精度。

Abstract: Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.

</details>


### [72] [Enhancing Demand-Oriented Regionalization with Agentic AI and Local Heterogeneous Data for Adaptation Planning](https://arxiv.org/abs/2511.10857)
*Seyedeh Mobina Noorani,Shangde Gao,Changjie Chen,Karla Saldana Ochoa*

Main category: cs.AI

TL;DR: 本研究提出一种融合AI与人类决策的动态区域划分系统，通过RepSC-SOM模型及智能体辅助，有效支持灾害规划中的区划生成与评估，增强了用户交互和决策灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统城市规划单元（如普查区、邮政编码、邻里）难以有效应对特定社区需求及灾害管理挑战，缺乏灵活性和透明度，因此需开发更能适应多样需求并促进有效治理的区域划分方法。

Method: 基于代表性初始化的空间约束自组织映射（RepSC-SOM），结合AI智能体实现动态区域划分，融入人机协同探索、特征推荐和空间约束引导。通过实证案例（如佛罗里达州杰克逊维尔洪灾风险）展示应用效果。

Result: 该平台可交互地生成以实际需求为导向的灾害防控区域，提升区域划分的科学性、透明度和动态适应能力，在杰克逊维尔洪灾场景下取得良好效果。

Conclusion: 论文提出并验证了一种结合AI智能体与人机互动的动态规划支持系统，能够根据需求灵活划分灾害防控区域，并提高灾害响应规划的科学性与可操作性。

Abstract: Conventional planning units or urban regions, such as census tracts, zip codes, or neighborhoods, often do not capture the specific demands of local communities and lack the flexibility to implement effective strategies for hazard prevention or response. To support the creation of dynamic planning units, we introduce a planning support system with agentic AI that enables users to generate demand-oriented regions for disaster planning, integrating the human-in-the-loop principle for transparency and adaptability. The platform is built on a representative initialized spatially constrained self-organizing map (RepSC-SOM), extending traditional SOM with adaptive geographic filtering and region-growing refinement, while AI agents can reason, plan, and act to guide the process by suggesting input features, guiding spatial constraints, and supporting interactive exploration. We demonstrate the capabilities of the platform through a case study on the flooding-related risk in Jacksonville, Florida, showing how it allows users to explore, generate, and evaluate regionalization interactively, combining computational rigor with user-driven decision making.

</details>


### [73] [LLM enhanced graph inference for long-term disease progression modelling](https://arxiv.org/abs/2511.10890)
*Tiantian He,An Zhao,Elinor Thompson,Anna Schroder,Ahmed Abdulaal,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.AI

TL;DR: 文章提出用大语言模型辅助挖掘脑区交互并预测阿尔茨海默病病理扩散的新方法，在可解释性和预测效果上明显优于传统模型，并能发现新的致病因素。


<details>
  <summary>Details</summary>
Motivation: 当前以单一模态脑连接组为基础推断神经变性疾病进展关系的方法过于简化，导致长期预测不准确；纯数据驱动的图结构学习又缺乏可辨识性。作者旨在解决这两大不足。

Method: 该方法以LLMs为专家引导，整合多模态关系与多样的病理机制，从不规则纵向病人数据中协同优化疾病进展轨迹与有生物约束的区域交互图结构。基于tau-PET影像数据进行实证检验。

Result: 新框架在阿尔茨海默病队列的tau-PET数据实验中表现出更优的预测精度和可解释性，并发现了超越传统脑连接性的新病理机制。

Conclusion: 提出的利用大语言模型(LLMs)指导脑区间变量交互学习的新方法，在构建阿尔茨海默病病理传播模型方面优于传统手段，预测更准确且具有更高的可解释性，同时挖掘出超越传统连接性的新疾病推动因子。

Abstract: Understanding the interactions between biomarkers among brain regions during neurodegenerative disease is essential for unravelling the mechanisms underlying disease progression. For example, pathophysiological models of Alzheimer's Disease (AD) typically describe how variables, such as regional levels of toxic proteins, interact spatiotemporally within a dynamical system driven by an underlying biological substrate, often based on brain connectivity. However, current methods grossly oversimplify the complex relationship between brain connectivity by assuming a single-modality brain connectome as the disease-spreading substrate. This leads to inaccurate predictions of pathology spread, especially during the long-term progression period. Meanhwile, other methods of learning such a graph in a purely data-driven way face the identifiability issue due to lack of proper constraint. We thus present a novel framework that uses Large Language Models (LLMs) as expert guides on the interaction of regional variables to enhance learning of disease progression from irregularly sampled longitudinal patient data. By leveraging LLMs' ability to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, our method simultaneously optimizes 1) the construction of long-term disease trajectories from individual-level observations and 2) the biologically-constrained graph structure that captures interactions among brain regions with better identifiability. We demonstrate the new approach by estimating the pathology propagation using tau-PET imaging data from an Alzheimer's disease cohort. The new framework demonstrates superior prediction accuracy and interpretability compared to traditional approaches while revealing additional disease-driving factors beyond conventional connectivity measures.

</details>


### [74] [Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints](https://arxiv.org/abs/2511.10952)
*Steven J. Jones,Robert E. Wray,John E. Laird*

Main category: cs.AI

TL;DR: 本文指出，AI系统在现实复杂环境中会遇到超出训练范围的情形，必须整合多种知识类型，才能做到决策与人类价值对齐。


<details>
  <summary>Details</summary>
Motivation: 在真实复杂环境中，AI系统常遇到训练策略未覆盖的情境，且任何行动方案都难以完全符合集体约束，导致需要新的决策机制来兼顾规范、人类期望与现实目标。

Method: 通过理论分析与实证案例研究，论文分析了AI系统决策时对知识类型的需求，并探讨了它们如何整合规范性、实用性和情境理解。

Result: 研究识别出AI在应对复杂情境时需融合规范性、实用性和情境三类知识，从而选择和执行更符合人类价值的行为。

Conclusion: 本文认为，仅凭以往的训练策略，自动化AI系统难以完全应对新颖或不明确的情境，需具备更丰富的知识和决策整合能力，才能做出与人类期望相一致的决策。

Abstract: Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual "knowledge" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.

</details>


### [75] [AI Agent-Driven Framework for Automated Product Knowledge Graph Construction in E-Commerce](https://arxiv.org/abs/2511.11017)
*Dimitar Peshevski,Riste Stojanov,Dimitar Trajanov*

Main category: cs.AI

TL;DR: 本文提出一种全自动AI智能体驱动的商品知识图谱构建方法，核心依靠大模型，实现高覆盖率和低冗余，无需人工规则，有效适用于大规模零售商品数据的结构化处理。


<details>
  <summary>Details</summary>
Motivation: 随着电商平台产生大量非结构化商品数据，信息检索、推荐和分析面临巨大挑战。知识图谱可以结构化管理这些数据，但现有构建方法繁琐且依赖人工，亟需自动化、可扩展的构建方式。

Method: 方法包含三个阶段，分别为本体创建与扩展、本体细化和知识图谱填充，全部由利用大型语言模型驱动的专属AI智能体自动完成，无需预定义的模式或手工提取规则。

Result: 在空调产品描述真实数据集上评估，系统在本体生成和知识图谱填充方面表现优异，实现了超过97%的属性覆盖率和极低冗余，验证了方法的实用性和高效性。

Conclusion: 本文提出了一个基于AI智能体的自动化产品知识图谱构建框架，在实际商品数据集上验证了方法的有效性，实现了高度的属性覆盖率和低冗余，展示了大型语言模型在零规则、零模板下的结构化知识抽取潜力。

Abstract: The rapid expansion of e-commerce platforms generates vast amounts of unstructured product data, creating significant challenges for information retrieval, recommendation systems, and data analytics. Knowledge Graphs (KGs) offer a structured, interpretable format to organize such data, yet constructing product-specific KGs remains a complex and manual process. This paper introduces a fully automated, AI agent-driven framework for constructing product knowledge graphs directly from unstructured product descriptions. Leveraging Large Language Models (LLMs), our method operates in three stages using dedicated agents: ontology creation and expansion, ontology refinement, and knowledge graph population. This agent-based approach ensures semantic coherence, scalability, and high-quality output without relying on predefined schemas or handcrafted extraction rules. We evaluate the system on a real-world dataset of air conditioner product descriptions, demonstrating strong performance in both ontology generation and KG population. The framework achieves over 97\% property coverage and minimal redundancy, validating its effectiveness and practical applicability. Our work highlights the potential of LLMs to automate structured knowledge extraction in retail, providing a scalable path toward intelligent product data integration and utilization.

</details>


### [76] [Faster Symmetry Breaking Constraints for Abstract Structures](https://arxiv.org/abs/2511.11029)
*Özgür Akgün,Mun See Chang,Ian P. Gent,Christopher Jefferson*

Main category: cs.AI

TL;DR: 提出了一种针对高抽象结构的新型对称性消除方法，在约束编程中显著提高了解决不可区分对象相关对称问题的效率。


<details>
  <summary>Details</summary>
Motivation: 现有对称性消除技术在处理高层次抽象变量时通常产生大量低效的复杂约束，严重影响求解器性能，因此需要针对抽象结构开发更加高效的对称性消除方案。

Method: 开发了一种利用抽象结构表示的新对称性消除技术，重点在于利用结构的具体表示形式而非对所有抽象变量施加复杂约束；通过与现有方法进行实验对比，验证新方法的效率。

Result: 新方法成功利用抽象结构的表示最佳化了对称性消除，对不可区分对象相关的对称拥有更高的求解速度，在实验中优于前人方法。

Conclusion: 提出了一种新的不完全对称性消除方法，用于约束编程中的抽象结构，并且实验证明该方法在处理不可区分对象相关对称性时性能优于以往方法。

Abstract: In constraint programming and related paradigms, a modeller specifies their problem in a modelling language for a solver to search and return its solution(s). Using high-level modelling languages such as Essence, a modeller may express their problems in terms of abstract structures. These are structures not natively supported by the solvers, and so they have to be transformed into or represented as other structures before solving. For example, nested sets are abstract structures, and they can be represented as matrices in constraint solvers. Many problems contain symmetries and one very common and highly successful technique used in constraint programming is to "break" symmetries, to avoid searching for symmetric solutions. This can speed up the solving process by many orders of magnitude. Most of these symmetry-breaking techniques involve placing some kind of ordering for the variables of the problem, and picking a particular member under the symmetries, usually the smallest. Unfortunately, applying this technique to abstract variables produces a very large number of complex constraints that perform poorly in practice. In this paper, we demonstrate a new incomplete method of breaking the symmetries of abstract structures by better exploiting their representations. We apply the method in breaking the symmetries arising from indistinguishable objects, a commonly occurring type of symmetry, and show that our method is faster than the previous methods proposed in (Akgün et al. 2025).

</details>


### [77] [Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?](https://arxiv.org/abs/2511.11040)
*Qian Zhang,Yan Zheng,Jinyi Liu,Hebin Liang,Lanjun Wang*

Main category: cs.AI

TL;DR: 通过创新角色分配和一致性评估方法显著提升了多智能体辩论在LLM推理任务的性能，提出MADC策略，有效解决真相未知场景下的推理瓶颈。


<details>
  <summary>Details</summary>
Motivation: 以往多智能体辩论（MAD）被证明有助于提升大语言模型（LLM）的推理能力，但如何分配和设计不同角色尚未被充分研究。作者关注角色分配策略对辩论推理效果的影响，尤其是在实际应用中如何应对“真相未知”的问题。

Method: 作者提出了一种新的角色分配策略“Truth Last”，以及一种新的一致性评估方法MADC（Multi-Agent Debate Consistency）。MADC通过模拟多角色间的路径一致性来判定最可能为‘真相’的角色，并在9种不同的大语言模型上进行推理任务实验。

Result: “Truth Last”策略在推理任务中将MAD性能提升高达22%。MADC在多种LLM上的实验结果显示其能持续克服MAD的性能瓶颈，并具备迁移性和提升空间。

Conclusion: 角色分配和一致性评估是多智能体辩论推理能力提升的关键，MADC为大语言模型智能体尺度扩展和推理性能改进提供了有效路径。

Abstract: Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, "Truth Last", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.

</details>


### [78] [Autonomous Vehicle Path Planning by Searching With Differentiable Simulation](https://arxiv.org/abs/2511.11043)
*Asen Nachkov,Jan-Nico Zaech,Danda Pani Paudel,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 提出通过可微仿真器支持的搜索框架DSS，有效提升了自动驾驶规划能力及精度，实验效果明显优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶场景复杂，碰撞风险高，需要高效、安全的动作规划。现有方法在所有核心组件（策略、状态预测、评价器）均需学习时难以高效搜索最佳路径。

Method: 采用可微仿真器Waymax作为下一状态预测器和评价器，通过梯度下降优化未来行动序列，结合规划梯度与随机搜索。

Result: 实验表明，所提出的方法在跟踪和路径规划准确性方面，优于序列预测、模仿学习、无模型强化学习以及其他主流规划方法。

Conclusion: 提出了一种能有效提升自动驾驶安全性和规划准确性的可微仿真搜索框架。

Abstract: Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.

</details>


### [79] [ARCTraj: A Dataset and Benchmark of Human Reasoning Trajectories for Abstract Problem Solving](https://arxiv.org/abs/2511.11079)
*Sejin Kim,Hayan Choi,Seokki Lee,Sundong Kim*

Main category: cs.AI

TL;DR: ARCTraj通过收集并建模人类在ARC任务上的动态推理行为，为解释性推理研究和强化学习等方法在该领域的应用奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 现有ARC相关工作多为静态输入输出监督，无法揭示人类推理的动态过程。该工作旨在通过收集人类操作轨迹，深入理解推理过程的中间步骤。

Method: 通过O2ARC网页端收集过程性轨迹数据，将人类操作抽象为对象级动作序列，伴随时间戳、ID和成功标记，并在数据管道中定义轨迹抽象、MDP建模、以及可与序列/生成/强化学习方法融合的学习流程。

Result: 提供了包含约10000条轨迹数据集，涵盖400个训练任务，实现任务流程抽象和多种学习范式的集成，并揭示了人类推理在空间选择、颜色归因和策略收敛上的结构性及多样性。

Conclusion: ARCTraj数据集及其方法框架为建模人类在复杂视觉任务中的推理过程提供了基础，有助于促进人类类推理的可解释性、对齐性和泛化智能的研究。

Abstract: We present ARCTraj, a dataset and methodological framework for modeling human reasoning through complex visual tasks in the Abstraction and Reasoning Corpus (ARC). While ARC has inspired extensive research on abstract reasoning, most existing approaches rely on static input--output supervision, which limits insight into how reasoning unfolds over time. ARCTraj addresses this gap by recording temporally ordered, object-level actions that capture how humans iteratively transform inputs into outputs, revealing intermediate reasoning steps that conventional datasets overlook. Collected via the O2ARC web interface, it contains around 10,000 trajectories annotated with task identifiers, timestamps, and success labels across 400 training tasks from the ARC-AGI-1 benchmark. It further defines a unified reasoning pipeline encompassing data collection, action abstraction, Markov decision process (MDP) formulation, and downstream learning, enabling integration with reinforcement learning, generative modeling, and sequence modeling methods such as PPO, World Models, GFlowNets, Diffusion agents, and Decision Transformers. Analyses of spatial selection, color attribution, and strategic convergence highlight the structure and diversity of human reasoning. Together, these contributions position ARCTraj as a structured and interpretable foundation for studying human-like reasoning, advancing explainability, alignment, and generalizable intelligence.

</details>


### [80] [Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)](https://arxiv.org/abs/2511.11095)
*Dillon Z. Chen,Till Hofmann,Toryn Q. Klassen,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 提出一种通过训练问题的回归与提炼生成通用规划规则的新方法，理论和实验均显示其在多个评测指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 通用规划旨在自动合成能解决一组相关规划问题的程序，当前方法在综合成本、覆盖率、解质量方面仍有提升空间，因此需要新的简单高效方法。

Method: 针对一组训练问题，每个问题的每个目标原子都计算最优计划，对结果计划进行目标回归，再提升为一阶条件-动作规则，从而得出可执行或用于剪枝的通用方案。

Result: 实验证明在合成成本、规划覆盖率与解的质量三方面，该方法均显著优于当前领先通用规划器。

Conclusion: 本文提出的方法能够有效地合成适用于相关问题族的通用规划方案，并且在多个经典和数值规划领域上达到或超越了现有最优规划器。

Abstract: Generalised planning (GP) refers to the task of synthesising programs that solve families of related planning problems. We introduce a novel, yet simple method for GP: given a set of training problems, for each problem, compute an optimal plan for each goal atom in some order, perform goal regression on the resulting plans, and lift the corresponding outputs to obtain a set of first-order $\textit{Condition} \rightarrow \textit{Actions}$ rules. The rules collectively constitute a generalised plan that can be executed as is or alternatively be used to prune the planning search space. We formalise and prove the conditions under which our method is guaranteed to learn valid generalised plans and state space pruning axioms for search. Experiments demonstrate significant improvements over state-of-the-art (generalised) planners with respect to the 3 metrics of synthesis cost, planning coverage, and solution quality on various classical and numeric planning domains.

</details>


### [81] [GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models](https://arxiv.org/abs/2511.11134)
*Jingxuan Wei,Caijun Jia,Xi Bai,Xinglong Xu,Siyuan Li,Linzhuang Sun,Bihui Yu,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: 本文提出了专门用于评测多模态模型几何生成推理能力的新基准GGBench，解决了现有评测方法片面的问题，推动智能系统更有效融合语言理解与视觉生成。


<details>
  <summary>Details</summary>
Motivation: 当前统一多模态模型在融合感知与生成方面取得进展，但评测方法仍然欠缺对模型“生成推理”能力的综合考察，迫切需要新的评测框架。

Method: 构建了专门测试几何生成推理能力的基准——GGBench，通过结合语言理解和精确视觉生成，系统地诊断模型的综合表现。

Result: GGBench基准可以全面系统地测试模型的几何生成推理能力，为智能系统的未来升级设立了更高标准。

Conclusion: 论文指出现有基准评测无法有效衡量统一多模态模型在生成推理中的综合能力，因此提出了GGBench基准，以更严格评估模型的生成推理能力。

Abstract: The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.

</details>


### [82] [Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning](https://arxiv.org/abs/2511.11182)
*Dayong Liang,Xiao-Yong Wei,Changmeng Zheng*

Main category: cs.AI

TL;DR: 本文提出MUG协议，通过设计‘卧底’检测和反事实多模态测试，解决了多智能体辩论机制中agent本身产生幻觉的问题，大幅提升了大模型的多模态推理和事实验证能力。


<details>
  <summary>Details</summary>
Motivation: 原有的多智能体辩论（MAD）机制假设所有agent均理性，但实际LLMs本身就可能输出幻觉，导致共识不可盲信。本文意在解决这一漏洞，引入更有效且贴合现实的幻觉检测与多模态推理机制。

Method: 作者设计了Multi-agent Undercover Gaming（MUG）协议，仿照‘谁是卧底’社交推理游戏，引入‘卧底’（易产生幻觉的agent）测试。通过动态修改参考图片，加入反事实证据，让多个agent对证据信息进行辨识和讨论，以此来辨别出现幻觉的agent。

Result: MUG协议在三个层面上改进了MAD：1) 通过反事实测试实现事实检验，2) 采用动态证据增强跨证据推理能力，3) 鼓励agent主动推理讨论而非被动问答。这些提升显著加强了LLMs在多模态场景下的可靠性和推理效果。

Conclusion: 本文提出的MUG协议，通过将‘卧底’检测机制引入多智能体辩论，利用多模态反事实测试，有效提升了LLMs多模态推理的可靠性和鲁棒性。

Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.

</details>


### [83] [STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models](https://arxiv.org/abs/2511.11233)
*Huajian Zhang,Mingyue Cheng,Yucong Luo,Xiaoyu Tao*

Main category: cs.AI

TL;DR: STaR通过慢思考和不确定性建模，提升了大语言模型表格推理的深度与稳定性，经验证优于现有方法，并能可靠泛化到新任务。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型表格推理不仅缺乏类似人类的深度和反复推敲，还存在推理不稳定的问题，影响下游应用可靠性。

Method: STaR采用两阶段难度感知强化学习进行训练，并在推理阶段结合逐步思考与不确定性量化，实现稳健且可信的表格推理过程。

Result: STaR在多个基准上表现优异，推理稳定性显著提升，在域外数据集上也展现强泛化能力。

Conclusion: STaR框架提高了大语言模型在表格推理任务中的表现和稳定性，展现出可靠与认知启发的潜力，尤其能良好泛化到域外数据集。

Abstract: Table reasoning with the large language models (LLMs) is a fundamental path toward building intelligent systems that can understand and analyze over structured data. While recent progress has shown promising results, they still suffer from two key limitations: (i) the reasoning processes lack the depth and iterative refinement characteristic of human cognition; and (ii) the reasoning processes exhibit instability, which compromises their reliability in downstream applications. In this work, we present STaR (slow-thinking for table reasoning), a new framework achieving cognitive table reasoning, in which LLMs are equipped with slow-thinking capabilities by explicitly modeling step-by-step thinking and uncertainty-aware inference. During training, STaR employs two-stage difficulty-aware reinforcement learning (DRL), progressively learning from simple to complex queries under a composite reward. During inference, STaR performs trajectory-level uncertainty quantification by integrating token-level confidence and answer consistency, enabling selection of more credible reasoning paths. Extensive experiments on benchmarks demonstrate that STaR achieves superior performance and enhanced reasoning stability. Moreover, strong generalization over out-of-domain datasets further demonstrates STaR's potential as a reliable and cognitively inspired solution for table reasoning with LLMs.

</details>


### [84] [A Workflow for Full Traceability of AI Decisions](https://arxiv.org/abs/2511.11275)
*Julius Wenzel,Syeda Umaima Alam,Andreas Schmidt,Hanwei Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: 本文针对AI决策过程不可追踪带来的责任与合规风险，提出并实现了一种结合DBOM与保密计算的工作流，确保AI决策过程全链路留痕、可验证且不可篡改，并以识别蘑菇应用为例展示其可行性。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在关键决策中几乎没有可溯源的文档，这阻碍了对决策成因的追踪和责任归属的认定，尤其在涉及到人权和法律时有较大风险。因此亟需一种方法体系能确保决策过程清晰可查并能在法庭上被采信。

Method: 将DBOM（Decentralized Bill of Materials）概念扩展，并结合保密计算技术，开发出可实际运行的工作流，用于生成防篡改、可验证且全面的AI决策追踪文档。以开发区分有毒/可食用蘑菇的应用为例，演示了该系统的具体流程。

Result: 提出并实现了全球首个支持AI决策全流程追踪、可验证且不可篡改记录的工作流系统。该方法增强了AI系统的可追责性和透明性，提升了对高风险决策场景的法律和社会信任度。

Conclusion: 本论文提出了一种可行且创新的方法，通过对AI决策训练与推理过程中的每个组件进行强制性记录，实现了AI决策过程的全面可追溯性，从而有助于责任链重建和法律合规。

Abstract: An ever increasing number of high-stake decisions are made or assisted by automated systems employing brittle artificial intelligence technology. There is a substantial risk that some of these decision induce harm to people, by infringing their well-being or their fundamental human rights. The state-of-the-art in AI systems makes little effort with respect to appropriate documentation of the decision process. This obstructs the ability to trace what went into a decision, which in turn is a prerequisite to any attempt of reconstructing a responsibility chain. Specifically, such traceability is linked to a documentation that will stand up in court when determining the cause of some AI-based decision that inadvertently or intentionally violates the law.
  This paper takes a radical, yet practical, approach to this problem, by enforcing the documentation of each and every component that goes into the training or inference of an automated decision. As such, it presents the first running workflow supporting the generation of tamper-proof, verifiable and exhaustive traces of AI decisions. In doing so, we expand the DBOM concept into an effective running workflow leveraging confidential computing technology. We demonstrate the inner workings of the workflow in the development of an app to tell poisonous and edible mushrooms apart, meant as a playful example of high-stake decision support.

</details>


### [85] [Can You Tell the Difference? Contrastive Explanations for ABox Entailments](https://arxiv.org/abs/2511.11281)
*Patrick Koopmann,Yasir Mahmood,Axel-Cyrille Ngonga Ngomo,Balram Tiwari*

Main category: cs.AI

TL;DR: 本论文提出并分析了用于描述逻辑本体ABox推理的对比式解释方法，能更好地比较和解释不同个体的推理归属，并完成初步实现和评估。


<details>
  <summary>Details</summary>
Motivation: 当前存在很多针对正向归纳推理或缺失归纳推理的解释方法，但它们只关注一方，而对比式解释能够同时考虑正向和缺失推理，突出两个个体在知识库中的相关差异和共同点，满足更复杂的解释需求。

Method: 提出了针对描述逻辑本体ABox推理的对比解释框架，分析了不同变体和最优性标准下的计算复杂性；实现了一种计算对比解释的方法，并在真实知识库生成的问题上进行了实验评估。

Result: 分析了多种描述逻辑下，该对比解释方法的计算复杂性；实际实现并评估了一种计算方法，展示了其在现实知识库问题中的可行性和有效性。

Conclusion: 论文提出了一种对比式ABox解释方法，可以同时比较知识库中两个个体在某一概念下的归属与否，从而突出它们之间的关键差异和共同点。实现并初步验证了该方法的有效性。

Abstract: We introduce the notion of contrastive ABox explanations to answer questions of the type "Why is a an instance of C, but b is not?". While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.

</details>


### [86] [EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment](https://arxiv.org/abs/2511.11301)
*Ruoxi Cheng,Haoxuan Ma,Teng Ma,Hongyi Zhang*

Main category: cs.AI

TL;DR: EcoAlign以经济理性原则优化LVLM对齐，同时提升安全性和实用性、显著减少算力消耗，验证结果表现优异。


<details>
  <summary>Details</summary>
Motivation: LVLMs现有对齐方法面临安全性、效用和成本三者难以统一，以及“过程盲区”导致有害推理被无害理由掩盖，浪费计算资源。需寻找更经济且安全的对齐方法。

Method: 提出EcoAlign推理时框架，将模型对齐问题建模为有限理性的经济搜索过程，通过增量扩展思维图，并用前瞻性函数动态权衡安全、效用与成本，剩余预算有限。采用最薄弱环节原则防止有害推理伪装为安全步骤。

Result: 实验覆盖3个闭源、2个开源模型和6个数据集，EcoAlign在安全性与效用上达到或优于SOTA水平，同时计算成本更低，展现高效且可扩展的对齐能力。

Conclusion: EcoAlign能够在保证或提升安全性和实用性的同时，显著降低大视觉语言模型（LVLM）对齐过程的计算成本。

Abstract: Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.

</details>


### [87] [RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms](https://arxiv.org/abs/2511.11323)
*Yitian Kou,Yihe Gu,Chen Zhou,DanDan Zhu,Shuguang Kuai*

Main category: cs.AI

TL;DR: 该文提出了一种将社会心理规则与强化学习结合的社会导航算法，实现了更舒适、高效且解释性强的人机共处导航能力，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的社会导航方法虽具可解释性但难以泛化，数据驱动方法则存在效率低、解释性差等问题。为兼顾灵活性、有效性和可解释性，需融合认知科学和机器学习的新方法。

Method: 提出并实现了RLSLM混合型强化学习框架，将基于行为实验的规则型社会移动模型融入RL奖励函数，通过VR人机交互实验进行性能评估，同时进行了消融及敏感性分析。

Result: RLSLM在用户体验方面优于最先进的基于规则模型，同时大大增强了社会导航策略的可解释性，实验结果表明该方法具备良好的扩展性和以人为本的特性。

Conclusion: RLSLM框架能够有效提升社会化智能体在人员密集环境中的导航能力，在提升用户体验及解释性的同时，优于现有基于规则的方法。

Abstract: Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.

</details>


### [88] [Robust and Efficient Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.11393)
*Zejiao Liu,Yi Li,Jiali Wang,Junqi Tu,Yitian Hong,Fangfei Li,Yang Liu,Toshiharu Sugawara,Yang Tang*

Main category: cs.AI

TL;DR: 本综述分析了受限制通信环境下的多智能体强化学习前沿进展，涵盖实时性、带宽、隐私等问题，并针对合作自动驾驶、定位与建图、联邦学习进行案例总结，提出未来挑战及共设计策略建议。


<details>
  <summary>Details</summary>
Motivation: 传统MARL方法假设通信条件理想，与实际场景如低延迟、带宽受限、隐私保护等需求不符，因此需要调研和总结更具现实约束下的高效鲁棒通信机制。

Method: 论文系统性地回顾了近年多智能体强化学习在通信受限情境下的策略，包括应对消息扰动、传输延迟和带宽受限等问题。通过文献分析，归纳总结了三大实际应用领域：协作自动驾驶、分布式定位与建图、联邦学习。

Result: 梳理了现有应对通信不完美的主要方法，总结实际案例应用，并指出实践中存在的关键开放难题以及未来研究方向，强调需联合通信、学习与鲁棒性设计新方法。

Conclusion: 该综述论文认为，现有多智能体强化学习（MARL）方法在通讯假设上与现实存在较大差距，提倡在设计时联合考虑通信、学习和鲁棒性，以缩小理论与实际应用之间的鸿沟。

Abstract: Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.

</details>


### [89] [CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction](https://arxiv.org/abs/2511.11423)
*Cong-Tinh Dao,Nguyen Minh Thao Phan,Jun-En Ding,Chenwei Wu,David Restrepo,Dongsheng Luo,Fanyi Zhao,Chun-Chieh Liao,Wen-Chih Peng,Chi-Te Wang,Pei-Fu Chen,Ling Chen,Xinglong Ju,Feng Liu,Fang-Ming Hung*

Main category: cs.AI

TL;DR: CURENet通过融合医疗文本、化验与访问时序数据，极大地提升了慢性病预测的准确性（>94%），表明多模态EHR有助于优化医疗决策和患者健康管理。


<details>
  <summary>Details</summary>
Motivation: 现有预测模型往往只关注单一数据类型，未能充分挖掘不同模态数据间的互动、冗余及时序关系，导致预测能力不足。作者希望通过整合EHR多模态和序列信息，改善慢性病预测和临床应用效果。

Method: 该研究提出了CURENet，一个多模态预测模型。它利用大型语言模型（LLMs）处理临床文本和化验结果，采用transformer编码器处理患者的时间序列访问数据，实现不同数据类型的深度融合。

Result: 在MIMIC-III和FEMH两个真实数据集上，CURENet对前十种慢性疾病的多标签预测准确率超过94%。

Conclusion: CURENet模型能够有效整合多模态的电子健康记录数据（包括临床文本、化验结果和时间序列数据），提升慢性疾病预测的准确性，促进临床决策和患者预后改善。

Abstract: Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.

</details>


### [90] [Experience-Guided Adaptation of Inference-Time Reasoning Strategies](https://arxiv.org/abs/2511.11519)
*Adam Stein,Matthew Trager,Benjamin Bowman,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: EGuR是一个基于经验动态生成且优化推理策略的系统，能全面适应推理需求，在诸多复杂任务上显著提升准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有推理系统在推理阶段的策略适应性不足，只能通过修改输入来调整行为，无法灵活改变系统参数或结构。更灵活的系统需要离线优化，部署后难以动态调整。为解决这些局限，该工作提出一种能在运行时根据经验动态生成和调整整体推理策略的系统。

Method: 利用LLM构建元策略，以经验为基础动态生成包括LLM调用、工具选择、采样参数和控制逻辑在内的完整推理策略。系统由两个组件组成：Guide根据当前问题和结构化记忆生成多个候选策略，Consolidator整合执行反馈优化未来策略生成。

Result: 在AIME 2025、3-SAT及Big Bench Extra Hard等五个基准测试中，EGuR比最强基线提升至多14%的准确率，计算成本减少至多111倍，并且系统表现随经验积累同步提升。

Conclusion: EGuR系统通过动态生成并优化策略，在多个复杂任务上显著提升准确率和计算效率，且随着经验积累持续优化表现。

Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.

</details>
